{"0": {
    "doc": "Agents vs Tools",
    "title": "Agents versus Tools: Why Use Multiple Agents?",
    "content": "In the MUXI Framework, you have two primary approaches to building intelligent systems: . | Single Agent with Multiple Tools: One agent that has access to various tools | Multiple Specialized Agents: Several agents managed by an orchestrator | . While both approaches have their merits, this document explains why the multi-agent approach with an orchestrator often provides significant advantages for complex applications. ",
    "url": "/muxi/agents-vs-tools/#agents-versus-tools-why-use-multiple-agents",
    
    "relUrl": "/agents-vs-tools/#agents-versus-tools-why-use-multiple-agents"
  },"1": {
    "doc": "Agents vs Tools",
    "title": "Architectural Benefits of Multiple Agents",
    "content": "1. Specialization and Expertise . Multiple agents can specialize in different domains, providing more accurate and relevant responses in their areas of expertise. | Domain-specific knowledge: Create agents that excel in particular fields (medical, legal, finance, programming) | Specialized system prompts: Each agent can have tailored instructions optimized for its specific purpose | Varied personalities: Different communication styles can be used for different contexts (formal for documentation, casual for assistance) | Role separation: Some agents can focus on creativity, others on factual accuracy | . # Create specialized agents for different domains orchestrator.create_agent( agent_id=\"code_assistant\", model=OpenAIModel(model=\"gpt-4o\"), system_message=\"You are an expert programmer specializing in Python and JavaScript.\" ) orchestrator.create_agent( agent_id=\"marketing_assistant\", model=AnthropicModel(model=\"claude-3-opus\"), system_message=\"You are a creative marketing expert who helps craft compelling content.\" ) . 2. Separation of Concerns . Each agent maintains its own independent state and memory, which provides several advantages: . | Isolated memory contexts: Conversation histories don’t get mixed between different tasks | Reduced context length: Each agent only processes information relevant to its domain | Cleaner mental models: Agents can maintain consistent personalities without confusion | Privacy boundaries: User information can be compartmentalized appropriately | . # Different memory configurations for different agents orchestrator.create_agent( agent_id=\"customer_service\", buffer_memory=BufferMemory(max_tokens=8000), # Longer context for complex issues long_term_memory=LongTermMemory() # Persistent memory for customer interactions ) orchestrator.create_agent( agent_id=\"quick_assistant\", buffer_memory=BufferMemory(max_tokens=2000), # Short context for simple Q&amp;A long_term_memory=None # No long-term memory needed ) . 3. Technical Advantages . Having multiple agents enables more sophisticated system architectures: . | Model optimization: Use different language models based on task requirements . | Powerful models for complex reasoning | Efficient models for simple, frequent tasks | . | Resource allocation: Distribute computational resources efficiently | Parallel processing: Execute multiple tasks simultaneously | Model experimentation: Test different models for the same purpose to compare results | . # Cost optimization with different models for different tasks orchestrator.create_agent( agent_id=\"reasoning_agent\", model=OpenAIModel(model=\"gpt-4o\") # More powerful, expensive model ) orchestrator.create_agent( agent_id=\"classification_agent\", model=OpenAIModel(model=\"gpt-3.5-turbo\") # More efficient, less expensive model ) . 4. Enhanced User Experience . Multiple agents can provide a more dynamic and personalized experience: . | Multi-user support: Each user can have personalized experiences with the same agent types | Context switching: Users can interact with different agents for different needs without losing conversation flow | Team simulation: Create a “team” of agents with different roles working together | Continuous availability: If one agent is busy or fails, others can still respond | . # Supporting multiple users with Memobase long_term_memory = LongTermMemory() memobase = Memobase(long_term_memory=long_term_memory) orchestrator.create_agent( agent_id=\"personal_assistant\", long_term_memory=memobase, # Pass Memobase as long_term_memory system_message=\"You are a personal assistant that remembers user preferences.\" ) # User 123 and User 456 can interact with the same agent but have separate memories response1 = orchestrator.chat(\"personal_assistant\", \"Remember I like pizza\", user_id=123) response2 = orchestrator.chat(\"personal_assistant\", \"Remember I like sushi\", user_id=456) . 5. System Design Benefits . A multi-agent architecture produces more maintainable and flexible systems: . | Microservice architecture: More modular system design with better separation of concerns | Easier testing: Test each agent’s functionality in isolation | Flexible deployment: Deploy or update individual agents without affecting the whole system | Graceful degradation: System can continue functioning if one agent fails | Scalability: Add new agents for new capabilities without modifying existing ones | . ",
    "url": "/muxi/agents-vs-tools/#architectural-benefits-of-multiple-agents",
    
    "relUrl": "/agents-vs-tools/#architectural-benefits-of-multiple-agents"
  },"2": {
    "doc": "Agents vs Tools",
    "title": "The Role of the Orchestrator",
    "content": "The orchestrator acts as the central coordinator for all agents and provides several key functions: . | Request routing: Direct user requests to the appropriate agent | Agent lifecycle management: Create, configure, and destroy agents as needed | Resource allocation: Manage computational resources across agents | Cross-agent coordination: Enable agents to communicate and collaborate | System-wide settings: Maintain configurations that apply to all agents | Centralized logging: Track all agent activities and interactions | . # Orchestrator directing requests to different agents def process_request(user_query, user_id): # Route to the appropriate agent based on query content if \"code\" in user_query or \"programming\" in user_query: return orchestrator.chat(\"code_assistant\", user_query, user_id=user_id) elif \"marketing\" in user_query or \"content\" in user_query: return orchestrator.chat(\"marketing_assistant\", user_query, user_id=user_id) else: return orchestrator.chat(\"general_assistant\", user_query, user_id=user_id) . ",
    "url": "/muxi/agents-vs-tools/#the-role-of-the-orchestrator",
    
    "relUrl": "/agents-vs-tools/#the-role-of-the-orchestrator"
  },"3": {
    "doc": "Agents vs Tools",
    "title": "When to Use a Single Agent with Multiple Tools",
    "content": "While the multi-agent approach has many advantages, there are cases where a single agent with multiple tools is more appropriate: . | Simple applications: When the scope is limited and well-defined | Resource constraints: When computational or financial resources are limited | Linear workflows: When tasks follow a predictable, sequential pattern | Unified persona: When a consistent personality is essential | . # Single agent with multiple tools agent = Agent( name=\"multipurpose_assistant\", model=OpenAIModel(model=\"gpt-4o\"), memory=BufferMemory(), tools={ \"calculator\": Calculator(), \"web_search\": WebSearch(), \"weather\": WeatherTool(), \"file_operations\": FileOperations() }, system_message=\"You are a helpful assistant that can use various tools to solve problems.\" ) . ",
    "url": "/muxi/agents-vs-tools/#when-to-use-a-single-agent-with-multiple-tools",
    
    "relUrl": "/agents-vs-tools/#when-to-use-a-single-agent-with-multiple-tools"
  },"4": {
    "doc": "Agents vs Tools",
    "title": "Conclusion",
    "content": "The choice between multiple specialized agents and a single agent with multiple tools depends on your specific requirements. For complex applications that span multiple domains, handle multiple users, or require different types of AI capabilities, the multi-agent approach with an orchestrator provides significant architectural advantages. The MUXI Framework is designed to support both approaches, giving you the flexibility to choose the right architecture for your needs or even combine them as your system evolves. ",
    "url": "/muxi/agents-vs-tools/#conclusion",
    
    "relUrl": "/agents-vs-tools/#conclusion"
  },"5": {
    "doc": "Agents vs Tools",
    "title": "Agents vs Tools",
    "content": " ",
    "url": "/muxi/agents-vs-tools/",
    
    "relUrl": "/agents-vs-tools/"
  },"6": {
    "doc": "Agents",
    "title": "Agents Overview",
    "content": "Agents are the core component of the MUXI Framework. An agent combines a language model, memory systems, and tools to create an intelligent assistant that can understand and respond to user requests. ",
    "url": "/muxi/agents/#agents-overview",
    
    "relUrl": "/agents/#agents-overview"
  },"7": {
    "doc": "Agents",
    "title": "What is an Agent?",
    "content": "An agent is an autonomous entity that: . | Communicates with users via natural language | Processes requests and generates responses using a language model | Stores conversation history in memory | Executes tools to perform actions or retrieve information | Can maintain separate memory contexts for different users (with Memobase) | . ",
    "url": "/muxi/agents/#what-is-an-agent",
    
    "relUrl": "/agents/#what-is-an-agent"
  },"8": {
    "doc": "Agents",
    "title": "Creating an Agent",
    "content": "Agents can be created through the Orchestrator, which manages multiple agents and their interactions. Here are several ways to create an agent: . Via Python Code . import asyncio from src.models import OpenAIModel from src.memory.buffer import BufferMemory from src.memory.long_term import LongTermMemory from src.memory.memobase import Memobase from src.tools.web_search import WebSearchTool from src.tools.calculator import CalculatorTool from src.core.orchestrator import Orchestrator async def create_agent(): # Create language model model = OpenAIModel(model=\"gpt-4o\") # Create memory systems buffer_memory = BufferMemory(max_tokens=4000) long_term_memory = LongTermMemory( connection_string=\"postgresql://user:password@localhost:5432/ai_agent_db\", table_name=\"agent_memories\" ) # For multi-user support, add Memobase memobase = Memobase(long_term_memory=long_term_memory) # Create tools tools = [WebSearchTool(), CalculatorTool()] # Create orchestrator and agent orchestrator = Orchestrator() agent_id = \"my_agent\" # Create a standard agent orchestrator.create_agent( agent_id=agent_id, model=model, buffer_memory=buffer_memory, long_term_memory=long_term_memory, tools=tools, system_message=\"You are a helpful AI assistant.\", set_as_default=True ) # Create a multi-user agent orchestrator.create_agent( agent_id=\"multi_user_agent\", model=model, buffer_memory=BufferMemory(), long_term_memory=memobase, # Pass Memobase as long_term_memory tools=tools, system_message=\"You are a helpful AI assistant that remembers user preferences.\" ) return orchestrator, agent_id # Usage async def main(): orchestrator, agent_id = await create_agent() # Standard agent usage response = await orchestrator.run(agent_id, \"What's the weather in New York?\") print(response) # Multi-user agent usage (with user_id) response = await orchestrator.run(\"multi_user_agent\", \"My name is Alice\", user_id=123) print(response) # Later, the agent will remember Alice response = await orchestrator.run(\"multi_user_agent\", \"What's my name?\", user_id=123) print(response) # Should respond with \"Your name is Alice\" if __name__ == \"__main__\": asyncio.run(main()) . Via the REST API . You can create an agent by making a POST request to the API: . # Create a standard agent curl -X POST http://localhost:5050/agents \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"my_agent\", \"system_message\": \"You are a helpful AI assistant.\", \"tools\": [\"web_search\", \"calculator\"] }' # Create a multi-user agent curl -X POST http://localhost:5050/agents \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"system_message\": \"You are a helpful AI assistant that remembers user preferences.\", \"tools\": [\"web_search\", \"calculator\"], \"use_long_term_memory\": true, \"multi_user_support\": true }' . Via the CLI . The framework provides a CLI command to create agents: . # Create a standard agent python -m src.cli.agent create my_agent --system \"You are a helpful AI assistant.\" --tools web_search calculator # Create a multi-user agent python -m src.cli.agent create multi_user_agent --system \"You are a helpful AI assistant that remembers user preferences.\" --tools web_search calculator --multi-user . ",
    "url": "/muxi/agents/#creating-an-agent",
    
    "relUrl": "/agents/#creating-an-agent"
  },"9": {
    "doc": "Agents",
    "title": "Agent Parameters",
    "content": "When creating an agent, you can configure various parameters: . | agent_id (required): A unique identifier for the agent | model (required): The language model provider to use (e.g., OpenAIModel, AnthropicModel) | buffer_memory: Short-term memory for the current conversation | long_term_memory: Persistent memory for storing information across sessions. Can be a LongTermMemory or Memobase instance for multi-user support. | tools: A list of tools the agent can use | system_message: Instructions that define the agent’s behavior | description: A concise description of the agent’s capabilities and purpose, used for intelligent message routing (critical for multi-agent systems) | set_as_default: Whether to set this as the default agent for the orchestrator | multi_user_support: Whether to enable user-specific memory via Memobase | . ",
    "url": "/muxi/agents/#agent-parameters",
    
    "relUrl": "/agents/#agent-parameters"
  },"10": {
    "doc": "Agents",
    "title": "Intelligent Message Routing",
    "content": "The MUXI framework includes an intelligent message routing system that can automatically direct user messages to the most appropriate agent based on their content and the agents’ descriptions. How It Works . | When a user sends a message without specifying an agent, the routing system analyzes the message. | The system compares the message content against each agent’s description. | Using a dedicated LLM (configurable via environment variables), it selects the most appropriate agent. | The message is then sent to the selected agent for processing. | . Configuring Agent Descriptions . For effective routing, provide clear and specific descriptions for each agent: . # Create a weather-focused agent orchestrator.create_agent( agent_id=\"weather_assistant\", model=OpenAIModel(model=\"gpt-4o\"), description=\"Specialized in providing weather forecasts, answering questions about climate and weather phenomena, and reporting current conditions in various locations.\", # ... other parameters ) # Create a finance-focused agent orchestrator.create_agent( agent_id=\"finance_assistant\", model=OpenAIModel(model=\"gpt-4o\"), description=\"Expert in financial analysis, investment strategies, market trends, stock recommendations, and personal finance advice.\", # ... other parameters ) . Configuration via YAML/JSON . You can also specify descriptions in configuration files: . name: travel_assistant description: \"Specialized in travel recommendations, flight and hotel bookings, tourist attractions, and travel planning.\" system_message: \"You are a helpful travel assistant.\" # ... other configuration . Routing Configuration . The routing system can be configured through environment variables: . # Routing LLM provider (defaults to \"openai\") ROUTING_LLM=openai # Model to use for routing (defaults to \"gpt-4o-mini\") ROUTING_LLM_MODEL=gpt-4o-mini # Temperature for routing decisions (defaults to 0.0) ROUTING_LLM_TEMPERATURE=0.0 # Whether to cache routing decisions (defaults to true) ROUTING_USE_CACHING=true # Time in seconds to cache routing decisions (defaults to 3600) ROUTING_CACHE_TTL=3600 . ",
    "url": "/muxi/agents/#intelligent-message-routing",
    
    "relUrl": "/agents/#intelligent-message-routing"
  },"11": {
    "doc": "Agents",
    "title": "Interacting with an Agent",
    "content": "Once you’ve created an agent, you can interact with it in several ways: . Via Python Code with Configuration-based Approach (Recommended) . from src import muxi # Initialize MUXI mx = muxi() # Add agents from configuration files mx.add_agent(\"assistant\", \"configs/general_assistant.yaml\") mx.add_agent(\"weather\", \"configs/weather_agent.yaml\") mx.add_agent(\"finance\", \"configs/finance_agent.yaml\") # Let the orchestrator automatically select the appropriate agent (recommended) response = mx.chat(\"What's the weather in New York?\") print(response) # Will likely be handled by the weather agent # Or chat with a specific agent if needed (optional) response = mx.chat(\"Tell me about investment strategies\", agent_name=\"finance\") print(response) # With user-specific context response = mx.chat(\"What's my favorite color?\", user_id=123) print(response) # Will use the user's context if available . Via Python Code with Traditional Approach . # Continue from previous example # Standard agent interaction response = await orchestrator.chat(message=\"What's the population of Tokyo?\") print(response) # Orchestrator will select the appropriate agent # Explicitly specifying an agent response = await orchestrator.chat(message=\"Tell me about quantum computing\", agent_id=agent_id) print(response) # Multi-user agent interaction response = await orchestrator.chat(message=\"My favorite color is blue\", user_id=123) print(response) # The multi-user agent will remember user-specific information response = await orchestrator.chat(message=\"What's my favorite color?\", user_id=123) print(response) # Should respond with \"Your favorite color is blue\" . Via the REST API . # Chat with a specific agent curl -X POST http://localhost:5050/agents/chat \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"my_agent\", \"message\": \"What is the capital of France?\" }' # Chat with a multi-user agent (specify user_id) curl -X POST http://localhost:5050/agents/chat \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"message\": \"What is the capital of France?\", \"user_id\": 123 }' # Search memory for a multi-user agent curl -X POST http://localhost:5050/agents/memory/search \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"query\": \"favorite color\", \"user_id\": 123 }' . Via WebSocket . import asyncio import json import websockets async def chat_with_agent(): uri = \"ws://localhost:5050/ws\" async with websockets.connect(uri) as websocket: # Set user ID for multi-user agents await websocket.send(json.dumps({ \"type\": \"set_user\", \"user_id\": 123 })) # Subscribe to an agent (optional - if you want messages from a specific agent) await websocket.send(json.dumps({ \"type\": \"subscribe\", \"agent_id\": \"multi_user_agent\" })) # Wait for subscription confirmation response = await websocket.recv() print(f\"Subscription response: {response}\") # Send a message without specifying an agent (automatic selection) await websocket.send(json.dumps({ \"type\": \"chat\", \"message\": \"What is the capital of France?\" })) # Or send a message to a specific agent await websocket.send(json.dumps({ \"type\": \"chat\", \"message\": \"What's the weather forecast?\", \"agent_id\": \"weather_agent\" })) # Receive response while True: response = await websocket.recv() data = json.loads(response) print(f\"Received: {data}\") if data[\"type\"] == \"agent_done\": break asyncio.run(chat_with_agent()) . ",
    "url": "/muxi/agents/#interacting-with-an-agent",
    
    "relUrl": "/agents/#interacting-with-an-agent"
  },"12": {
    "doc": "Agents",
    "title": "Advanced Agent Features",
    "content": "Custom System Messages . You can customize your agent’s behavior by providing a detailed system message: . system_message = \"\"\" You are an AI assistant specialized in helping with scientific research. - Always cite your sources - When uncertain, acknowledge the limits of your knowledge - Provide step-by-step explanations for complex topics - Use LaTeX formatting for mathematical equations \"\"\" orchestrator.create_agent( agent_id=\"science_agent\", model=model, system_message=system_message, # Other parameters... ) . Specialized Agents . You can create specialized agents for specific tasks: . # Create a programming assistant orchestrator.create_agent( agent_id=\"code_assistant\", model=OpenAIModel(model=\"gpt-4o\"), tools=[CalculatorTool()], system_message=\"You are an expert coding assistant. Provide clean, efficient code examples and explain your reasoning.\", ) # Create a customer service agent orchestrator.create_agent( agent_id=\"customer_service\", model=AnthropicModel(model=\"claude-3-opus\"), tools=[WebSearchTool()], system_message=\"You are a friendly customer service representative. Help users with their questions in a polite and helpful manner.\", ) . Multi-User Support . You can create agents that support multiple users with separate memory contexts: . from src.memory.memobase import Memobase from src.memory.long_term import LongTermMemory # Create a multi-user agent # Memobase extends LongTermMemory with multi-user capabilities memobase = Memobase(long_term_memory=LongTermMemory()) orchestrator.create_agent( agent_id=\"customer_service\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), long_term_memory=memobase, # Pass Memobase as long_term_memory system_message=\"\"\" You are a customer service assistant that helps different customers. Maintain a personalized conversation with each user. Remember their preferences and previous interactions. \"\"\" ) # Different users interact with the same agent user1_response = await orchestrator.chat(\"customer_service\", \"My name is John and I need help with my order #12345\", user_id=1001) user2_response = await orchestrator.chat(\"customer_service\", \"I'm Sarah and I have a question about your return policy\", user_id=1002) # Later interactions - the agent remembers each user user1_followup = await orchestrator.chat(\"customer_service\", \"Any updates on my order?\", user_id=1001) # Agent will remember John and order #12345 user2_followup = await orchestrator.chat(\"customer_service\", \"Thanks for the information yesterday\", user_id=1002) # Agent will remember Sarah and the return policy discussion . ",
    "url": "/muxi/agents/#advanced-agent-features",
    
    "relUrl": "/agents/#advanced-agent-features"
  },"13": {
    "doc": "Agents",
    "title": "Best Practices",
    "content": ". | Choose the right language model: Different tasks require different language models. For complex reasoning, use advanced models like GPT-4 or Claude 3. | Craft effective system messages: Be specific about the agent’s role, tone, and constraints. | Provide relevant tools: Only give the agent tools it needs for its specific purpose. | Memory management: For long conversations, ensure your buffer size is adequate. For persistent knowledge, use long-term memory. | Error handling: Implement proper error handling for tool execution failures. | Regular testing: Test your agents with diverse inputs to ensure they behave as expected. | User ID management: For multi-user agents, ensure user IDs are consistently applied across interactions. | Memory partitioning: Use Memobase for applications where user data isolation is important. | . ",
    "url": "/muxi/agents/#best-practices",
    
    "relUrl": "/agents/#best-practices"
  },"14": {
    "doc": "Agents",
    "title": "Troubleshooting",
    "content": "Agent Not Responding . | Check if the language model API key is valid | Ensure the agent ID is correct | Verify that the WebSocket connection is established | . Agent Not Using Tools Correctly . | Check if tools are properly registered | Review the system message for clear instructions | Ensure tool parameters are correctly defined | . Memory Issues . | Verify database connection for long-term memory | Check buffer memory size for token limits | Ensure memory systems are properly initialized | . ",
    "url": "/muxi/agents/#troubleshooting",
    
    "relUrl": "/agents/#troubleshooting"
  },"15": {
    "doc": "Agents",
    "title": "Next Steps",
    "content": "After creating your agent, you might want to: . | Add custom tools to extend its capabilities | Configure memory systems for better recall | Implement multi-agent collaboration for complex tasks | Connect to the WebSocket server for real-time interaction | . ",
    "url": "/muxi/agents/#next-steps",
    
    "relUrl": "/agents/#next-steps"
  },"16": {
    "doc": "Agents",
    "title": "Agents",
    "content": " ",
    "url": "/muxi/agents/",
    
    "relUrl": "/agents/"
  },"17": {
    "doc": "API",
    "title": "REST API",
    "content": "The MUXI Framework provides a comprehensive REST API for interacting with agents, managing tools, and accessing memory. This guide explains the available endpoints and how to use them. ",
    "url": "/muxi/api/#rest-api",
    
    "relUrl": "/api/#rest-api"
  },"18": {
    "doc": "API",
    "title": "API Overview",
    "content": "The REST API offers the following capabilities: . | Create, manage, and delete agents | Send messages to agents | Access and search agent memory | List and manage available tools | Monitor agent and system status | Support multi-user contexts with user-specific memory | . ",
    "url": "/muxi/api/#api-overview",
    
    "relUrl": "/api/#api-overview"
  },"19": {
    "doc": "API",
    "title": "Getting Started",
    "content": "Starting the API Server . To start the API server, run: . # Start the API server on the default port (5050) python -m src.api.run # Start the API server on a different port python -m src.api.run --port 8080 # Start the API server with auto-reload for development python -m src.api.run --reload . API Documentation . The API includes Swagger documentation, which can be accessed at: . http://localhost:5050/docs . Authentication . By default, the API doesn’t require authentication in development mode. For production deployments, you can enable authentication by setting the API_AUTH_ENABLED environment variable: . # Enable authentication export API_AUTH_ENABLED=true export API_KEY=your_secret_api_key # Then start the server python -m src.api.run . When authentication is enabled, include the API key in the headers: . curl -X GET http://localhost:5050/agents \\ -H \"Authorization: Bearer your_secret_api_key\" . ",
    "url": "/muxi/api/#getting-started",
    
    "relUrl": "/api/#getting-started"
  },"20": {
    "doc": "API",
    "title": "API Endpoints",
    "content": "Health Check . Check if the API server is running: . # Request curl -X GET http://localhost:5050/ # Response (200 OK) { \"status\": \"ok\", \"version\": \"0.1.0\" } . Agent Management . List Agents . Retrieve all registered agents: . # Request curl -X GET http://localhost:5050/agents # Response (200 OK) { \"agents\": [ { \"agent_id\": \"research_assistant\", \"tools\": [\"web_search\", \"calculator\"], \"is_default\": true }, { \"agent_id\": \"coding_assistant\", \"tools\": [\"calculator\"], \"is_default\": false } ] } . Create Agent . Create a new agent: . # Request curl -X POST http://localhost:5050/agents \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"my_agent\", \"model\": \"gpt-4o\", \"system_message\": \"You are a helpful AI assistant.\", \"enable_web_search\": true, \"enable_calculator\": true, \"use_long_term_memory\": true, \"multi_user_support\": true }' # Response (200 OK) { \"message\": \"Agent 'my_agent' created successfully\" } . Parameters: . | agent_id (required): Unique identifier for the agent | model: LLM model to use (default: “gpt-4o”) | system_message: Instructions for the agent’s behavior | enable_web_search: Whether to enable the web search tool (default: false) | enable_calculator: Whether to enable the calculator tool (default: false) | use_long_term_memory: Whether to enable long-term memory (default: false) | multi_user_support: Whether to enable multi-user support via Memobase (default: false) | . Get Agent . Retrieve information about a specific agent: . # Request curl -X GET http://localhost:5050/agents/data_analyst # Response (200 OK) { \"agent_id\": \"data_analyst\", \"tools\": [\"calculator\", \"web_search\"], \"is_default\": false, \"created_at\": \"2023-06-15T14:30:00Z\" } . Delete Agent . Delete an agent: . # Request curl -X DELETE http://localhost:5050/agents/data_analyst # Response (200 OK) { \"status\": \"success\", \"message\": \"Agent 'data_analyst' deleted successfully\" } . Set Default Agent . Set an agent as the default: . # Request curl -X POST http://localhost:5050/agents/research_assistant/set_default # Response (200 OK) { \"status\": \"success\", \"message\": \"Agent 'research_assistant' set as default\" } . Chat . Send Message to Agent . Send a message to an agent: . # Request curl -X POST http://localhost:5050/agents/chat \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"my_agent\", \"message\": \"What is the capital of France?\", \"user_id\": 123 }' # Response (200 OK) { \"message\": \"The capital of France is Paris.\", \"agent_id\": \"my_agent\", \"user_id\": 123, \"tools_used\": [] } . Parameters: . | message (required): The message to send to the agent | agent_id: ID of the agent to send the message to (uses default if omitted) | user_id: User ID for multi-user support (default: 0) | . Chat with Default Agent . Send a message to the default agent: . # Request curl -X POST http://localhost:5050/agents/chat \\ -H \"Content-Type: application/json\" \\ -d '{ \"message\": \"What is the capital of France?\" }' # Response (200 OK) { \"agent_id\": \"research_assistant\", \"message\": \"The capital of France is Paris.\", \"tokens\": { \"prompt\": 45, \"completion\": 8, \"total\": 53 } } . Streaming Chat . Stream a response from an agent: . # Request curl -X POST http://localhost:5050/agents/chat/stream \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"research_assistant\", \"message\": \"Tell me about quantum computing\" }' # Response (200 OK, server-sent events) data: {\"chunk\": \"Quantum computing is\", \"done\": false} data: {\"chunk\": \" a type of computing\", \"done\": false} data: {\"chunk\": \" that uses quantum mechanics\", \"done\": false} ... data: {\"chunk\": \"\", \"done\": true} . Memory . Search Agent Memory . Search an agent’s memory for relevant information: . # Request curl -X POST http://localhost:5050/agents/memory/search \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"my_agent\", \"query\": \"What did we discuss about Paris?\", \"limit\": 5, \"use_long_term\": true, \"user_id\": 123 }' # Response (200 OK) { \"query\": \"What did we discuss about Paris?\", \"agent_id\": \"my_agent\", \"user_id\": 123, \"results\": [ { \"text\": \"User: What is the capital of France?\\nAssistant: The capital of France is Paris.\", \"source\": \"buffer\", \"distance\": 0.15, \"metadata\": { \"timestamp\": 1625097600 } } ] } . Parameters: . | query (required): The search query | agent_id: ID of the agent to search (uses default if omitted) | limit: Maximum number of results to return (default: 5) | use_long_term: Whether to include long-term memory in search (default: true) | user_id: User ID for multi-user support (default: 0) | . Clear Agent Memory . Clear an agent’s memory: . # Request curl -X POST http://localhost:5050/agents/memory/clear \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"my_agent\", \"clear_long_term\": false, \"user_id\": 123 }' # Response (200 OK) { \"message\": \"Memory cleared successfully\", \"agent_id\": \"my_agent\", \"user_id\": 123 } . Parameters: . | agent_id: ID of the agent to clear memory for (uses default if omitted) | clear_long_term: Whether to clear long-term memory as well (default: false) | user_id: User ID for multi-user support (default: 0) | . Tools . List Tools . List all available tools: . # Request curl -X GET http://localhost:5050/tools # Response (200 OK) { \"tools\": [ { \"name\": \"calculator\", \"description\": \"Perform mathematical calculations\", \"parameters\": { \"expression\": { \"type\": \"string\", \"description\": \"The mathematical expression to evaluate\" } }, \"required_parameters\": [\"expression\"] }, { \"name\": \"web_search\", \"description\": \"Search the web for information\", \"parameters\": { \"query\": { \"type\": \"string\", \"description\": \"The search query\" }, \"num_results\": { \"type\": \"integer\", \"description\": \"Number of results to return\" } }, \"required_parameters\": [\"query\"] } ] } . Get Tool Information . Get information about a specific tool: . # Request curl -X GET http://localhost:5050/tools/calculator # Response (200 OK) { \"name\": \"calculator\", \"description\": \"Perform mathematical calculations\", \"parameters\": { \"expression\": { \"type\": \"string\", \"description\": \"The mathematical expression to evaluate\" } }, \"required_parameters\": [\"expression\"] } . Register a Tool . Register a new tool: . # Request curl -X POST http://localhost:5050/tools/register \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"weather\", \"class_path\": \"src.tools.weather.WeatherTool\", \"config\": { \"api_key\": \"your_weather_api_key\" } }' # Response (201 Created) { \"status\": \"success\", \"message\": \"Tool 'weather' registered successfully\" } . Update Tool Configuration . Update a tool’s configuration: . # Request curl -X PATCH http://localhost:5050/tools/weather \\ -H \"Content-Type: application/json\" \\ -d '{ \"config\": { \"api_key\": \"new_weather_api_key\" } }' # Response (200 OK) { \"status\": \"success\", \"message\": \"Tool 'weather' updated successfully\" } . Unregister a Tool . Remove a tool from the registry: . # Request curl -X DELETE http://localhost:5050/tools/weather # Response (200 OK) { \"status\": \"success\", \"message\": \"Tool 'weather' unregistered successfully\" } . System Management . Get System Status . Retrieve system status information: . # Request curl -X GET http://localhost:5050/system/status # Response (200 OK) { \"status\": \"healthy\", \"version\": \"0.1.0\", \"uptime_seconds\": 3600, \"memory_usage_mb\": 156.4, \"active_connections\": 3, \"agent_count\": 2 } . Get Resource Usage . Get resource usage statistics: . # Request curl -X GET http://localhost:5050/system/resources # Response (200 OK) { \"cpu_percent\": 12.5, \"memory_percent\": 23.7, \"memory_used_mb\": 156.4, \"total_memory_mb\": 8192, \"disk_percent\": 45.2, \"agents\": { \"research_assistant\": { \"requests_processed\": 42, \"average_latency_ms\": 980.5, \"token_usage\": { \"prompt\": 12500, \"completion\": 4350, \"total\": 16850 } }, \"coding_assistant\": { \"requests_processed\": 17, \"average_latency_ms\": 1120.8, \"token_usage\": { \"prompt\": 8700, \"completion\": 3200, \"total\": 11900 } } } } . ",
    "url": "/muxi/api/#api-endpoints",
    
    "relUrl": "/api/#api-endpoints"
  },"21": {
    "doc": "API",
    "title": "Multi-User Support",
    "content": "The MUXI Framework supports multi-user operations through the user_id parameter. This allows for: . | User-specific memory contexts: Each user gets their own memory space | Personalized conversations: Agents can remember information specific to each user | Privacy boundaries: User memories are isolated from each other | . Creating a Multi-User Agent . curl -X POST http://localhost:5050/agents \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"system_message\": \"You are a helpful assistant that remembers information about different users.\", \"use_long_term_memory\": true, \"multi_user_support\": true }' . Interacting with a Multi-User Agent . # User 123 introduces themselves curl -X POST http://localhost:5050/agents/chat \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"message\": \"My name is Alice and I live in New York.\", \"user_id\": 123 }' # User 456 introduces themselves curl -X POST http://localhost:5050/agents/chat \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"message\": \"My name is Bob and I live in London.\", \"user_id\": 456 }' # User 123 asks a question (agent remembers it's Alice) curl -X POST http://localhost:5050/agents/chat \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"message\": \"Where do I live?\", \"user_id\": 123 }' # Response will mention New York # User 456 asks the same question (agent remembers it's Bob) curl -X POST http://localhost:5050/agents/chat \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"message\": \"Where do I live?\", \"user_id\": 456 }' # Response will mention London . Searching User-Specific Memory . # Search Alice's memories curl -X POST http://localhost:5050/agents/memory/search \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"query\": \"Where does the user live?\", \"user_id\": 123 }' # Results will include information about New York # Search Bob's memories curl -X POST http://localhost:5050/agents/memory/search \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"query\": \"Where does the user live?\", \"user_id\": 456 }' # Results will include information about London . Clearing User-Specific Memory . # Clear Alice's buffer memory curl -X POST http://localhost:5050/agents/memory/clear \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"user_id\": 123, \"clear_long_term\": false }' # Clear Bob's entire memory (buffer and long-term) curl -X POST http://localhost:5050/agents/memory/clear \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"user_id\": 456, \"clear_long_term\": true }' # Clear default user memory curl -X POST http://localhost:5050/agents/memory/clear \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"multi_user_agent\", \"clear_long_term\": false }' . ",
    "url": "/muxi/api/#multi-user-support",
    
    "relUrl": "/api/#multi-user-support"
  },"22": {
    "doc": "API",
    "title": "Advanced API Usage",
    "content": "Error Handling . The API uses standard HTTP status codes: . | 200 OK: The request was successful | 201 Created: The resource was created successfully | 400 Bad Request: The request was invalid | 401 Unauthorized: Authentication is required | 404 Not Found: The requested resource was not found | 500 Internal Server Error: An error occurred on the server | . Error responses include detailed information: . { \"error\": true, \"message\": \"Agent not found\", \"detail\": \"No agent with ID 'unknown_agent' exists\", \"status_code\": 404 } . Pagination . For endpoints that return multiple items, pagination is supported: . # Request with pagination curl -X GET \"http://localhost:5050/agents?page=2&amp;limit=10\" # Response (200 OK) { \"agents\": [ ... ], \"pagination\": { \"page\": 2, \"limit\": 10, \"total_items\": 35, \"total_pages\": 4 } } . Filtering . Some endpoints support filtering: . # Filter agents by tool curl -X GET \"http://localhost:5050/agents?tool=web_search\" # Response (200 OK) { \"agents\": [ { \"agent_id\": \"research_assistant\", \"tools\": [\"web_search\", \"calculator\"], \"is_default\": true }, // Other agents with web_search tool ] } . Rate Limiting . The API includes rate limiting to prevent abuse: . HTTP/1.1 429 Too Many Requests Retry-After: 30 X-RateLimit-Limit: 100 X-RateLimit-Remaining: 0 X-RateLimit-Reset: 1623763200 { \"error\": true, \"message\": \"Rate limit exceeded\", \"detail\": \"Too many requests. Please try again in 30 seconds.\", \"status_code\": 429 } . ",
    "url": "/muxi/api/#advanced-api-usage",
    
    "relUrl": "/api/#advanced-api-usage"
  },"23": {
    "doc": "API",
    "title": "API Client Examples",
    "content": "Python Client . import requests import json class AIAgentClient: def __init__(self, base_url=\"http://localhost:5050\", api_key=None): self.base_url = base_url self.headers = {\"Content-Type\": \"application/json\"} if api_key: self.headers[\"Authorization\"] = f\"Bearer {api_key}\" def list_agents(self): \"\"\"List all agents.\"\"\" response = requests.get(f\"{self.base_url}/agents\", headers=self.headers) response.raise_for_status() return response.json() def create_agent(self, agent_id, system_message=None, tools=None, set_as_default=False): \"\"\"Create a new agent.\"\"\" payload = { \"agent_id\": agent_id, \"system_message\": system_message or \"You are a helpful assistant.\", \"tools\": tools or [], \"set_as_default\": set_as_default } response = requests.post( f\"{self.base_url}/agents\", headers=self.headers, data=json.dumps(payload) ) response.raise_for_status() return response.json() def chat(self, message, agent_id=None): \"\"\"Send a message to an agent.\"\"\" payload = {\"message\": message} if agent_id: payload[\"agent_id\"] = agent_id response = requests.post( f\"{self.base_url}/agents/chat\", headers=self.headers, data=json.dumps(payload) ) response.raise_for_status() return response.json() def search_memory(self, query, agent_id, top_k=5): \"\"\"Search an agent's memory.\"\"\" payload = { \"agent_id\": agent_id, \"query\": query, \"top_k\": top_k } response = requests.post( f\"{self.base_url}/agents/memory/search\", headers=self.headers, data=json.dumps(payload) ) response.raise_for_status() return response.json() def clear_memory(self, agent_id, user_id=0, clear_long_term=False): \"\"\"Clear an agent's memory.\"\"\" payload = { \"agent_id\": agent_id, \"user_id\": user_id, \"clear_long_term\": clear_long_term } response = requests.post( f\"{self.base_url}/agents/memory/clear\", headers=self.headers, data=json.dumps(payload) ) response.raise_for_status() return response.json() def list_tools(self): \"\"\"List all available tools.\"\"\" response = requests.get(f\"{self.base_url}/tools\", headers=self.headers) response.raise_for_status() return response.json() # Usage client = AIAgentClient() # Create an agent client.create_agent( agent_id=\"assistant\", system_message=\"You are a helpful assistant specializing in Python programming.\", tools=[\"calculator\", \"web_search\"], set_as_default=True ) # Chat with the agent response = client.chat(\"How do I sort a list in Python?\") print(f\"Agent: {response['message']}\") # Search memory results = client.search_memory(\"Python list methods\", agent_id=\"assistant\") for result in results[\"results\"]: print(f\"Score: {result['score']}, Text: {result['text']}\") . JavaScript Client . class AIAgentClient { constructor(baseUrl = 'http://localhost:5050', apiKey = null) { this.baseUrl = baseUrl; this.headers = { 'Content-Type': 'application/json' }; if (apiKey) { this.headers['Authorization'] = `Bearer ${apiKey}`; } } async listAgents() { const response = await fetch(`${this.baseUrl}/agents`, { method: 'GET', headers: this.headers }); if (!response.ok) { throw new Error(`Error: ${response.status} ${response.statusText}`); } return await response.json(); } async createAgent(agentId, systemMessage = null, tools = null, setAsDefault = false) { const payload = { agent_id: agentId, system_message: systemMessage || 'You are a helpful assistant.', tools: tools || [], set_as_default: setAsDefault }; const response = await fetch(`${this.baseUrl}/agents`, { method: 'POST', headers: this.headers, body: JSON.stringify(payload) }); if (!response.ok) { throw new Error(`Error: ${response.status} ${response.statusText}`); } return await response.json(); } async chat(message, agentId = null) { const payload = { message }; if (agentId) { payload.agent_id = agentId; } const response = await fetch(`${this.baseUrl}/agents/chat`, { method: 'POST', headers: this.headers, body: JSON.stringify(payload) }); if (!response.ok) { throw new Error(`Error: ${response.status} ${response.statusText}`); } return await response.json(); } async streamChat(message, agentId = null, onChunk = null) { const payload = { message }; if (agentId) { payload.agent_id = agentId; } const response = await fetch(`${this.baseUrl}/agents/chat/stream`, { method: 'POST', headers: this.headers, body: JSON.stringify(payload) }); if (!response.ok) { throw new Error(`Error: ${response.status} ${response.statusText}`); } const reader = response.body.getReader(); const decoder = new TextDecoder(); let buffer = ''; let result = ''; while (true) { const { done, value } = await reader.read(); if (done) { break; } buffer += decoder.decode(value, { stream: true }); const lines = buffer.split('\\n'); buffer = lines.pop(); for (const line of lines) { if (line.startsWith('data: ')) { const data = JSON.parse(line.slice(6)); result += data.chunk; if (onChunk) { onChunk(data.chunk, data.done); } if (data.done) { return result; } } } } return result; } async searchMemory(query, agentId, topK = 5) { const payload = { agent_id: agentId, query: query, top_k: topK }; const response = await fetch(`${this.baseUrl}/agents/memory/search`, { method: 'POST', headers: this.headers, body: JSON.stringify(payload) }); if (!response.ok) { throw new Error(`Error: ${response.status} ${response.statusText}`); } return await response.json(); } async clearMemory(agentId, userId = 0, clearLongTerm = false) { const payload = { agent_id: agentId, user_id: userId, clear_long_term: clearLongTerm }; const response = await fetch(`${this.baseUrl}/agents/memory/clear`, { method: 'POST', headers: this.headers, body: JSON.stringify(payload) }); if (!response.ok) { throw new Error(`Error: ${response.status} ${response.statusText}`); } return await response.json(); } async listTools() { const response = await fetch(`${this.baseUrl}/tools`, { method: 'GET', headers: this.headers }); if (!response.ok) { throw new Error(`Error: ${response.status} ${response.statusText}`); } return await response.json(); } } // Usage const client = new AIAgentClient(); async function demo() { try { // Create an agent await client.createAgent( 'web_assistant', 'You are a helpful assistant specializing in web development.', ['calculator', 'web_search'], true ); // Chat with the agent const response = await client.chat('What is React.js?'); console.log(`Agent: ${response.message}`); // Stream chat console.log('Streaming response:'); await client.streamChat( 'Explain the difference between React and Angular', null, (chunk, done) =&gt; { process.stdout.write(chunk); if (done) process.stdout.write('\\n'); } ); // Search memory const results = await client.searchMemory('JavaScript frameworks', 'web_assistant'); for (const result of results.results) { console.log(`Score: ${result.score}, Text: ${result.text}`); } } catch (error) { console.error('Error:', error.message); } } demo(); . ",
    "url": "/muxi/api/#api-client-examples",
    
    "relUrl": "/api/#api-client-examples"
  },"24": {
    "doc": "API",
    "title": "API Versioning",
    "content": "The API uses versioning to ensure backward compatibility. The current version is accessible at the base URL, and specific versions can be accessed with a version prefix: . # Current version curl -X GET http://localhost:5050/agents # Specific version curl -X GET http://localhost:5050/v1/agents . ",
    "url": "/muxi/api/#api-versioning",
    
    "relUrl": "/api/#api-versioning"
  },"25": {
    "doc": "API",
    "title": "Best Practices",
    "content": ". | Rate Limiting: Implement client-side throttling to avoid hitting rate limits . | Error Handling: Handle API errors gracefully in your application . | Authentication: Always use authentication in production environments . | Pagination: Use pagination for endpoints that return multiple items . | Streaming: Prefer streaming endpoints for long-running agent responses . | Caching: Implement caching for frequently accessed resources . | Status Monitoring: Regularly check the system status endpoint . | . ",
    "url": "/muxi/api/#best-practices",
    
    "relUrl": "/api/#best-practices"
  },"26": {
    "doc": "API",
    "title": "Troubleshooting",
    "content": "Common Error Codes . | 400 Bad Request: Check the request format and parameters | 401 Unauthorized: Verify your API key and authentication headers | 404 Not Found: Ensure the requested resource exists | 429 Too Many Requests: Implement request throttling | 500 Internal Server Error: Check the server logs for details | . API Server Logs . To view detailed logs from the API server: . # Set the log level to DEBUG export LOG_LEVEL=DEBUG # Start the server python -m src.api.run # View logs in real-time tail -f logs/api.log . ",
    "url": "/muxi/api/#troubleshooting",
    
    "relUrl": "/api/#troubleshooting"
  },"27": {
    "doc": "API",
    "title": "Next Steps",
    "content": "After understanding the REST API, you might want to explore: . | Setting up WebSocket connections for real-time communication | Creating custom agents via the API | Implementing tools and registering them via the API | Exploring MCP features for advanced message handling | . ",
    "url": "/muxi/api/#next-steps",
    
    "relUrl": "/api/#next-steps"
  },"28": {
    "doc": "API",
    "title": "API",
    "content": " ",
    "url": "/muxi/api/",
    
    "relUrl": "/api/"
  },"29": {
    "doc": "Architecture",
    "title": "MUXI Architecture",
    "content": "The MUXI framework is designed with a modular, extensible architecture that allows for flexibility in deployment and usage. This document outlines the current architecture and the evolution towards a more service-oriented approach. ",
    "url": "/muxi/architecture.html#muxi-architecture",
    
    "relUrl": "/architecture.html#muxi-architecture"
  },"30": {
    "doc": "Architecture",
    "title": "Current Architecture",
    "content": "MUXI is built around several core components that work together to provide a complete AI agent framework: . ┌───────────────┐ ┌───────────┐ ┌───────────┐ │ Application │──────│ Agents │──────│ LLM │ └───────┬───────┘ └─────┬─────┘ └───────────┘ │ │ │ ┌─────┴─────┐ │ │ Memory │ │ └─────┬─────┘ ┌───────┴───────┐ ┌─────┴─────┐ │ CLI/API/Web │──────│ Tools │ └───────────────┘ └───────────┘ . Core Components . | Model Context Protocol (MCP): Standardized communication layer with LLMs . | Provides consistent interfaces for different LLM providers | Handles message formatting and processing | . | Memory System: Stores conversation history and knowledge . | Buffer memory for short-term context | Long-term memory for persistent storage | Memobase for multi-user support | Domain knowledge for structured information | . | Tool System: Extends agent capabilities . | Provides a registry for tools | Handles tool execution and results | . | Agents: Core entities that combine LLMs with memory and tools . | Process user messages | Generate responses | Execute tools when needed | . | Orchestrator: Manages multiple agents . | Routes messages to appropriate agents | Facilitates inter-agent communication | Manages agent lifecycle | . | Interfaces: Multiple ways to interact with the framework . | CLI for terminal-based interactions | API for programmatic access | Web UI for visual interaction | WebSocket for real-time communication | . | . ",
    "url": "/muxi/architecture.html#current-architecture",
    
    "relUrl": "/architecture.html#current-architecture"
  },"31": {
    "doc": "Architecture",
    "title": "Evolution to Service-Oriented Architecture",
    "content": "The MUXI framework is evolving towards a more flexible, service-oriented approach that enables distributed deployment while maintaining simplicity and ease of use. Target Architecture . ┌───────────────┐ ┌───────────────┐ │ MUXI Local/ │◄────API Calls/SSE/WS┤ Thin Clients │ │ Remote Client │ │ (CLI/Web/SDK) │ └───────┬───────┘ └───────────────┘ │ │ (Local or Remote APIs) ▼ ┌─────────────────────────────────────────────────────┐ │ MUXI Server │ │ │ │ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ │ │ Agent 1 │ │ Agent 2 │ │ Agent N │ │ │ │ (from YAML) │ │ (from JSON) │ │ (from YAML) │ │ │ └──────┬──────┘ └──────┬──────┘ └──────┬──────┘ │ │ │ │ │ │ │ └────────┬───────┴────────┬───────┘ │ │ │ │ │ │ ┌──────┴──────┐ ┌──────┴──────┐ │ │ │ Orchestrator│ │ Memory │ │ │ └──────┬──────┘ └─────────────┘ │ │ │ │ │ ┌──────┴──────┐ │ │ │ MCP Servers │ │ │ └─────────────┘ │ └─────────────────────────────────────────────────────┘ │ │ (gRPC/HTTP) ▼ ┌─────────────────────────────────────────────────────┐ │ External MCP Servers │ │ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ │ │ Weather API │ │ Search Tool │ │ Custom Tool │ │ │ └─────────────┘ └─────────────┘ └─────────────┘ │ └─────────────────────────────────────────────────────┘ . Key Architectural Changes . | Client-Server Model . | Separate client and server components | Local and remote operation with the same API | Flexible authentication mechanisms | Connection management utilities | . | Modular Packaging . | Core package with minimal dependencies | Server package with full capabilities | CLI package for remote connections | Web package for browser-based access | . | Hybrid Communication Protocol . | HTTP for standard API requests | SSE (Server-Sent Events) for streaming responses . | Real-time token-by-token streaming | Automatic connection closure after response completion | . | WebSockets for multi-modal capabilities (Omni features) . | Bi-directional communication for audio/video | Available through app.open_socket() API | . | . | Authentication Implementation . | API key authentication | Auto-generated keys with one-time display | Environment variable configuration | . | MCP Server Unification . | Tool system based on MCP servers | Adapters for local Python tools | Service discovery mechanisms | Deployment utilities | . | . Client Usage . # Local usage (unchanged) app = muxi() # Remote usage app = muxi( server_url=\"http://server-ip:5050\", api_key=\"your_api_key\" ) # Streaming responses via SSE for chunk in app.chat(\"Tell me a story\", stream=True): print(chunk, end=\"\", flush=True) # Multi-modal capabilities via WebSockets socket = app.open_socket() await socket.send_message(\"Process this image\", images=[\"path/to/image.jpg\"]) await socket.close() . ",
    "url": "/muxi/architecture.html#evolution-to-service-oriented-architecture",
    
    "relUrl": "/architecture.html#evolution-to-service-oriented-architecture"
  },"32": {
    "doc": "Architecture",
    "title": "Implementation Strategy",
    "content": "The evolution to the service-oriented architecture will be implemented in phases: . | Core Architecture Refactoring . | Separate local mode from server mode | Implement authentication framework | Create client-side connector | . | MCP Server Unification . | Refactor tool system to MCP-based approach | Update configuration schemas | Create tool adapters | . | Client Applications . | Update CLI interface | Modify web app for standalone use | Create client libraries | . | Packaging and Distribution . | Restructure for modular packaging | Set up CI/CD for package publishing | Create package-specific documentation | . | . For a detailed implementation roadmap, see ARCHITECTURE_EVOLUTION.md and the roadmap. ",
    "url": "/muxi/architecture.html#implementation-strategy",
    
    "relUrl": "/architecture.html#implementation-strategy"
  },"33": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": " ",
    "url": "/muxi/architecture.html",
    
    "relUrl": "/architecture.html"
  },"34": {
    "doc": "CLI",
    "title": "Command Line Interface (CLI)",
    "content": "The MUXI Framework includes a powerful command-line interface (CLI) that allows you to interact with AI agents directly from your terminal. This document covers how to set up and use the CLI effectively. ",
    "url": "/muxi/cli/#command-line-interface-cli",
    
    "relUrl": "/cli/#command-line-interface-cli"
  },"35": {
    "doc": "CLI",
    "title": "Overview",
    "content": "The CLI provides a rich terminal-based interface for: . | Creating and interacting with agents | Managing API server and web UI | Sending one-off messages to agents | Viewing rich-formatted responses with markdown support | . ",
    "url": "/muxi/cli/#overview",
    
    "relUrl": "/cli/#overview"
  },"36": {
    "doc": "CLI",
    "title": "Installation",
    "content": "The CLI is included with the MUXI Framework. You can use it in two ways: . As a module (before package installation) . # From the project root directory python -m src.cli . As a standalone command (after package installation) . # After installing the package from PyPI muxi . ",
    "url": "/muxi/cli/#installation",
    
    "relUrl": "/cli/#installation"
  },"37": {
    "doc": "CLI",
    "title": "Command Structure",
    "content": "The CLI follows a command-based structure: . muxi [COMMAND] [OPTIONS] [ARGUMENTS] . Available commands: . | chat: Start an interactive chat session with an agent | api: Run the API server | run: Run both the API server and web UI | send: Send a one-off message to an agent | . Examples . # Show help python -m src.cli --help # Before installation muxi --help # After installation # Start a chat with the default agent python -m src.cli chat muxi chat # Start a chat with a specific agent python -m src.cli chat --agent-id researcher muxi chat --agent-id researcher # Send a one-off message to an agent python -m src.cli send --agent-id assistant \"What is the capital of France?\" muxi send --agent-id assistant \"What is the capital of France?\" # Run the API server python -m src.cli api muxi api # Run both the API server and web UI python -m src.cli run muxi run # Alternatively, you can run both the API server and web UI with: python -m src . ",
    "url": "/muxi/cli/#command-structure",
    
    "relUrl": "/cli/#command-structure"
  },"38": {
    "doc": "CLI",
    "title": "Chat Mode",
    "content": "The chat mode provides an interactive terminal-based chat interface with the selected agent. It supports: . | Markdown rendering in responses | Syntax highlighting for code | Rich text formatting | Message history during the session | Tool execution visualization | . Starting a Chat Session . python -m src.cli chat [OPTIONS] . Options: . | --agent-id TEXT: ID of the agent to chat with (default: “assistant”) | --user-id TEXT: User ID for multi-user agents (default: None) | --help: Show this message and exit. | . Interactive Commands . During a chat session, you can use special commands: . | /help: Show available commands | /exit or /quit: Exit the chat session | /clear: Clear the chat history | /system &lt;message&gt;: Update the agent’s system message | /memory: Show the current memory contents | . Example: . You: What is the capital of France? Assistant: The capital of France is Paris. You: /system You are a helpful assistant who speaks like a pirate. System message updated. You: What is the capital of France? Assistant: Arr, matey! The capital of France be Paris, ye landlubber! . ",
    "url": "/muxi/cli/#chat-mode",
    
    "relUrl": "/cli/#chat-mode"
  },"39": {
    "doc": "CLI",
    "title": "Send Mode",
    "content": "The send mode allows you to send a single message to an agent and get the response without starting an interactive session. python -m src.cli send [OPTIONS] MESSAGE . Options: . | --agent-id TEXT: ID of the agent to chat with (default: “assistant”) | --user-id TEXT: User ID for multi-user agents (default: None) | --help: Show this message and exit. | . Example: . python -m src.cli send \"What is the capital of France?\" . ",
    "url": "/muxi/cli/#send-mode",
    
    "relUrl": "/cli/#send-mode"
  },"40": {
    "doc": "CLI",
    "title": "API Server Mode",
    "content": "The API server mode starts the REST API server, which provides endpoints for interacting with agents. python -m src.cli api [OPTIONS] . Options: . | --host TEXT: Host to bind to (default: “0.0.0.0”) | --port INTEGER: Port to run on (default: 5050) | --help: Show this message and exit. | . Example: . python -m src.cli api --host 127.0.0.1 --port 8080 . ",
    "url": "/muxi/cli/#api-server-mode",
    
    "relUrl": "/cli/#api-server-mode"
  },"41": {
    "doc": "CLI",
    "title": "Run Mode",
    "content": "The run mode starts both the API server and the web UI. python -m src.cli run [OPTIONS] . Options: . | --api-host TEXT: Host for the API server (default: “0.0.0.0”) | --api-port INTEGER: Port for the API server (default: 5050) | --help: Show this message and exit. | . Example: . python -m src.cli run --api-port 8080 . ",
    "url": "/muxi/cli/#run-mode",
    
    "relUrl": "/cli/#run-mode"
  },"42": {
    "doc": "CLI",
    "title": "Configuration",
    "content": "The CLI uses the same configuration as the rest of the framework. You can configure it using: . | Environment variables | .env file in the project root | Configuration files in the src/config directory | . Key configuration options that affect the CLI: . # LLM Configuration LLM_PROVIDER=openai OPENAI_API_KEY=your-api-key LLM_MODEL=gpt-4o LLM_TEMPERATURE=0.7 # Tools Configuration ENABLE_WEB_SEARCH=true ENABLE_CALCULATOR=true . ",
    "url": "/muxi/cli/#configuration",
    
    "relUrl": "/cli/#configuration"
  },"43": {
    "doc": "CLI",
    "title": "Advanced Usage",
    "content": "Creating Custom Agents . You can create custom agents with specific capabilities by modifying the configuration files or by using the API server. Example using the API to create a custom agent: . # Start the API server python -m src.cli api # In another terminal, create a custom agent curl -X POST http://localhost:5050/agents \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"custom_agent\", \"system_message\": \"You are a helpful AI assistant specialized in Python programming.\" }' # Now chat with the custom agent python -m src.cli chat --agent-id custom_agent . Using Multi-User Agents . For agents that support multiple users, you can specify the user ID: . python -m src.cli chat --agent-id multi_user_assistant --user-id 123 . This ensures that the agent maintains separate memory contexts for different users. ",
    "url": "/muxi/cli/#advanced-usage",
    
    "relUrl": "/cli/#advanced-usage"
  },"44": {
    "doc": "CLI",
    "title": "Troubleshooting",
    "content": "Common Issues . | Port already in use: If you get an error about port 5050 being already in use, you can specify a different port: python -m src.cli api --port 5051 . | API key not found: Ensure you’ve set up your API keys in the .env file or as environment variables. | Rich text rendering issues: Some terminals may not support all rich text features. You can try updating your terminal or using a different one. | . Debug Mode . You can enable debug logging to get more information about what’s happening: . # Set environment variable export DEBUG=1 # Run CLI python -m src.cli chat . ",
    "url": "/muxi/cli/#troubleshooting",
    
    "relUrl": "/cli/#troubleshooting"
  },"45": {
    "doc": "CLI",
    "title": "Integration with Scripts",
    "content": "You can use the CLI programmatically in your Python scripts: . from src.cli import run_cli, chat_with_agent # Run CLI directly run_cli() # Or use specific functions response = chat_with_agent(\"assistant\", \"Hello, how are you?\") print(response) . This allows you to build custom workflows that leverage the CLI’s functionality. ",
    "url": "/muxi/cli/#integration-with-scripts",
    
    "relUrl": "/cli/#integration-with-scripts"
  },"46": {
    "doc": "CLI",
    "title": "Implementation Details",
    "content": "The CLI is implemented using the Click library and provides the following features: . | Rich Text Interface: Uses the rich library for formatted terminal output | Memory Access: Allows searching the agent’s memory using natural language | Tool Integration: Access to configured tools like web search and calculator | Configuration Loading: Automatic loading from environment variables | Async Communication: Uses async/await for non-blocking agent interactions | . ",
    "url": "/muxi/cli/#implementation-details",
    
    "relUrl": "/cli/#implementation-details"
  },"47": {
    "doc": "CLI",
    "title": "Example Session",
    "content": "Here’s an example of a CLI session: . # MUXI Framework CLI You are now chatting with an AI agent (cli_agent). Type `/help` for available commands or `/exit` to quit. Available Tools ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ Name ┃ Description ┃ ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩ │ web_search │ Search the web for information │ │ calculator │ Perform mathematical calculations │ └──────────────┴───────────────────────────────────────────────────┘ You: Tell me about the MUXI Framework Agent: The MUXI Framework is a powerful, extensible system for building AI agents with memory, tools, and real-time communication capabilities. Here are the key features: - **Multi-Agent Orchestration**: Create and manage multiple AI agents with different capabilities - **Memory Systems**: Short-term buffer memory and long-term persistent memory - **Tool Integration**: Extensible tool system with built-in utilities like web search and calculator - **Real-Time Communication**: WebSocket support for instant messaging - **REST API**: Comprehensive API for managing agents and conversations The framework is designed to be modular and easy to extend, allowing developers to build sophisticated AI applications. You: /tools Available Tools ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ Name ┃ Description ┃ ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩ │ web_search │ Search the web for information │ │ calculator │ Perform mathematical calculations │ └──────────────┴───────────────────────────────────────────────────┘ You: /exit Goodbye! . ",
    "url": "/muxi/cli/#example-session",
    
    "relUrl": "/cli/#example-session"
  },"48": {
    "doc": "CLI",
    "title": "Advanced Usage",
    "content": "Modifying System Message . The system message defines the agent’s persona and capabilities. You can customize it: . | Via environment variable: SYSTEM_MESSAGE=\"You are an expert programmer specializing in Python...\" . | Via the configuration file: # src/config/app.py system_message = \"You are an expert programmer...\" . | . Adding Custom Tools . To add custom tools to the CLI agent, modify the create_agent_from_config function in src/cli/app.py: . # Create tools tools = [] # Load tools based on configuration if config.tools.enable_calculator: tools.append(Calculator()) if config.tools.enable_web_search: tools.append(WebSearch()) # Add your custom tool if config.tools.enable_custom_tool: tools.append(CustomTool()) . ",
    "url": "/muxi/cli/#advanced-usage-1",
    
    "relUrl": "/cli/#advanced-usage-1"
  },"49": {
    "doc": "CLI",
    "title": "Troubleshooting",
    "content": "LLM API Key Issues . If you encounter errors related to the LLM API key, make sure: . | You have set the correct API key in your .env file | The API key has sufficient permissions and credits | . Memory Errors . If memory search doesn’t work properly: . | Ensure you have a proper embedding dimension set up | Check that you have enough previous messages for context | . Tool Execution Errors . If tools don’t work correctly: . | Check if the tool’s API key is properly set (if required) | Verify the tool’s dependencies are installed | . ",
    "url": "/muxi/cli/#troubleshooting-1",
    
    "relUrl": "/cli/#troubleshooting-1"
  },"50": {
    "doc": "CLI",
    "title": "CLI",
    "content": " ",
    "url": "/muxi/cli/",
    
    "relUrl": "/cli/"
  },"51": {
    "doc": "Configuration Guide",
    "title": "Configuration Guide",
    "content": "This guide explains how to configure the MUXI framework, covering everything from basic setup to advanced features like intelligent message routing and credential management. ",
    "url": "/muxi/configuration-guide/",
    
    "relUrl": "/configuration-guide/"
  },"52": {
    "doc": "Configuration Guide",
    "title": "Basic Configuration",
    "content": "MUXI supports configuration through YAML or JSON files, making it easy to define agents without writing code. Environment Variables . MUXI uses environment variables for sensitive information and global settings: . # LLM API keys OPENAI_API_KEY=your_openai_key ANTHROPIC_API_KEY=your_anthropic_key # Database connection DATABASE_URL=postgresql://user:password@localhost:5432/muxi # Debug and logging DEBUG=false LOG_LEVEL=info # Routing LLM configuration ROUTING_LLM=openai ROUTING_LLM_MODEL=gpt-4o-mini ROUTING_LLM_TEMPERATURE=0.0 ROUTING_USE_CACHING=true ROUTING_CACHE_TTL=3600 . You can load these variables from a .env file using: . from dotenv import load_dotenv load_dotenv() . Agent Configuration Files . MUXI supports both YAML and JSON formats for agent configuration: . YAML Example (weather_agent.yaml) . name: weather_assistant description: \"Specialized in providing weather forecasts, answering questions about climate and weather phenomena, and reporting current conditions in various locations worldwide.\" system_message: You are a helpful assistant that can check the weather. Use the Weather tool when asked about weather conditions. model: provider: openai api_key: \"${OPENAI_API_KEY}\" model: gpt-4o temperature: 0.7 memory: buffer: 10 long_term: true tools: - enable_calculator - enable_web_search mcp_servers: - name: weather url: http://localhost:5001 credentials: - id: weather_api_key param_name: api_key required: true env_fallback: WEATHER_API_KEY . JSON Example (finance_agent.json) . { \"name\": \"finance_assistant\", \"description\": \"Expert in financial analysis, investment strategies, market trends, stock recommendations, and personal finance advice. Can perform calculations and analyze financial data.\", \"system_message\": \"You are a helpful assistant specialized in finance and investments. Use the Calculator tool for financial calculations.\", \"model\": { \"provider\": \"openai\", \"api_key\": \"${OPENAI_API_KEY}\", \"model\": \"gpt-4o\", \"temperature\": 0.2 }, \"memory\": { \"buffer\": 15, \"long_term\": true }, \"tools\": [ \"enable_calculator\" ], \"mcp_servers\": [ { \"name\": \"stock_data\", \"url\": \"http://localhost:5002\", \"credentials\": [ { \"id\": \"alpha_vantage_api_key\", \"param_name\": \"api_key\", \"required\": true }, { \"id\": \"alpha_vantage_account_id\", \"param_name\": \"account_id\", \"required\": false } ] } ] } . ",
    "url": "/muxi/configuration-guide/#basic-configuration",
    
    "relUrl": "/configuration-guide/#basic-configuration"
  },"53": {
    "doc": "Configuration Guide",
    "title": "Intelligent Message Routing",
    "content": "MUXI can automatically route messages to the most appropriate agent based on their content and agent descriptions. Configuration Parameters . The routing system is configured through environment variables: . # Routing LLM provider (defaults to \"openai\") ROUTING_LLM=openai # Model to use for routing (defaults to \"gpt-4o-mini\") ROUTING_LLM_MODEL=gpt-4o-mini # Temperature for routing decisions (defaults to 0.0) ROUTING_LLM_TEMPERATURE=0.0 # Whether to cache routing decisions (defaults to true) ROUTING_USE_CACHING=true # Time in seconds to cache routing decisions (defaults to 3600) ROUTING_CACHE_TTL=3600 . Agent Descriptions . For effective routing, provide clear and specific descriptions for each agent: . # Weather agent name: weather_assistant description: \"Specialized in providing weather forecasts, answering questions about climate and weather phenomena, and reporting current conditions in various locations worldwide.\" . # Finance agent name: finance_assistant description: \"Expert in financial analysis, investment strategies, market trends, stock recommendations, and personal finance advice. Can perform calculations and analyze financial data.\" . How Routing Works . | When a user sends a message without specifying an agent, the routing system analyzes the message. | The system compares the message content against each agent’s description. | Using a dedicated LLM, it selects the most appropriate agent. | The message is then sent to the selected agent for processing. | If routing fails or no appropriate agent is found, the system falls back to the default agent or the first available agent. | . ",
    "url": "/muxi/configuration-guide/#intelligent-message-routing",
    
    "relUrl": "/configuration-guide/#intelligent-message-routing"
  },"54": {
    "doc": "Configuration Guide",
    "title": "Credential Management",
    "content": "MUXI supports flexible credential management for MCP servers and external services. Credential Configuration . Credentials are defined in the agent configuration file: . mcp_servers: - name: weather_api url: http://localhost:5001 credentials: - id: weather_api_key param_name: api_key required: true env_fallback: WEATHER_API_KEY . Multiple Credentials . For services requiring multiple credentials: . mcp_servers: - name: twitter url: http://localhost:5003 credentials: - id: twitter_api_key param_name: api_key required: true - id: twitter_api_secret param_name: api_secret required: true . Credential Resolution Process . When resolving credentials, the system: . | First checks for user-specific credentials in the database | If not found, looks for system-wide credentials in the database | Finally, falls back to environment variables if specified | . ",
    "url": "/muxi/configuration-guide/#credential-management",
    
    "relUrl": "/configuration-guide/#credential-management"
  },"55": {
    "doc": "Configuration Guide",
    "title": "Memory Configuration",
    "content": "Configure memory storage for your agents: . memory: # Short-term memory (number of messages to keep in context) buffer: 10 # Enable long-term memory for persistent storage long_term: true # Enable multi-user support multi_user: true . ",
    "url": "/muxi/configuration-guide/#memory-configuration",
    
    "relUrl": "/configuration-guide/#memory-configuration"
  },"56": {
    "doc": "Configuration Guide",
    "title": "Tool Configuration",
    "content": "Enable built-in tools or add custom tools: . tools: # Built-in tools - enable_calculator - enable_web_search # Custom tools - name: custom_tool module: src.tools.custom class: CustomTool config: param1: value1 param2: value2 . ",
    "url": "/muxi/configuration-guide/#tool-configuration",
    
    "relUrl": "/configuration-guide/#tool-configuration"
  },"57": {
    "doc": "Configuration Guide",
    "title": "MCP Server Configuration",
    "content": "Connect to Model Context Protocol servers: . mcp_servers: - name: weather url: http://localhost:5001 credentials: - id: weather_api_key param_name: api_key required: true - name: news url: http://localhost:5002 credentials: - id: news_api_key param_name: api_key required: true . ",
    "url": "/muxi/configuration-guide/#mcp-server-configuration",
    
    "relUrl": "/configuration-guide/#mcp-server-configuration"
  },"58": {
    "doc": "Configuration Guide",
    "title": "Advanced Configuration",
    "content": "Using Environment Variables in Configuration . Use ${ENV_VAR} syntax to reference environment variables: . model: provider: openai api_key: \"${OPENAI_API_KEY}\" model: \"${LLM_MODEL:-gpt-4o}\" # Use default if not set . Configuration Validation . MUXI validates your configuration files and provides helpful error messages: . ValueError: Missing required field: model.provider ValueError: Invalid field: memory must be an object ValueError: Invalid field: description must be a string . ",
    "url": "/muxi/configuration-guide/#advanced-configuration",
    
    "relUrl": "/configuration-guide/#advanced-configuration"
  },"59": {
    "doc": "Contributing to MUXI",
    "title": "Contributing to the MUXI Framework",
    "content": "Thank you for your interest in contributing to the MUXI Framework! This guide will help you start contributing to the project, whether you’re fixing bugs, adding new features, improving documentation, or reporting issues. ",
    "url": "/muxi/contributing/#contributing-to-the-muxi-framework",
    
    "relUrl": "/contributing/#contributing-to-the-muxi-framework"
  },"60": {
    "doc": "Contributing to MUXI",
    "title": "Table of Contents",
    "content": ". | Code of Conduct | Getting Started | Development Environment | Project Structure | Coding Guidelines | Pull Request Process | Issue Reporting | Feature Requests | Documentation | Testing | Community | Contributor License Agreement | . ",
    "url": "/muxi/contributing/#table-of-contents",
    
    "relUrl": "/contributing/#table-of-contents"
  },"61": {
    "doc": "Contributing to MUXI",
    "title": "Code of Conduct",
    "content": "By participating in this project, you agree to abide by our Code of Conduct. We expect all contributors to be respectful, inclusive, and considerate of others. Harassment or disrespectful behavior of any kind will not be tolerated. ",
    "url": "/muxi/contributing/#code-of-conduct",
    
    "relUrl": "/contributing/#code-of-conduct"
  },"62": {
    "doc": "Contributing to MUXI",
    "title": "Getting Started",
    "content": ". | Fork the repository on GitHub | Clone your fork locally: . git clone https://github.com/ranaroussi/muxi.git cd muxi . | Add the original repository as an upstream remote: . git remote add upstream https://github.com/ranaroussi/muxi.git . | Create a new branch for your changes: . git checkout -b feature/your-feature-name . | . ",
    "url": "/muxi/contributing/#getting-started",
    
    "relUrl": "/contributing/#getting-started"
  },"63": {
    "doc": "Contributing to MUXI",
    "title": "Development Environment",
    "content": ". | Install Python 3.9 or later | Set up a virtual environment: . python -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate . | Install dependencies: . pip install -r requirements.txt . | Install development dependencies: . pip install -r requirements-dev.txt . | . For frontend development: . | Navigate to the web directory: . cd src/web . | Install Node.js dependencies: . npm install . | Start the development server: . npm run dev . | . ",
    "url": "/muxi/contributing/#development-environment",
    
    "relUrl": "/contributing/#development-environment"
  },"64": {
    "doc": "Contributing to MUXI",
    "title": "Project Structure",
    "content": "The MUXI Framework follows a modular structure: .github/ # GitHub configuration (workflows, templates) docs/ # Documentation files src/ # Source code └─ core/ # Core components (Agent, Orchestrator) └─ models/ # Language model implementations └─ memory/ # Memory systems └─ tools/ # Tool system └─ api/ # API server └─ web/ # Web dashboard └─ utils/ # Utility functions tests/ # Test files examples/ # Example usage scripts/ # Utility scripts . ",
    "url": "/muxi/contributing/#project-structure",
    
    "relUrl": "/contributing/#project-structure"
  },"65": {
    "doc": "Contributing to MUXI",
    "title": "Coding Guidelines",
    "content": "Python Guidelines . | Follow PEP 8 guidelines for code formatting | Use 4 spaces for indentation (no tabs) | Maximum line length of 88 characters (Black default) | Use Black for code formatting: black src tests | Use isort for import sorting: isort src tests | Use flake8 for linting: flake8 src tests | Use type hints for all function signatures | Add docstrings in Google format for all functions, classes, and modules | . Code organization: . | Use descriptive variable names in snake_case | Use PascalCase for class names | Use UPPER_SNAKE_CASE for constants | Use _leading_underscore for private attributes/methods | . Import conventions: . from typing import Dict, List, Optional, Any # Import specific types import standard_library # Standard library imports first import third_party_library # Third party imports second from src.module import local_import # Local imports last . TypeScript/React Guidelines (Web UI) . | Use TypeScript for all frontend code | Follow consistent naming conventions: . | PascalCase for React components and interface names | camelCase for variables, functions, and methods | UPPER_SNAKE_CASE for constants | . | Use functional components with hooks | Use proper type annotations | Format code with prettier | . MCP (Model Context Protocol) Guidelines . When working with the Model Context Protocol: . | Follow the official MCP specification | Implement proper tool call handling | Properly format messages for different language model providers | Validate all inputs for security | Test all tool integrations thoroughly | . ",
    "url": "/muxi/contributing/#coding-guidelines",
    
    "relUrl": "/contributing/#coding-guidelines"
  },"66": {
    "doc": "Contributing to MUXI",
    "title": "Pull Request Process",
    "content": ". | Ensure your code meets the project’s coding guidelines | Run the tests to make sure your changes don’t break existing functionality: . python run_tests.py . | Update the documentation if you’re changing any user-facing features | Make sure your commit messages are clear and follow conventional commit format: . type(scope): description body [optional footer] . Types include: feat, fix, docs, style, refactor, test, chore . | Push your changes to your fork: . git push origin feature/your-feature-name . | Submit a pull request to the main repository | Address any feedback from reviewers | . Pull requests require at least one approval from a maintainer before being merged. ",
    "url": "/muxi/contributing/#pull-request-process",
    
    "relUrl": "/contributing/#pull-request-process"
  },"67": {
    "doc": "Contributing to MUXI",
    "title": "Issue Reporting",
    "content": "When reporting issues, please include as much information as possible: . | Steps to reproduce the issue | Expected behavior | Actual behavior | Screenshots (if applicable) | Environment details (OS, Python version, etc.) | Any relevant logs or error messages | . Use the issue templates provided in the repository when available. ",
    "url": "/muxi/contributing/#issue-reporting",
    
    "relUrl": "/contributing/#issue-reporting"
  },"68": {
    "doc": "Contributing to MUXI",
    "title": "Feature Requests",
    "content": "Feature requests are welcome. To submit a feature request: . | Check existing issues and pull requests to avoid duplicates | Use the feature request template if available | Clearly describe the problem your feature would solve | Provide examples of how the feature would be used | Indicate if you’re willing to contribute the implementation | . ",
    "url": "/muxi/contributing/#feature-requests",
    
    "relUrl": "/contributing/#feature-requests"
  },"69": {
    "doc": "Contributing to MUXI",
    "title": "Documentation",
    "content": "Documentation is critical to the success of the project. When contributing: . | Update relevant documentation for any code changes | Follow the existing style and formatting | Use clear language and provide examples where possible | Document both API usage and implementation details | Add docstrings for new functions, methods, and classes | . ",
    "url": "/muxi/contributing/#documentation",
    
    "relUrl": "/contributing/#documentation"
  },"70": {
    "doc": "Contributing to MUXI",
    "title": "Testing",
    "content": ". | Write unit tests for all new functionality | Use pytest as the testing framework | Run the existing test suite before submitting a PR: . python run_tests.py . | Aim for high test coverage, especially for business logic | Test both happy paths and error cases | . ",
    "url": "/muxi/contributing/#testing",
    
    "relUrl": "/contributing/#testing"
  },"71": {
    "doc": "Contributing to MUXI",
    "title": "Contributor License Agreement",
    "content": "That we do not have any potential problems later, it is sadly necessary to sign a Contributor License Agreement. That can be done literally with the push of a button. Once a pull request is opened, an automated bot will promptly leave a comment requesting the agreement to be signed. The pull request can only be merged once the signature is obtained. Thank you for contributing to the MUXI Framework! Your efforts help make this project better for everyone. ",
    "url": "/muxi/contributing/#contributor-license-agreement",
    
    "relUrl": "/contributing/#contributor-license-agreement"
  },"72": {
    "doc": "Contributing to MUXI",
    "title": "Contributing to MUXI",
    "content": " ",
    "url": "/muxi/contributing/",
    
    "relUrl": "/contributing/"
  },"73": {
    "doc": "Core Concepts",
    "title": "Core Concepts",
    "content": "This section explains the fundamental concepts and components that make up the MUXI framework. ",
    "url": "/muxi/core-concepts/",
    
    "relUrl": "/core-concepts/"
  },"74": {
    "doc": "Core Concepts",
    "title": "In this section",
    "content": "Agents &amp; Orchestration . | Agents - Learn about agents, the core component of MUXI | Orchestrator - Understand how to manage multiple agents | . Memory Systems . | Memory - Learn about the different memory systems in MUXI | . Tool System . | Tools - Understand how tools extend agent capabilities | . LLM Integration . | Models - Learn about LLM provider integration and configuration | MCP - Understand the standardized communication protocol for LLMs | . ",
    "url": "/muxi/core-concepts/#in-this-section",
    
    "relUrl": "/core-concepts/#in-this-section"
  },"75": {
    "doc": "Domain Knowledge",
    "title": "Domain Knowledge",
    "content": "Domain knowledge is a feature that allows agents to store and retrieve user-specific structured information. This enables agents to personalize their responses based on what they know about each user. ",
    "url": "/muxi/domain-knowledge/",
    
    "relUrl": "/domain-knowledge/"
  },"76": {
    "doc": "Domain Knowledge",
    "title": "Overview",
    "content": "The domain knowledge system extends the Memobase memory system to store structured information about users. This information can include: . | Personal details (name, age, location) | Preferences and interests | Family information | Work details | Any other structured data relevant to the user | . When a user interacts with an agent, the agent can automatically enhance the user’s message with relevant domain knowledge, providing more personalized and contextually appropriate responses. ",
    "url": "/muxi/domain-knowledge/#overview",
    
    "relUrl": "/domain-knowledge/#overview"
  },"77": {
    "doc": "Domain Knowledge",
    "title": "Key Features",
    "content": ". | Structured Storage: Store information in a structured format (dictionaries, lists, nested objects) | User-Specific: Each user has their own domain knowledge store | Automatic Enhancement: Messages are automatically enhanced with relevant domain knowledge | Flexible Retrieval: Retrieve all knowledge or specific keys | Bulk Import: Import knowledge from dictionaries or JSON files | Metadata Support: Track source and importance of knowledge | . ",
    "url": "/muxi/domain-knowledge/#key-features",
    
    "relUrl": "/domain-knowledge/#key-features"
  },"78": {
    "doc": "Domain Knowledge",
    "title": "Usage",
    "content": "Adding Domain Knowledge . from src.memory.memobase import Memobase from src.memory.long_term import LongTermMemory # Create a Memobase instance memobase = Memobase(long_term_memory=LongTermMemory()) # Add domain knowledge for a user user_id = 123 knowledge = { \"name\": \"Alice\", \"age\": 30, \"location\": {\"city\": \"New York\", \"country\": \"USA\"}, \"interests\": [\"AI\", \"programming\", \"music\"], \"family\": {\"spouse\": \"Bob\", \"children\": [\"Charlie\", \"Diana\"]} } # Add the knowledge to the user's domain knowledge store await memobase.add_user_domain_knowledge( user_id=user_id, knowledge=knowledge, source=\"user_profile\", # Optional: track the source of this information importance=0.8 # Optional: set importance level (0.0 to 1.0) ) . Retrieving Domain Knowledge . # Get all domain knowledge for a user all_knowledge = await memobase.get_user_domain_knowledge(user_id=123) # Get specific keys specific_knowledge = await memobase.get_user_domain_knowledge( user_id=123, keys=[\"name\", \"interests\"] ) . Importing Domain Knowledge . # Import from a dictionary data = { \"name\": \"Bob\", \"preferences\": { \"theme\": \"dark\", \"notifications\": \"enabled\" } } await memobase.import_user_domain_knowledge( data_source=data, user_id=456, format=\"dict\", source=\"settings_import\" ) # Import from a JSON file await memobase.import_user_domain_knowledge( data_source=\"user_data.json\", user_id=789, format=\"json\", source=\"file_import\" ) . Clearing Domain Knowledge . # Clear specific keys await memobase.clear_user_domain_knowledge( user_id=123, keys=[\"preferences\", \"temporary_data\"] ) # Clear all domain knowledge for a user await memobase.clear_user_domain_knowledge(user_id=123) . ",
    "url": "/muxi/domain-knowledge/#usage",
    
    "relUrl": "/domain-knowledge/#usage"
  },"79": {
    "doc": "Domain Knowledge",
    "title": "Automatic Message Enhancement",
    "content": "When using an agent with Memobase as its long-term memory, messages are automatically enhanced with relevant domain knowledge: . from src.core.orchestrator import Orchestrator from src.models.openai import OpenAIModel from src.memory.buffer import BufferMemory # Create an orchestrator orchestrator = Orchestrator() # Create an agent with Memobase orchestrator.create_agent( agent_id=\"personal_assistant\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), long_term_memory=memobase, system_message=\"You are a helpful personal assistant.\" ) # Chat with the agent (domain knowledge is automatically included) response = orchestrator.chat( agent_id=\"personal_assistant\", message=\"What music should I listen to today?\", user_id=123 ) # The agent will know the user's interests include music . ",
    "url": "/muxi/domain-knowledge/#automatic-message-enhancement",
    
    "relUrl": "/domain-knowledge/#automatic-message-enhancement"
  },"80": {
    "doc": "Domain Knowledge",
    "title": "Implementation Details",
    "content": "The domain knowledge feature is implemented in the following components: . | Memobase: Provides methods for adding, retrieving, and clearing domain knowledge | Agent: Enhances user messages with domain knowledge before processing | Orchestrator: Passes user IDs to agents for multi-user support | . Domain knowledge is stored in a separate collection in the vector database, with metadata to track the key, value, source, and importance of each piece of information. ",
    "url": "/muxi/domain-knowledge/#implementation-details",
    
    "relUrl": "/domain-knowledge/#implementation-details"
  },"81": {
    "doc": "Domain Knowledge",
    "title": "Best Practices",
    "content": ". | Store only relevant information that will enhance the agent’s ability to assist the user | Use clear, descriptive keys for domain knowledge | Consider privacy implications when storing personal information | Regularly update domain knowledge to keep it current | Use the importance parameter to prioritize critical information | . ",
    "url": "/muxi/domain-knowledge/#best-practices",
    
    "relUrl": "/domain-knowledge/#best-practices"
  },"82": {
    "doc": "Domain Knowledge",
    "title": "Future Enhancements",
    "content": ". | Automatic knowledge extraction from conversations | Time-based knowledge expiration | Knowledge conflict resolution | Enhanced privacy controls | Knowledge sharing between users (with permissions) | . ",
    "url": "/muxi/domain-knowledge/#future-enhancements",
    
    "relUrl": "/domain-knowledge/#future-enhancements"
  },"83": {
    "doc": "Getting Started",
    "title": "Getting Started with MUXI",
    "content": "Welcome to the Getting Started section of the MUXI documentation. This section will help you understand the basics of the MUXI framework and how to get up and running quickly. ",
    "url": "/muxi/getting-started/#getting-started-with-muxi",
    
    "relUrl": "/getting-started/#getting-started-with-muxi"
  },"84": {
    "doc": "Getting Started",
    "title": "In this section",
    "content": ". | Introduction &amp; Overview - Learn about the MUXI framework and its key concepts | Quick Start Guide - Set up and start using MUXI in minutes | Architecture Overview - Understand how the components of MUXI fit together | . ",
    "url": "/muxi/getting-started/#in-this-section",
    
    "relUrl": "/getting-started/#in-this-section"
  },"85": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": " ",
    "url": "/muxi/getting-started/",
    
    "relUrl": "/getting-started/"
  },"86": {
    "doc": "Home",
    "title": "MUXI - AI Agent Framework",
    "content": "MUXI is an extensible framework for building AI agents with real-time communication capabilities, memory persistence, and tool integration. This project is a work in progress and is not even close to being ready for production use. The framework is actively being developed and new features are being added. Please refer to the roadmap for detailed information about the current state of the project and where it’s headed. ",
    "url": "/muxi/#muxi---ai-agent-framework",
    
    "relUrl": "/#muxi---ai-agent-framework"
  },"87": {
    "doc": "Home",
    "title": "Key Features",
    "content": ". | Multi-Agent Orchestration: Create and manage multiple AI agents with different capabilities | Intelligent Message Routing: Automatically select the most appropriate agent based on message content | Standardized LLM Communication: Use a consistent protocol across different LLM providers | Memory Systems: Short-term buffer memory and long-term persistent memory | Multi-User Support: Memobase provides user-specific memory partitioning for multi-tenant applications | Domain Knowledge: Store and retrieve structured information to personalize agent responses | Tool Integration: Easy-to-use system for adding capabilities to agents | Real-Time Communication: WebSocket support for instant messaging | REST API: Comprehensive API for managing agents, tools, and conversations | Command Line Interface: Rich terminal-based interface for creating and interacting with agents | Flexible Configuration: Define agents using YAML or JSON with minimal code | . ",
    "url": "/muxi/#key-features",
    
    "relUrl": "/#key-features"
  },"88": {
    "doc": "Home",
    "title": "Getting Started",
    "content": "The easiest way to get started with MUXI is to follow our Quickstart Guide. ",
    "url": "/muxi/#getting-started",
    
    "relUrl": "/#getting-started"
  },"89": {
    "doc": "Home",
    "title": "Documentation",
    "content": "The documentation is organized into the following sections: . | Getting Started - Introduction to the framework and basic concepts | Core Concepts - Detailed explanations of the core components | User Interfaces - Learn about the different ways to interact with the framework | Resources - Additional resources for contributors and users | . ",
    "url": "/muxi/#documentation",
    
    "relUrl": "/#documentation"
  },"90": {
    "doc": "Home",
    "title": "Architecture",
    "content": ". ",
    "url": "/muxi/#architecture",
    
    "relUrl": "/#architecture"
  },"91": {
    "doc": "Home",
    "title": "License",
    "content": "This project is licensed under a dual licensing model to balance open-source collaboration with sustainable business practices. During the development phase, the software is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 (CC BY-NC-ND 4.0) license. ",
    "url": "/muxi/#license",
    
    "relUrl": "/#license"
  },"92": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/muxi/",
    
    "relUrl": "/"
  },"93": {
    "doc": "MCPs",
    "title": "Model Context Protocol (MCP)",
    "content": "The Model Context Protocol (MCP) is a standardized format for communication between applications and Large Language Models. It provides a structured way to send prompts, receive responses, and process tool calls. ",
    "url": "/muxi/mcp/#model-context-protocol-mcp",
    
    "relUrl": "/mcp/#model-context-protocol-mcp"
  },"94": {
    "doc": "MCPs",
    "title": "What is MCP?",
    "content": "MCP is a protocol that: . | Standardizes the format of messages sent to and received from language models | Enables structured tool calling by language models | Provides a consistent interface across different language model providers | Improves the control and predictability of language model interactions | Facilitates advanced features like parallel tool execution and multi-step reasoning | . ",
    "url": "/muxi/mcp/#what-is-mcp",
    
    "relUrl": "/mcp/#what-is-mcp"
  },"95": {
    "doc": "MCPs",
    "title": "MCP vs Tools: Understanding the Distinction",
    "content": "One common source of confusion is the relationship between MCP and tools. Let’s clarify this important distinction: . MCP is a Communication Protocol, Not the Tools . | MCP (Model Context Protocol): A standardized message format and communication protocol for interacting with language models. It’s the “language” your application uses to talk to models like GPT-4 or Claude. | Tools: Specific capabilities that extend what your agent can do (like web search, calculations, database queries, etc.). Each tool has its own functionality that executes in your application environment. | . Tool Execution Flow . The complete flow of tool usage looks like this: . | User sends a query to your agent | The agent passes this query to the language model via the MCP Handler | The language model determines a tool is needed and responds with a tool call request (in MCP format) | The MCP Handler parses this response and recognizes the tool call | The framework (not the language model, not the MCP) passes the request to the appropriate tool | The tool executes in your application environment with access to your system resources | The tool returns results to your framework | Results are formatted as an MCP message and sent back to the language model | The language model incorporates these results into its final response | . ┌───────────┐ Query ┌──────────────┐ MCP format ┌─────┐ │ User ├─────────────►│ Agent/ ├───────────────────►│ │ └───────────┘ │ Orchestrator │ │ │ └──────┬───────┘ │ │ │ │ │ │ │ LLM │ ┌─────────▼────────┐ │ │ ┌─────────────┐ │ │ Tool request │ │ │ Application │ │ MCP Handler │◄─────────────────┤ │ │ Environment │ │ │ └─────┘ │ │ └─────────┬────────┘ │ ┌────────▼─────┐ │ │ │ │ Tool │ │ │ Tool │◄────request─┘ │ │ Execution │ │ │ │ │ └──────┬───────┘ │ │ │ Tool │ │ result │ │ │ └───────────▼───────┐ MCP │ format│ ▼ Results to LLM . Key Points to Remember . | MCP is the messenger: It formats messages between your application and the language model | Language model makes the decision: The language model (not your code) decides when a tool is needed | Your framework executes tools: Tools run in your application’s environment, not inside the language model | MCP formats tool results: The results are sent back to the language model through the MCP format | . This clarifies that MCP is not the tools themselves—it’s just the standardized way to communicate about tools with the language model. ",
    "url": "/muxi/mcp/#mcp-vs-tools-understanding-the-distinction",
    
    "relUrl": "/mcp/#mcp-vs-tools-understanding-the-distinction"
  },"96": {
    "doc": "MCPs",
    "title": "Message Structure",
    "content": "The MCPMessage class is the core data structure used for communication in the MCP system. It represents a message in a conversation, with attributes for the sender role, content, and additional metadata. Key Attributes . A proper MCPMessage contains the following key attributes: . | role: The role of the sender (e.g., “user”, “assistant”, “system”, “function”) | content: The actual content of the message | name: Optional name for function messages | context: Optional context information | . This structure ensures compatibility with various LLM providers like OpenAI, Anthropic, and others. Example Usage . from src.core.mcp import MCPMessage # Create a user message user_msg = MCPMessage( role=\"user\", content=\"What's the weather like in New York?\" ) # Create an assistant response assistant_msg = MCPMessage( role=\"assistant\", content=\"The weather in New York is currently sunny with a temperature of 72°F.\" ) # Create a function message function_msg = MCPMessage( role=\"function\", content=\"The current temperature is 72°F with clear skies.\", name=\"get_weather\", metadata={\"location\": \"New York\", \"unit\": \"fahrenheit\"} ) . Internal Processing . When messages are processed, they are converted to a format suitable for the language model: . def _context_to_model_messages(context): \"\"\" Convert MCPContext to a list of messages for the language model. This method processes each message in the context and extracts the appropriate attributes (role, content, name, etc.) to create a properly formatted message for the LLM. \"\"\" model_messages = [] for message in context.messages: # Create the base message with required attributes model_message = { \"role\": message.role, \"content\": message.content } # Add optional attributes if present if hasattr(message, \"name\") and message.name: model_message[\"name\"] = message.name if hasattr(message, \"context\") and message.context: model_message[\"context\"] = message.context model_messages.append(model_message) return model_messages . This ensures that messages maintain proper structure throughout the system. ",
    "url": "/muxi/mcp/#message-structure",
    
    "relUrl": "/mcp/#message-structure"
  },"97": {
    "doc": "MCPs",
    "title": "MCP Message Structure",
    "content": "The core of MCP is the message structure: . class MCPMessage: def __init__(self, role, content=None, tool_calls=None, tool_call_id=None): self.role = role # \"user\", \"assistant\", \"system\", or \"tool\" self.content = content # Text content of the message self.tool_calls = tool_calls or [] # List of tool calls made by the assistant self.tool_call_id = tool_call_id # ID of the tool call this message responds to . Message Roles . MCP supports several message roles: . | user: Messages from the user to the language model | assistant: Responses from the language model | system: System instructions that guide the language model’s behavior | tool: Results returned from tool executions | . Tool Calls . Tool calls are structured as: . class MCPToolCall: def __init__(self, tool_name, tool_id, tool_args): self.tool_name = tool_name # Name of the tool to call self.tool_id = tool_id # Unique identifier for this tool call self.tool_args = tool_args # Arguments for the tool, as a dictionary . ",
    "url": "/muxi/mcp/#mcp-message-structure",
    
    "relUrl": "/mcp/#mcp-message-structure"
  },"98": {
    "doc": "MCPs",
    "title": "Using MCP in the Framework",
    "content": "Basic Message Exchange . from src.mcp.message import MCPMessage from src.models.openai import OpenAIModel # Create messages system_message = MCPMessage(role=\"system\", content=\"You are a helpful assistant.\") user_message = MCPMessage(role=\"user\", content=\"What's the weather in London?\") # Prepare messages for language model messages = [system_message, user_message] # Send to language model model = OpenAIModel(model=\"gpt-4o\") response = await model.generate(messages) # Process response if isinstance(response, MCPMessage): if response.content: print(f\"Assistant: {response.content}\") # Handle any tool calls if response.tool_calls: for tool_call in response.tool_calls: print(f\"Tool Call: {tool_call.tool_name} with args {tool_call.tool_args}\") . Tool Calling Flow . The complete flow of a tool-enabled conversation: . from src.mcp.message import MCPMessage, MCPToolCall from src.models.openai import OpenAIModel from src.tools.registry import ToolRegistry from src.tools.weather import WeatherTool # Set up language model and tools model = OpenAIModel(model=\"gpt-4o\") registry = ToolRegistry() registry.register(WeatherTool()) # Create initial messages messages = [ MCPMessage(role=\"system\", content=\"You are a helpful assistant with access to tools.\"), MCPMessage(role=\"user\", content=\"What's the weather in London?\") ] # Send to language model response = await model.generate(messages) messages.append(response) # Process tool calls if response.tool_calls: for tool_call in response.tool_calls: # Get the tool tool = registry.get_tool(tool_call.tool_name) if tool: # Execute the tool tool_result = await tool.execute(**tool_call.tool_args) # Create tool response message tool_message = MCPMessage( role=\"tool\", content=str(tool_result), tool_call_id=tool_call.tool_id ) # Add tool response to messages messages.append(tool_message) # Get final response from language model final_response = await model.generate(messages) messages.append(final_response) print(f\"Assistant: {final_response.content}\") . ",
    "url": "/muxi/mcp/#using-mcp-in-the-framework",
    
    "relUrl": "/mcp/#using-mcp-in-the-framework"
  },"99": {
    "doc": "MCPs",
    "title": "MCP Handler",
    "content": "The framework provides an MCP handler to abstract the message processing: . from src.mcp.handler import MCPHandler from src.models.openai import OpenAIModel from src.tools.registry import ToolRegistry from src.tools.weather import WeatherTool async def chat_with_tools(): # Set up language model, tools, and handler model = OpenAIModel(model=\"gpt-4o\") registry = ToolRegistry() registry.register(WeatherTool()) handler = MCPHandler(model=model, tool_registry=registry) # Set system message handler.set_system_message(\"You are a helpful assistant with access to tools.\") # Process a user message response = await handler.process_message(\"What's the weather in London?\") print(f\"Assistant: {response}\") # Continue the conversation response = await handler.process_message(\"How about Paris?\") print(f\"Assistant: {response}\") # Run the chat await chat_with_tools() . ",
    "url": "/muxi/mcp/#mcp-handler",
    
    "relUrl": "/mcp/#mcp-handler"
  },"100": {
    "doc": "MCPs",
    "title": "Creating Custom MCP Servers",
    "content": "The MUXI framework provides a convenient utility for creating custom MCP servers through a CLI wizard: . # Generate a new MCP server with an interactive wizard muxi create mcp-server # Specify output directory and optional name muxi create mcp-server --output-dir ./my_servers --name MyCustomMCP . The wizard guides you through: . | Naming your MCP server | Providing a description | Adding custom tools with descriptions | Generating all necessary boilerplate code | . Generated Project Structure . The generator creates a complete MCP server project with the following structure: . my_custom_mcp/ ├── __init__.py ├── my_custom_mcp_server.py ├── README.md ├── setup.py ├── tools/ │ ├── __init__.py │ └── tool1.py ├── tests/ │ └── __init__.py └── examples/ └── example_client.py . This structure includes: . | A properly configured MCP server implementation | Tool class templates | Installation setup | README documentation | Example client code | . Using the Generated MCP Server . Once generated, you can install and run your custom MCP server: . # Install in development mode cd my_custom_mcp pip install -e . # Run the server python -m my_custom_mcp.my_custom_mcp_server . The server will be available at http://localhost:5001 by default. ",
    "url": "/muxi/mcp/#creating-custom-mcp-servers",
    
    "relUrl": "/mcp/#creating-custom-mcp-servers"
  },"101": {
    "doc": "MCPs",
    "title": "Language Model Providers",
    "content": "The MCP standardizes interactions across different language model providers: . OpenAI Implementation . from src.mcp.message import MCPMessage from src.models.openai import OpenAIModel # Create an OpenAI language model model = OpenAIModel(model=\"gpt-4o\") # Process MCP messages messages = [ MCPMessage(role=\"system\", content=\"You are a helpful assistant.\"), MCPMessage(role=\"user\", content=\"Who won the 2022 World Cup?\") ] response = await model.generate(messages) print(response.content) . Anthropic Implementation . from src.mcp.message import MCPMessage from src.models.anthropic import AnthropicModel # Create an Anthropic language model model = AnthropicModel(model=\"claude-3-opus\") # Process MCP messages (same format as OpenAI) messages = [ MCPMessage(role=\"system\", content=\"You are a helpful assistant.\"), MCPMessage(role=\"user\", content=\"Who won the 2022 World Cup?\") ] response = await model.generate(messages) print(response.content) . ",
    "url": "/muxi/mcp/#language-model-providers",
    
    "relUrl": "/mcp/#language-model-providers"
  },"102": {
    "doc": "MCPs",
    "title": "Advanced MCP Features",
    "content": "Parallel Tool Execution . MCP allows for parallel execution of multiple tool calls: . from src.mcp.handler import MCPHandler, execute_tool_calls_parallel async def process_with_parallel_tools(handler, user_message): # Get response from language model response = await handler.model.generate([ MCPMessage(role=\"system\", content=handler.system_message), MCPMessage(role=\"user\", content=user_message) ]) if response.tool_calls: # Execute all tool calls in parallel tool_results = await execute_tool_calls_parallel( response.tool_calls, handler.tool_registry ) # Add all tool results to the conversation tool_messages = [] for tool_call_id, result in tool_results.items(): tool_messages.append(MCPMessage( role=\"tool\", content=str(result), tool_call_id=tool_call_id )) # Get final response with tool results messages = [ MCPMessage(role=\"system\", content=handler.system_message), MCPMessage(role=\"user\", content=user_message), response, # The assistant's response with tool calls *tool_messages # All tool results ] final_response = await handler.model.generate(messages) return final_response.content return response.content . Multi-Model Chain . You can chain different language models together using MCP as a common interface: . from src.mcp.message import MCPMessage from src.models.openai import OpenAIModel from src.models.anthropic import AnthropicModel async def multi_model_processing(query): # First model generates a detailed plan planning_model = OpenAIModel(model=\"gpt-4o\") plan_messages = [ MCPMessage(role=\"system\", content=\"You are a planning assistant. Create a detailed plan.\"), MCPMessage(role=\"user\", content=f\"Create a research plan for: {query}\") ] plan_response = await planning_model.generate(plan_messages) # Second model executes the plan execution_model = AnthropicModel(model=\"claude-3-opus\") execution_messages = [ MCPMessage(role=\"system\", content=\"You are a research assistant. Execute the given plan.\"), MCPMessage(role=\"user\", content=f\"Execute this research plan:\\n\\n{plan_response.content}\") ] execution_response = await execution_model.generate(execution_messages) # Third model summarizes the results summary_model = OpenAIModel(model=\"gpt-3.5-turbo\") summary_messages = [ MCPMessage(role=\"system\", content=\"You are a summarization assistant. Create concise summaries.\"), MCPMessage(role=\"user\", content=f\"Summarize these research results:\\n\\n{execution_response.content}\") ] summary_response = await summary_model.generate(summary_messages) return { \"plan\": plan_response.content, \"execution\": execution_response.content, \"summary\": summary_response.content } . ",
    "url": "/muxi/mcp/#advanced-mcp-features",
    
    "relUrl": "/mcp/#advanced-mcp-features"
  },"103": {
    "doc": "MCPs",
    "title": "Custom MCP Extensions",
    "content": "You can extend the MCP format for custom needs: . from src.mcp.message import MCPMessage class ExtendedMCPMessage(MCPMessage): def __init__(self, role, content=None, tool_calls=None, tool_call_id=None, metadata=None): super().__init__(role, content, tool_calls, tool_call_id) self.metadata = metadata or {} # Additional metadata for the message # Example usage message = ExtendedMCPMessage( role=\"user\", content=\"What's the weather in London?\", metadata={ \"timestamp\": \"2023-06-15T14:30:00Z\", \"user_id\": \"user_123\", \"session_id\": \"session_456\", \"client_info\": { \"device\": \"mobile\", \"browser\": \"chrome\", \"language\": \"en-US\" } } ) . ",
    "url": "/muxi/mcp/#custom-mcp-extensions",
    
    "relUrl": "/mcp/#custom-mcp-extensions"
  },"104": {
    "doc": "MCPs",
    "title": "MCP Message Serialization",
    "content": "For storing or transmitting MCP messages, you can serialize them: . import json from src.mcp.message import MCPMessage, MCPToolCall def serialize_mcp_message(message): \"\"\"Serialize an MCP message to a dictionary.\"\"\" data = { \"role\": message.role, \"content\": message.content, } if message.tool_calls: data[\"tool_calls\"] = [ { \"tool_name\": tc.tool_name, \"tool_id\": tc.tool_id, \"tool_args\": tc.tool_args } for tc in message.tool_calls ] if message.tool_call_id: data[\"tool_call_id\"] = message.tool_call_id return data def deserialize_mcp_message(data): \"\"\"Deserialize a dictionary to an MCP message.\"\"\" tool_calls = None if \"tool_calls\" in data: tool_calls = [ MCPToolCall( tool_name=tc[\"tool_name\"], tool_id=tc[\"tool_id\"], tool_args=tc[\"tool_args\"] ) for tc in data[\"tool_calls\"] ] return MCPMessage( role=data[\"role\"], content=data.get(\"content\"), tool_calls=tool_calls, tool_call_id=data.get(\"tool_call_id\") ) # Example usage message = MCPMessage( role=\"assistant\", content=\"I'll check the weather for you.\", tool_calls=[ MCPToolCall( tool_name=\"weather\", tool_id=\"call_123\", tool_args={\"location\": \"London\"} ) ] ) # Serialize to JSON serialized = json.dumps(serialize_mcp_message(message)) # Deserialize from JSON deserialized_data = json.loads(serialized) restored_message = deserialize_mcp_message(deserialized_data) . ",
    "url": "/muxi/mcp/#mcp-message-serialization",
    
    "relUrl": "/mcp/#mcp-message-serialization"
  },"105": {
    "doc": "MCPs",
    "title": "Websocket Integration",
    "content": "MCP messages can be transmitted via WebSockets for real-time communication: . # Server-side from src.mcp.handler import MCPHandler from src.mcp.message import MCPMessage import json async def handle_websocket(websocket, path): # Set up MCP handler handler = MCPHandler(model=model, tool_registry=registry) handler.set_system_message(\"You are a helpful assistant.\") async for message in websocket: # Parse incoming message data = json.loads(message) if data[\"type\"] == \"chat\": # Process the user message through MCP user_message = data[\"message\"] # Add to conversation history handler.add_message(MCPMessage(role=\"user\", content=user_message)) # Process the message and get response response = await handler.process_current_messages() # Send response back await websocket.send(json.dumps({ \"type\": \"response\", \"message\": response.content, \"has_tool_calls\": bool(response.tool_calls) })) # If there are tool calls, send their results if response.tool_calls: for tool_result in handler.last_tool_results: await websocket.send(json.dumps({ \"type\": \"tool_result\", \"tool_name\": tool_result[\"tool_name\"], \"result\": tool_result[\"result\"] })) . ",
    "url": "/muxi/mcp/#websocket-integration",
    
    "relUrl": "/mcp/#websocket-integration"
  },"106": {
    "doc": "MCPs",
    "title": "Best Practices",
    "content": ". | Consistent Message Format: Always use the MCPMessage class for consistency . | Tool Result Handling: Properly format tool results as MCPMessages with the “tool” role . | System Messages: Use descriptive system messages to guide language model behavior . | Error Handling: Implement robust error handling for tool calls . | Message History Management: Keep message history manageable to avoid token limits . | Validation: Validate incoming messages to ensure they conform to MCP structure . | . ",
    "url": "/muxi/mcp/#best-practices",
    
    "relUrl": "/mcp/#best-practices"
  },"107": {
    "doc": "MCPs",
    "title": "Troubleshooting",
    "content": "Tool Call Format Issues . | Ensure tool calls are correctly formatted with proper tool names and arguments | Check that the tool implementation matches the expected parameter structure | . Message Handling Errors . | Verify that message roles are one of: “user”, “assistant”, “system”, or “tool” | Ensure that tool response messages include the correct tool_call_id | . Language Model Compatibility . | Different language models may have varying support for tool calling | Check documentation for provider-specific limitations | . ",
    "url": "/muxi/mcp/#troubleshooting",
    
    "relUrl": "/mcp/#troubleshooting"
  },"108": {
    "doc": "MCPs",
    "title": "Next Steps",
    "content": "After understanding MCP, you might want to explore: . | Creating agents that leverage MCP for structured communication | Implementing custom tools that integrate with the MCP format | Setting up WebSocket connections for real-time MCP communication | Understanding memory systems and how they store MCP messages | . ",
    "url": "/muxi/mcp/#next-steps",
    
    "relUrl": "/mcp/#next-steps"
  },"109": {
    "doc": "MCPs",
    "title": "MCPs",
    "content": " ",
    "url": "/muxi/mcp/",
    
    "relUrl": "/mcp/"
  },"110": {
    "doc": "Memory",
    "title": "Memory Systems",
    "content": "Memory is a crucial component of the MUXI Framework that allows agents to retain information over time. The framework provides three complementary memory systems: . | Buffer Memory: Short-term memory for the current conversation context | Long-term Memory: Persistent memory for storing information across multiple sessions | Memobase: Multi-user memory system that partitions memories by user ID | . ",
    "url": "/muxi/memory/#memory-systems",
    
    "relUrl": "/memory/#memory-systems"
  },"111": {
    "doc": "Memory",
    "title": "Buffer Memory",
    "content": "Buffer memory is designed to store the recent conversation history, providing context for the agent’s responses. How Buffer Memory Works . Buffer memory: . | Maintains a fixed-size buffer of recent messages | Ensures the context stays within token limits for LLM processing | Uses FAISS for semantic search capabilities | Handles automatic summarization when needed | . Using Buffer Memory . Basic Initialization . from src.memory.buffer import BufferMemory # Create a buffer memory with default settings buffer = BufferMemory() # Create a buffer memory with custom settings buffer = BufferMemory( max_tokens=4000, # Maximum token count to maintain model=\"gpt-3.5-turbo\", # Model used for token counting summarize_after=20 # Summarize after 20 messages ) . Adding Messages . # Add a user message await buffer.add_message(\"user\", \"What is artificial intelligence?\") # Add an assistant message await buffer.add_message(\"assistant\", \"Artificial intelligence is the simulation of human intelligence by machines.\") # Add a system message await buffer.add_message(\"system\", \"The user seems interested in AI concepts.\") . Retrieving Messages . # Get all messages messages = await buffer.get_messages() for msg in messages: print(f\"{msg['role']}: {msg['content']}\") # Get formatted messages for sending to an LLM formatted_messages = await buffer.get_formatted_messages() . Searching the Buffer . # Search for relevant messages search_results = await buffer.search(\"machine learning\", top_k=3) for result in search_results: print(f\"Score: {result['score']}, Message: {result['message']}\") . Clearing the Buffer . # Clear all messages await buffer.clear() . ",
    "url": "/muxi/memory/#buffer-memory",
    
    "relUrl": "/memory/#buffer-memory"
  },"112": {
    "doc": "Memory",
    "title": "Long-term Memory",
    "content": "Long-term memory provides persistent storage for important information across multiple sessions, using a vector database for efficient semantic retrieval. How Long-term Memory Works . Long-term memory: . | Stores embeddings of important information in a database | Uses pgvector extension in PostgreSQL for vector similarity search | Enables retrieval of relevant information based on semantic similarity | Persists across sessions and agent restarts | . Using Long-term Memory . Initialization . from src.memory.long_term import LongTermMemory # Create a long-term memory with database connection memory = LongTermMemory( connection_string=\"postgresql://user:password@localhost:5432/ai_agent_db\", table_name=\"agent_memories\", embedding_model=\"text-embedding-ada-002\" # OpenAI embedding model ) # Initialize the database schema (run once) await memory.initialize() . Storing Memories . # Store a simple memory await memory.store(\"The capital of France is Paris.\") # Store a memory with metadata await memory.store( \"The user prefers Python over JavaScript for data science projects.\", metadata={ \"source\": \"conversation\", \"date\": \"2023-06-15\", \"topic\": \"programming preferences\" } ) # Store a memory with a custom embedding custom_embedding = [0.1, 0.2, 0.3, ...] # Your custom embedding await memory.store_with_embedding( \"Solar panels convert sunlight into electricity.\", embedding=custom_embedding ) . Retrieving Memories . # Search for relevant memories results = await memory.search(\"What programming language is best for data science?\", top_k=5) for result in results: print(f\"Score: {result['score']}, Memory: {result['text']}\") # Search with metadata filters results = await memory.search( \"programming preferences\", top_k=3, metadata_filter={\"topic\": \"programming preferences\"} ) . Advanced Operations . # Delete a specific memory await memory.delete(memory_id) # Clear all memories await memory.clear() # Update a memory await memory.update(memory_id, \"The user now prefers JavaScript over Python for web development.\") . ",
    "url": "/muxi/memory/#long-term-memory",
    
    "relUrl": "/memory/#long-term-memory"
  },"113": {
    "doc": "Memory",
    "title": "Memobase",
    "content": "Memobase is a multi-user memory manager that provides user-specific memory contexts using PostgreSQL/PGVector for storage. It’s ideal for multi-tenant applications where each user should have their own memory context. How Memobase Works . Memobase: . | Partitions memories by user ID | Creates separate collections for each user | Maintains user context between sessions | Uses PostgreSQL/PGVector for efficient semantic search | Provides a simple interface for user-specific operations | Supports domain knowledge storage for user profiles | . Using Memobase . Initialization . from src.memory.long_term import LongTermMemory from src.memory.memobase import Memobase # First, initialize a long-term memory instance long_term_memory = LongTermMemory( connection_string=\"postgresql://user:password@localhost:5432/ai_agent_db\" ) # Then create a Memobase instance with the long-term memory memobase = Memobase( long_term_memory=long_term_memory, default_user_id=0 # Default user ID for single-user mode ) . Adding Memories for Specific Users . # Add a memory for user 123 await memobase.add( content=\"My name is Alice\", metadata={\"type\": \"user_info\"}, user_id=123 ) # Add a memory for user 456 await memobase.add( content=\"My name is Bob\", metadata={\"type\": \"user_info\"}, user_id=456 ) # Add to default user (single-user mode) await memobase.add( content=\"System-wide information\", metadata={\"type\": \"system_info\"} ) . Searching Memories for Specific Users . # Search for user 123's memories results = await memobase.search( query=\"What is my name?\", user_id=123, limit=5 ) # Will find \"My name is Alice\" # Search for user 456's memories results = await memobase.search( query=\"What is my name?\", user_id=456, limit=5 ) # Will find \"My name is Bob\" # Search default user memories results = await memobase.search( query=\"system information\" ) # Will find \"System-wide information\" . Clearing User-Specific Memories . # Clear all memories for user 123 memobase.clear_user_memory(user_id=123) # Clear default user memories memobase.clear_user_memory() . Retrieving All Memories for a User . # Get all memories for user 123 memories = memobase.get_user_memories( user_id=123, limit=10, sort_by=\"created_at\", ascending=False ) # Display the memories for memory in memories: print(f\"Content: {memory['content']}\") print(f\"Metadata: {memory['metadata']}\") print(f\"Created at: {memory['created_at']}\") . Advanced Filtering . # Search with additional metadata filters results = await memobase.search( query=\"info\", user_id=123, additional_filter={\"type\": \"user_info\"} ) . Domain Knowledge Management . Memobase provides specialized methods for managing user domain knowledge - structured information about users that can be used to personalize agent interactions. Adding Domain Knowledge . # Add domain knowledge for a user await memobase.add_user_domain_knowledge( user_id=123, knowledge={ \"name\": \"John Doe\", \"age\": 30, \"location\": {\"city\": \"New York\", \"country\": \"USA\"}, \"interests\": [\"programming\", \"AI\", \"music\"], \"job\": \"Software Engineer\", \"family\": { \"spouse\": \"Jane Doe\", \"children\": [\"Alice\", \"Bob\"] } }, source=\"profile_setup\", importance=0.9 # High importance ensures this is prioritized in retrieval ) . Retrieving Domain Knowledge . # Get all domain knowledge for a user knowledge = await memobase.get_user_domain_knowledge(user_id=123) print(f\"User name: {knowledge['name']}\") print(f\"User location: {knowledge['location']['city']}, {knowledge['location']['country']}\") # Get specific domain knowledge keys profile = await memobase.get_user_domain_knowledge( user_id=123, keys=[\"name\", \"job\"] ) print(f\"{profile['name']} works as a {profile['job']}\") . Importing Domain Knowledge . # Import from a dictionary user_data = { \"name\": \"John Doe\", \"preferences\": { \"theme\": \"dark\", \"language\": \"English\" } } await memobase.import_user_domain_knowledge( data_source=user_data, user_id=123, format=\"dict\" ) # Import from a JSON file await memobase.import_user_domain_knowledge( data_source=\"user_profiles/john_doe.json\", user_id=123, format=\"json\", source=\"profile_import\" ) . Clearing Domain Knowledge . # Clear all domain knowledge for a user await memobase.clear_user_domain_knowledge(user_id=123) # Clear specific domain knowledge keys await memobase.clear_user_domain_knowledge( user_id=123, keys=[\"preferences\", \"location\"] ) . Using Domain Knowledge in Agent Interactions . # When processing a user message, include relevant domain knowledge async def process_with_domain_knowledge(agent, message, user_id): # Get user domain knowledge knowledge = await agent.memobase.get_user_domain_knowledge(user_id) # Create a context with user information context = \"Information about the user:\\n\" if \"name\" in knowledge: context += f\"- Name: {knowledge['name']}\\n\" if \"location\" in knowledge and isinstance(knowledge['location'], dict): location = knowledge['location'] context += f\"- Location: {location.get('city', '')}, {location.get('country', '')}\\n\" if \"interests\" in knowledge: interests = knowledge['interests'] if isinstance(interests, list): context += f\"- Interests: {', '.join(interests)}\\n\" # Include the context in your prompt to the LLM prompt = f\"{context}\\n\\nUser message: {message}\" # Process with the enhanced context response = await agent.process_message(prompt, user_id) return response . ",
    "url": "/muxi/memory/#memobase",
    
    "relUrl": "/memory/#memobase"
  },"114": {
    "doc": "Memory",
    "title": "Integrating Memory with Agents",
    "content": "Using Memobase with Agents . from src.core.orchestrator import Orchestrator from src.models import OpenAIModel from src.memory.buffer import BufferMemory from src.memory.long_term import LongTermMemory from src.memory.memobase import Memobase # Initialize memory systems buffer_memory = BufferMemory() # Create a Memobase instance which extends LongTermMemory with multi-user capabilities long_term_memory = LongTermMemory( connection_string=\"postgresql://user:password@localhost:5432/ai_agent_db\" ) memobase = Memobase(long_term_memory=long_term_memory) # Create an agent with multi-user support orchestrator = Orchestrator() orchestrator.create_agent( agent_id=\"multi_user_agent\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=buffer_memory, long_term_memory=memobase, # Pass Memobase as long_term_memory system_message=\"You are an assistant that remembers information about different users.\" ) # Chat with the agent with different user contexts response1 = await orchestrator.chat(\"multi_user_agent\", \"My name is Alice\", user_id=123) response2 = await orchestrator.chat(\"multi_user_agent\", \"My name is Bob\", user_id=456) # Each user has their own memory context response1 = await orchestrator.chat(\"multi_user_agent\", \"What is my name?\", user_id=123) # Agent responds: \"Your name is Alice.\" response2 = await orchestrator.chat(\"multi_user_agent\", \"What is my name?\", user_id=456) # Agent responds: \"Your name is Bob.\" . Memory Augmented Generation . When an agent processes a user message with both buffer and long-term memory: . | The user message is added to buffer memory | The system searches long-term memory for relevant information | Relevant memories are included in the context provided to the LLM | The LLM response is added to buffer memory | Important information from the interaction may be stored in long-term memory | . async def process_message_with_memory(agent, user_message): # Add the user message to buffer memory await agent.buffer_memory.add_message(\"user\", user_message) # Retrieve relevant memories memories = await agent.long_term_memory.search(user_message, top_k=3) # Format memories as context memory_context = \"Relevant information from my memory:\\n\" for memory in memories: memory_context += f\"- {memory['text']}\\n\" # Create a system message with the memory context await agent.buffer_memory.add_message(\"system\", memory_context) # Get all messages for the LLM messages = await agent.buffer_memory.get_formatted_messages() # Send to LLM for processing response = await agent.model.generate(messages) # Add the response to buffer memory await agent.buffer_memory.add_message(\"assistant\", response) # Store important information in long-term memory # (This could be automated or triggered based on certain conditions) if \"important_fact\" in response: await agent.long_term_memory.store(response) return response . ",
    "url": "/muxi/memory/#integrating-memory-with-agents",
    
    "relUrl": "/memory/#integrating-memory-with-agents"
  },"115": {
    "doc": "Memory",
    "title": "Custom Memory Implementations",
    "content": "You can create custom memory implementations by extending the base classes: . Custom Buffer Memory . from src.memory.buffer import BufferMemory class CustomBufferMemory(BufferMemory): def __init__(self, max_tokens=2000, **kwargs): super().__init__(max_tokens=max_tokens, **kwargs) self.custom_property = kwargs.get(\"custom_property\") async def add_message(self, role, content): # Custom pre-processing if role == \"user\": content = f\"[User Input]: {content}\" # Call the parent method await super().add_message(role, content) # Custom post-processing # e.g., log the message, trigger an event, etc. print(f\"Added message: {role}: {content[:30]}...\") async def custom_method(self): # Implement custom functionality pass . Custom Long-term Memory . from src.memory.long_term import LongTermMemory class CustomLongTermMemory(LongTermMemory): def __init__(self, **kwargs): super().__init__(**kwargs) self.importance_threshold = kwargs.get(\"importance_threshold\", 0.7) async def store(self, text, metadata=None): # Add importance score importance = self._calculate_importance(text) if metadata is None: metadata = {} metadata[\"importance\"] = importance # Only store if important enough if importance &gt;= self.importance_threshold: return await super().store(text, metadata) return None def _calculate_importance(self, text): # Custom logic to determine importance # This is a simplified example keywords = [\"critical\", \"important\", \"remember\", \"key fact\"] score = sum(1 for keyword in keywords if keyword in text.lower()) / len(keywords) return score . ",
    "url": "/muxi/memory/#custom-memory-implementations",
    
    "relUrl": "/memory/#custom-memory-implementations"
  },"116": {
    "doc": "Memory",
    "title": "Memory Strategies",
    "content": "Context Window Management . To effectively manage the LLM’s context window: . class ContextWindowManager: def __init__(self, buffer_memory, max_tokens=4000): self.buffer_memory = buffer_memory self.max_tokens = max_tokens async def optimize_context(self): # Get current token count messages = await self.buffer_memory.get_messages() current_tokens = self._count_tokens(messages) if current_tokens &gt; self.max_tokens: # Summarize oldest messages oldest_messages = messages[:len(messages)//2] summary = await self._summarize_messages(oldest_messages) # Replace oldest messages with summary await self.buffer_memory.clear() await self.buffer_memory.add_message(\"system\", f\"Summary of previous conversation: {summary}\") # Add back the most recent messages for msg in messages[len(messages)//2:]: await self.buffer_memory.add_message(msg[\"role\"], msg[\"content\"]) async def _summarize_messages(self, messages): # Implementation of summarization logic # This could use the LLM itself pass def _count_tokens(self, messages): # Implementation of token counting pass . Information Extraction for Long-term Storage . class MemoryExtractor: def __init__(self, model, long_term_memory): self.model = model self.long_term_memory = long_term_memory async def extract_and_store(self, conversation): # Prompt the LLM to extract important information prompt = f\"\"\" Please extract important facts and information from this conversation that should be remembered long-term. Format each fact as a separate bullet point. Conversation: {conversation} Important facts to remember: \"\"\" extraction_result = await self.model.generate([{\"role\": \"user\", \"content\": prompt}]) # Parse the bullet points facts = [line.strip(\"- \") for line in extraction_result.split(\"\\n\") if line.strip().startswith(\"-\")] # Store each fact in long-term memory for fact in facts: await self.long_term_memory.store(fact) return facts . ",
    "url": "/muxi/memory/#memory-strategies",
    
    "relUrl": "/memory/#memory-strategies"
  },"117": {
    "doc": "Memory",
    "title": "Best Practices",
    "content": ". | Buffer Size Management: Set appropriate buffer sizes to balance context richness with token limits . | Memory Pruning: Regularly clean up outdated or irrelevant memories from long-term storage . | Semantic Search Optimization: Fine-tune search parameters for better retrieval relevance . | Memory Metadata: Use detailed metadata to enhance filtering and retrieval capabilities . | Backup and Recovery: Implement backup systems for long-term memory to prevent data loss . | Privacy Considerations: Implement proper data retention policies and user controls for stored memories . | . ",
    "url": "/muxi/memory/#best-practices",
    
    "relUrl": "/memory/#best-practices"
  },"118": {
    "doc": "Memory",
    "title": "Troubleshooting",
    "content": "Buffer Memory Issues . | Out of Memory Errors: Reduce max_tokens or implement more aggressive summarization | Context Loss: Adjust the summarization strategy to preserve important information | Relevance Issues: Fine-tune the embedding model for better semantic search | . Long-term Memory Issues . | Database Connection Failures: Check connection strings and ensure the database is running | Slow Queries: Add appropriate indexes to the vector database | Embedding Generation Errors: Verify API keys and model availability for embedding generation | . ",
    "url": "/muxi/memory/#troubleshooting",
    
    "relUrl": "/memory/#troubleshooting"
  },"119": {
    "doc": "Memory",
    "title": "Database Structure",
    "content": "The memory systems in MUXI are backed by a robust PostgreSQL database schema optimized for efficient storage and retrieval of vector embeddings and user-specific data. Core Tables . The database is structured around these key tables: . Users Table . CREATE TABLE users ( id SERIAL PRIMARY KEY, user_id CHAR(21) NOT NULL UNIQUE, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); . | id: Internal primary key | user_id: Public-facing NanoID (CHAR(21)) | Timestamps for tracking creation and updates | . Collections Table . CREATE TABLE collections ( id SERIAL PRIMARY KEY, user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE, collection_id CHAR(21) NOT NULL UNIQUE, name VARCHAR(255) NOT NULL, description TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); . | Stores memory collections that can be organized by topic or purpose | Associated with specific users via foreign key | Contains metadata about the collection (name, description) | . Memories Table . CREATE TABLE memories ( id SERIAL PRIMARY KEY, user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE, memory_id CHAR(21) NOT NULL UNIQUE, collection_id INTEGER REFERENCES collections(id) ON DELETE SET NULL, content TEXT NOT NULL, metadata JSONB DEFAULT '{}'::jsonb, embedding vector(1536), source VARCHAR(255), type VARCHAR(50), created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); . | Core table for storing memory content and embeddings | embedding column uses PostgreSQL’s pgvector extension for vector storage | metadata uses JSONB for flexible schema evolution | Associated with users and optionally with collections | . Credentials Table . CREATE TABLE credentials ( id SERIAL PRIMARY KEY, user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE, credential_id CHAR(21) NOT NULL UNIQUE, name VARCHAR(255) NOT NULL, service VARCHAR(255) NOT NULL, credentials JSONB NOT NULL DEFAULT '{}'::jsonb, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); . | Stores user credentials for MCP servers and external tools | Uses JSONB for flexible credential format storage | Secured with proper indexing and access controls | . Indexing Strategy . The database uses a comprehensive indexing strategy: . | Primary Keys: All tables have auto-incrementing primary keys for efficiency | Foreign Keys: Proper relationships with CASCADE delete where appropriate | NanoID Indexes: All *_id fields (user_id, memory_id, etc.) are indexed for direct lookups | Vector Indexes: IVFFLAT indexes on vector columns for efficient similarity search: CREATE INDEX memories_embedding_idx ON memories USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100); . | Timestamp Indexes: All tables have indexes on created_at and updated_at for time-based queries | Composite Indexes: For common query patterns (e.g., user_id + created_at) | JSONB Indexes: GIN indexes on JSONB columns for efficient querying of JSON contents: CREATE INDEX idx_credentials_json ON credentials USING GIN (credentials); . | . Schema Migrations . The database schema is managed through migrations that: . | Track applied changes to ensure consistent schema across environments | Support both forward (up) and backward (down) migrations for each change | Encapsulate SQL in Python functions for better version control | Create proper indexes for all query patterns | Enforce referential integrity through foreign key constraints | . This database structure provides a solid foundation for the memory systems within MUXI, enabling efficient retrieval of user-specific information, rich metadata storage, and vector similarity search for semantic understanding. ",
    "url": "/muxi/memory/#database-structure",
    
    "relUrl": "/memory/#database-structure"
  },"120": {
    "doc": "Memory",
    "title": "Next Steps",
    "content": "After implementing memory systems, you might want to explore: . | Creating custom tools that can access and manipulate memory | Setting up WebSocket connections for real-time memory updates | Implementing advanced MCP features to better control how the LLM uses memory | Developing agent collaboration methods that share memory between agents | . ",
    "url": "/muxi/memory/#next-steps",
    
    "relUrl": "/memory/#next-steps"
  },"121": {
    "doc": "Memory",
    "title": "Memory",
    "content": " ",
    "url": "/muxi/memory/",
    
    "relUrl": "/memory/"
  },"122": {
    "doc": "LLM Models",
    "title": "LLM Models",
    "content": "The MUXI framework integrates with various Large Language Model (LLM) providers through a standardized Model Context Protocol (MCP) interface. This document explains how models are integrated, configured, and used within the framework. ",
    "url": "/muxi/models/",
    
    "relUrl": "/models/"
  },"123": {
    "doc": "LLM Models",
    "title": "Supported LLM Providers",
    "content": "MUXI currently supports the following LLM providers: . | OpenAI - Access to GPT-3.5, GPT-4, and other OpenAI models | Anthropic - Support for Claude models (planned) | Ollama - Integration with locally hosted models (planned) | Gemini - Google’s Gemini models (planned) | Custom Models - Integration with custom-hosted models | . ",
    "url": "/muxi/models/#supported-llm-providers",
    
    "relUrl": "/models/#supported-llm-providers"
  },"124": {
    "doc": "LLM Models",
    "title": "The Model Layer",
    "content": "The model layer in MUXI serves as an abstraction between your agents and the underlying LLM providers: . graph TD A[Agent] --&gt; B[Model Layer] B --&gt; C[OpenAI] B --&gt; D[Anthropic] B --&gt; E[Ollama] B --&gt; F[Gemini] B --&gt; G[Custom] . Key Components . | Model Classes: Provider-specific implementations that handle communication with each LLM API | MCP Adapter: Converts between MUXI’s internal format and the provider-specific formats | Configuration: Settings for controlling model behavior, temperature, etc. | Response Processing: Handles streaming, parsing, and error management | . ",
    "url": "/muxi/models/#the-model-layer",
    
    "relUrl": "/models/#the-model-layer"
  },"125": {
    "doc": "LLM Models",
    "title": "Using Models in MUXI",
    "content": "Basic Model Configuration . from src.models.openai import OpenAIModel # Create a model with default settings model = OpenAIModel(model=\"gpt-4o\") # Create a model with custom settings custom_model = OpenAIModel( model=\"gpt-4-turbo\", temperature=0.7, max_tokens=2000, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0 ) . Assigning Models to Agents . from src.core.orchestrator import Orchestrator from src.models.openai import OpenAIModel from src.memory.buffer import BufferMemory # Create an orchestrator orchestrator = Orchestrator() # Create an agent with a specific model orchestrator.create_agent( agent_id=\"assistant\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), system_message=\"You are a helpful AI assistant.\" ) . Switching Models . You can update an agent’s model at any time: . # Switch to a different model new_model = OpenAIModel(model=\"gpt-3.5-turbo\") orchestrator.update_agent( agent_id=\"assistant\", model=new_model ) . ",
    "url": "/muxi/models/#using-models-in-muxi",
    
    "relUrl": "/models/#using-models-in-muxi"
  },"126": {
    "doc": "LLM Models",
    "title": "Model Configuration Options",
    "content": "OpenAI Models . | Parameter | Description | Default | Valid Range | . | model | The model to use | “gpt-4o” | Any valid OpenAI model ID | . | temperature | Controls randomness | 0.7 | 0.0 - 2.0 | . | max_tokens | Max tokens to generate | 1000 | 1 - 8192 (model dependent) | . | top_p | Nucleus sampling parameter | 1.0 | 0.0 - 1.0 | . | frequency_penalty | Penalizes repeated tokens | 0.0 | -2.0 - 2.0 | . | presence_penalty | Penalizes repeated topics | 0.0 | -2.0 - 2.0 | . Anthropic Models (Planned) . | Parameter | Description | Default | Valid Range | . | model | The model to use | “claude-3-opus” | Any valid Anthropic model | . | temperature | Controls randomness | 0.7 | 0.0 - 1.0 | . | max_tokens | Max tokens to generate | 1000 | 1 - model dependent | . | top_p | Nucleus sampling parameter | 1.0 | 0.0 - 1.0 | . ",
    "url": "/muxi/models/#model-configuration-options",
    
    "relUrl": "/models/#model-configuration-options"
  },"127": {
    "doc": "LLM Models",
    "title": "Streaming Responses",
    "content": "MUXI supports streaming responses from LLMs, which is particularly useful for real-time applications: . # Enable streaming for WebSocket or CLI response_stream = orchestrator.stream_chat( agent_id=\"assistant\", message=\"Generate a detailed explanation of quantum computing\", user_id=123 # Optional, for multi-user agents ) # Process the stream for chunk in response_stream: # Each chunk is a piece of the response as it's generated print(chunk, end=\"\", flush=True) . ",
    "url": "/muxi/models/#streaming-responses",
    
    "relUrl": "/models/#streaming-responses"
  },"128": {
    "doc": "LLM Models",
    "title": "Handling Model Errors",
    "content": "MUXI provides built-in error handling for common LLM provider issues: . | API Errors: Connection issues, rate limits, etc. | Context Length Exceeded: Automatic handling of token limit issues | Content Policy Violations: Detection and management of blocked content | Timeouts: Graceful handling of slow responses | . ",
    "url": "/muxi/models/#handling-model-errors",
    
    "relUrl": "/models/#handling-model-errors"
  },"129": {
    "doc": "LLM Models",
    "title": "Model Selection Best Practices",
    "content": "Choosing the Right Model . | GPT-4/Claude-3: For complex reasoning, creative tasks, or when accuracy is critical | GPT-3.5: For simpler tasks, faster responses, or cost-sensitive applications | Local Models: For privacy-sensitive applications or offline usage | . Performance Optimization . | Prompt Engineering: Craft effective prompts to get better results | Temperature Settings: Lower for factual tasks, higher for creative ones | Token Management: Balance context size vs cost by trimming unnecessary content | Request Batching: Group similar requests to reduce API overhead | . ",
    "url": "/muxi/models/#model-selection-best-practices",
    
    "relUrl": "/models/#model-selection-best-practices"
  },"130": {
    "doc": "LLM Models",
    "title": "Multi-Model Strategies",
    "content": "For advanced use cases, MUXI supports using different models for different agents or tasks: . # Create specialized agents with different models orchestrator.create_agent( agent_id=\"researcher\", model=OpenAIModel(model=\"gpt-4o\"), # Powerful model for complex research system_message=\"You are a research assistant skilled in deep analysis.\" ) orchestrator.create_agent( agent_id=\"responder\", model=OpenAIModel(model=\"gpt-3.5-turbo\"), # Faster model for simple responses system_message=\"You communicate research findings in simple language.\" ) . ",
    "url": "/muxi/models/#multi-model-strategies",
    
    "relUrl": "/models/#multi-model-strategies"
  },"131": {
    "doc": "LLM Models",
    "title": "Model Context Protocol (MCP)",
    "content": "The MCP standardizes communication between MUXI and LLM providers, enabling seamless switching between different models. For more details, see the MCP documentation. ",
    "url": "/muxi/models/#model-context-protocol-mcp",
    
    "relUrl": "/models/#model-context-protocol-mcp"
  },"132": {
    "doc": "LLM Models",
    "title": "Extending with New Model Providers",
    "content": "MUXI is designed to be extensible. To add support for a new LLM provider: . | Create a new model class that implements the model interface | Implement the provider-specific API communication | Create an MCP adapter for the provider | Register the new model type with the framework | . Example of a custom model provider implementation: . from src.models.base import BaseModel from src.models.mcp import MCPMessage, MCPRole class CustomProviderModel(BaseModel): def __init__(self, api_key=None, model=\"default-model\", **kwargs): super().__init__() self.api_key = api_key or os.environ.get(\"CUSTOM_PROVIDER_API_KEY\") self.model = model self.base_url = \"https://api.customprovider.com/v1\" self.kwargs = kwargs async def generate(self, messages, **kwargs): # Convert MCP messages to provider format provider_messages = self._convert_to_provider_format(messages) # Call the provider API response = await self._call_api(provider_messages, **kwargs) # Convert response back to MCP format return self._convert_to_mcp_format(response) # Implement other required methods... ",
    "url": "/muxi/models/#extending-with-new-model-providers",
    
    "relUrl": "/models/#extending-with-new-model-providers"
  },"133": {
    "doc": "LLM Models",
    "title": "Upcoming Model Enhancements",
    "content": "Future updates to MUXI’s model layer will include: . | Model Fallbacks: Automatic fallback to alternative models if primary fails | Model Routing: Intelligent routing of requests to appropriate models | Performance Monitoring: Tracking of model performance and cost metrics | Fine-tuned Model Support: Integration with custom fine-tuned models | Multi-modal Capabilities: Support for image and audio inputs/outputs | . ",
    "url": "/muxi/models/#upcoming-model-enhancements",
    
    "relUrl": "/models/#upcoming-model-enhancements"
  },"134": {
    "doc": "LLM Models",
    "title": "See Also",
    "content": ". | Model Context Protocol (MCP) | Agents | Orchestrator | Tools vs MCP | . ",
    "url": "/muxi/models/#see-also",
    
    "relUrl": "/models/#see-also"
  },"135": {
    "doc": "Multi-Modal Capabilities",
    "title": "Multi-Modal Capabilities (Coming Soon)",
    "content": "The MUXI Framework is expanding to support multi-modal agents capable of processing and generating content across different modalities, including text, images, audio, documents, and eventually video. ",
    "url": "/muxi/multi-modal/#multi-modal-capabilities-coming-soon",
    
    "relUrl": "/multi-modal/#multi-modal-capabilities-coming-soon"
  },"136": {
    "doc": "Multi-Modal Capabilities",
    "title": "Overview",
    "content": "Multi-modal agents can understand and interact with various types of media, providing a more natural and rich interaction experience. This allows agents to: . | Process images and understand their content | Handle audio for speech recognition and synthesis | Work with documents like PDFs and Office files | Stream real-time audio for voice conversations | (Eventually) Process video content | . ",
    "url": "/muxi/multi-modal/#overview",
    
    "relUrl": "/multi-modal/#overview"
  },"137": {
    "doc": "Multi-Modal Capabilities",
    "title": "Architecture",
    "content": "The multi-modal capabilities are being built as an extension to the existing framework, with these key components: . Enhanced Message Structure . The MCPMessage class will be extended to support media attachments: . class MCPMessage: def __init__( self, role: str, content: str, name: Optional[str] = None, attachments: List[Dict[str, Any]] = None, metadata: Optional[Dict[str, Any]] = None ): self.role = role self.content = content self.name = name self.attachments = attachments or [] self.metadata = metadata or {} . Each attachment will include metadata about the media type, format, and content. Media Processing Pipeline . A dedicated media processing module will handle different media types: . class MediaProcessor: def process_image(self, image_data, target_size=None, format=\"jpeg\"): \"\"\"Process image data for model consumption\"\"\" def process_audio(self, audio_data, target_format=\"mp3\", max_duration=None): \"\"\"Process audio data\"\"\" def process_video(self, video_data, extract_frames=False, max_duration=None): \"\"\"Process video data\"\"\" def encode_for_model(self, media_data, media_type, model_provider): \"\"\"Encode media in the format expected by the specific LLM provider\"\"\" . Media Storage System . A storage system will manage media files efficiently: . class MediaStorage: def store(self, media_data, media_type, file_name=None): \"\"\"Store media data and return a reference\"\"\" def retrieve(self, media_reference): \"\"\"Retrieve media data from storage\"\"\" def cleanup(self, older_than=None): \"\"\"Clean up old media files\"\"\" . Streaming Media Support . For real-time audio and video interactions: . | WebSocket extensions for streaming media | Buffer management for smooth playback | WebRTC integration for voice/video chat | Session management for maintaining connections | . ",
    "url": "/muxi/multi-modal/#architecture",
    
    "relUrl": "/multi-modal/#architecture"
  },"138": {
    "doc": "Multi-Modal Capabilities",
    "title": "Supported Media Types",
    "content": "Images . | Support for common formats (JPEG, PNG, GIF, WebP) | Integration with vision models (GPT-4V, Claude 3, Gemini) | Preprocessing for optimization (resizing, format conversion) | . Audio . | Support for audio files (MP3, WAV, OGG, etc.) | Real-time streaming for voice conversations | Speech-to-text using OpenAI Whisper and other providers | Text-to-speech for natural voice responses | . Documents . | PDF processing and text extraction | Support for Office documents (Word, Excel, PowerPoint) | OCR for scanned documents | Document summarization | . Video (Future) . | Video file processing | Frame extraction and analysis | Real-time video streaming | Video content understanding | . ",
    "url": "/muxi/multi-modal/#supported-media-types",
    
    "relUrl": "/multi-modal/#supported-media-types"
  },"139": {
    "doc": "Multi-Modal Capabilities",
    "title": "Implementation Roadmap",
    "content": ". | Image support (Phase 1) | Audio file support (Phase 1) | Document processing (Phase 2) | Streaming audio capabilities (Phase 2) | Video support (Phase 3) | . ",
    "url": "/muxi/multi-modal/#implementation-roadmap",
    
    "relUrl": "/multi-modal/#implementation-roadmap"
  },"140": {
    "doc": "Multi-Modal Capabilities",
    "title": "API Extensions",
    "content": "The API will be extended to support media: . # File upload endpoint @app.post(\"/chat/{agent_id}/with_media\") async def chat_with_media( agent_id: str, message: str = Form(...), files: List[UploadFile] = File(None), user_id: Optional[str] = None ) # WebSocket support for streaming media async def handle_audio_stream(ws, data): # Real-time audio streaming implementation . ",
    "url": "/muxi/multi-modal/#api-extensions",
    
    "relUrl": "/multi-modal/#api-extensions"
  },"141": {
    "doc": "Multi-Modal Capabilities",
    "title": "WebSocket Protocol Extensions",
    "content": "New message types for WebSocket communication: . // Send an image { \"type\": \"media_upload\", \"media_type\": \"image\", \"data\": \"base64_encoded_image\", \"file_name\": \"photo.jpg\" } // Start audio streaming { \"type\": \"start_audio_stream\", \"format\": \"mp3\", \"sample_rate\": 44100 } // Audio chunk in streaming session { \"type\": \"audio_chunk\", \"session_id\": \"abc123\", \"data\": \"base64_encoded_audio_chunk\", \"timestamp\": 1684321067 } . ",
    "url": "/muxi/multi-modal/#websocket-protocol-extensions",
    
    "relUrl": "/multi-modal/#websocket-protocol-extensions"
  },"142": {
    "doc": "Multi-Modal Capabilities",
    "title": "Usage Examples",
    "content": "Once implemented, multi-modal capabilities will enable rich interactions: . from src import muxi # Initialize with multi-modal support mx = muxi() mx.add_agent(\"assistant\", \"configs/vision_assistant.yaml\") # Upload an image with a question with open(\"image.jpg\", \"rb\") as img_file: response = mx.chat_with_media( \"What's in this image?\", attachments=[{ \"type\": \"image\", \"data\": img_file.read(), \"file_name\": \"image.jpg\" }], agent_name=\"assistant\" ) print(response) # The agent describes the image content . Stay tuned for updates as we implement these exciting capabilities! . ",
    "url": "/muxi/multi-modal/#usage-examples",
    
    "relUrl": "/multi-modal/#usage-examples"
  },"143": {
    "doc": "Multi-Modal Capabilities",
    "title": "Multi-Modal Capabilities",
    "content": " ",
    "url": "/muxi/multi-modal/",
    
    "relUrl": "/multi-modal/"
  },"144": {
    "doc": "Orchestrator",
    "title": "The Orchestrator",
    "content": "The Orchestrator is a central component of the MUXI Framework responsible for managing multiple agents and coordinating their interactions. It serves as the control center for agent creation, access, and communication. ",
    "url": "/muxi/orchestrator/#the-orchestrator",
    
    "relUrl": "/orchestrator/#the-orchestrator"
  },"145": {
    "doc": "Orchestrator",
    "title": "What is the Orchestrator?",
    "content": "The Orchestrator: . | Creates and manages multiple agents | Routes messages to the appropriate agents | Facilitates communication between agents | Maintains a registry of available agents | Controls access to shared resources | . ",
    "url": "/muxi/orchestrator/#what-is-the-orchestrator",
    
    "relUrl": "/orchestrator/#what-is-the-orchestrator"
  },"146": {
    "doc": "Orchestrator",
    "title": "Core Features",
    "content": "The Orchestrator provides several key features: . | Agent Management: Create, retrieve, and manage multiple agents | Default Agent Selection: Set and use a default agent for messages | Message Routing: Direct messages to the appropriate agent | Intelligent Message Routing: Automatically select the best agent for a message based on content analysis | Memory Management: Clear memories for individual agents or all agents | Tool Management: Centralized registry for tools used by agents | . ",
    "url": "/muxi/orchestrator/#core-features",
    
    "relUrl": "/orchestrator/#core-features"
  },"147": {
    "doc": "Orchestrator",
    "title": "Intelligent Message Routing",
    "content": "The Orchestrator includes an intelligent message routing system that can automatically direct user messages to the most appropriate agent based on the message content and agent descriptions. How It Works . | When a user sends a message without specifying an agent, the routing system analyzes the message. | The system compares the message content against each agent’s description using a dedicated LLM. | The LLM selects the most appropriate agent based on this analysis. | The message is then sent to the selected agent for processing. | If routing fails or no appropriate agent is found, the system falls back to the default agent or the first available agent. | . Configuration . The routing system is configured through environment variables: . # Routing LLM provider (defaults to \"openai\") ROUTING_LLM=openai # Model to use for routing (defaults to \"gpt-4o-mini\") ROUTING_LLM_MODEL=gpt-4o-mini # Temperature for routing decisions (defaults to 0.0) ROUTING_LLM_TEMPERATURE=0.0 # Whether to cache routing decisions (defaults to true) ROUTING_USE_CACHING=true # Time in seconds to cache routing decisions (defaults to 3600) ROUTING_CACHE_TTL=3600 . For optimal routing results, provide clear and specific descriptions for each agent when creating them: . orchestrator.create_agent( agent_id=\"weather_agent\", model=OpenAIModel(...), description=\"Specialized in providing weather forecasts, answering questions about climate and weather phenomena, and reporting current conditions in various locations.\" ) . ",
    "url": "/muxi/orchestrator/#intelligent-message-routing",
    
    "relUrl": "/orchestrator/#intelligent-message-routing"
  },"148": {
    "doc": "Orchestrator",
    "title": "Using the Orchestrator",
    "content": "Initialization . The Orchestrator can be initialized as a standalone component: . from src.core.orchestrator import Orchestrator # Create a new orchestrator orchestrator = Orchestrator() . Creating Agents . The primary function of the Orchestrator is to create and manage agents: . from src.models import OpenAIModel from src.memory.buffer import BufferMemory from src.tools.web_search import WebSearchTool # Create an agent with the orchestrator orchestrator.create_agent( agent_id=\"research_agent\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), tools=[WebSearchTool()], system_message=\"You are a research assistant specialized in finding and summarizing information.\", set_as_default=True ) # Create another agent orchestrator.create_agent( agent_id=\"coding_agent\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), tools=[], system_message=\"You are a coding assistant specialized in Python programming.\", ) . Interacting with Agents . You can interact with agents through the Orchestrator: . # Send a message to a specific agent response = await orchestrator.run( agent_id=\"research_agent\", message=\"What are the latest developments in quantum computing?\" ) print(response) # Send a message to the default agent response = await orchestrator.run( message=\"Explain the concept of quantum entanglement\" ) print(response) . Managing Agents . The Orchestrator provides methods to manage the agent lifecycle: . # Get a list of all agents agents = orchestrator.list_agents() print(f\"Available agents: {agents}\") # Check if an agent exists if orchestrator.has_agent(\"research_agent\"): print(\"Research agent exists\") # Get a reference to an agent agent = orchestrator.get_agent(\"research_agent\") # Delete an agent orchestrator.delete_agent(\"coding_agent\") # Set a different default agent orchestrator.set_default_agent(\"research_agent\") . ",
    "url": "/muxi/orchestrator/#using-the-orchestrator",
    
    "relUrl": "/orchestrator/#using-the-orchestrator"
  },"149": {
    "doc": "Orchestrator",
    "title": "Multi-Agent Collaboration",
    "content": "One of the powerful features of the Orchestrator is enabling collaboration between multiple agents: . Sequential Agent Collaboration . # Research agent finds information research_response = await orchestrator.run( agent_id=\"research_agent\", message=\"Find information about climate change mitigation strategies\" ) # Analysis agent analyzes the information analysis_response = await orchestrator.run( agent_id=\"analysis_agent\", message=f\"Analyze this information and identify the most promising strategies: {research_response}\" ) # Summary agent creates a final summary summary_response = await orchestrator.run( agent_id=\"summary_agent\", message=f\"Create a concise summary of these strategies: {analysis_response}\" ) print(summary_response) . Agent Communication . Agents can indirectly communicate through the Orchestrator: . # Define a collaboration protocol async def agent_collaboration(): # First agent generates ideas ideas = await orchestrator.run( agent_id=\"brainstorm_agent\", message=\"Generate 5 innovative solutions for reducing urban traffic congestion\" ) # Second agent evaluates the ideas evaluation = await orchestrator.run( agent_id=\"critic_agent\", message=f\"Critically evaluate these solutions, ranking them from most to least promising: {ideas}\" ) # Third agent synthesizes a comprehensive plan plan = await orchestrator.run( agent_id=\"synthesis_agent\", message=f\"Based on these evaluations, create a detailed implementation plan for the top 2 solutions: {evaluation}\" ) return plan # Execute the collaboration plan = await agent_collaboration() print(plan) . ",
    "url": "/muxi/orchestrator/#multi-agent-collaboration",
    
    "relUrl": "/orchestrator/#multi-agent-collaboration"
  },"150": {
    "doc": "Orchestrator",
    "title": "Advanced Features",
    "content": "Dynamic Agent Creation . You can dynamically create specialized agents based on user requests: . async def get_specialized_agent(domain): # Check if a specialized agent exists agent_id = f\"{domain}_specialist\" if not orchestrator.has_agent(agent_id): # Create a new specialized agent system_message = f\"You are an expert in {domain}. Provide detailed, accurate information about {domain} topics.\" orchestrator.create_agent( agent_id=agent_id, model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), system_message=system_message ) return agent_id # Use the function to get or create a specialized agent physics_agent_id = await get_specialized_agent(\"physics\") response = await orchestrator.run( agent_id=physics_agent_id, message=\"Explain quantum field theory in simple terms\" ) print(response) . Agent Teams . You can organize agents into teams for complex tasks: . class AgentTeam: def __init__(self, orchestrator, team_name, agent_ids): self.orchestrator = orchestrator self.team_name = team_name self.agent_ids = agent_ids async def run_team_task(self, task, coordinator_id=None): results = {} # Distribute the task to each team member for agent_id in self.agent_ids: results[agent_id] = await self.orchestrator.run( agent_id=agent_id, message=f\"Task for {agent_id}: {task}\" ) # If a coordinator is specified, have them synthesize the results if coordinator_id: synthesis_message = f\"Synthesize the following team outputs for task '{task}':\\n\\n\" for agent_id, result in results.items(): synthesis_message += f\"=== {agent_id} ===\\n{result}\\n\\n\" final_result = await self.orchestrator.run( agent_id=coordinator_id, message=synthesis_message ) return final_result return results # Create a research team research_team = AgentTeam( orchestrator=orchestrator, team_name=\"Research Team\", agent_ids=[\"data_collector\", \"analyst\", \"writer\"] ) # Run a team task result = await research_team.run_team_task( task=\"Research the impact of artificial intelligence on healthcare\", coordinator_id=\"research_director\" ) print(result) . ",
    "url": "/muxi/orchestrator/#advanced-features",
    
    "relUrl": "/orchestrator/#advanced-features"
  },"151": {
    "doc": "Orchestrator",
    "title": "Persistence and State Management",
    "content": "The Orchestrator can persist agent states to allow for long-running operations: . import json import os class PersistentOrchestrator(Orchestrator): def __init__(self, state_file=\"orchestrator_state.json\"): super().__init__() self.state_file = state_file self.load_state() def save_state(self): state = { \"agents\": list(self.agents.keys()), \"default_agent\": self.default_agent } with open(self.state_file, \"w\") as f: json.dump(state, f) def load_state(self): if not os.path.exists(self.state_file): return with open(self.state_file, \"r\") as f: state = json.load(f) # Recreate agents (simplified - in practice you'd need to store and restore more state) for agent_id in state.get(\"agents\", []): if agent_id not in self.agents: self.create_agent(agent_id=agent_id) # Restore default agent if state.get(\"default_agent\"): self.default_agent = state[\"default_agent\"] def create_agent(self, agent_id, **kwargs): super().create_agent(agent_id, **kwargs) self.save_state() def delete_agent(self, agent_id): super().delete_agent(agent_id) self.save_state() def set_default_agent(self, agent_id): super().set_default_agent(agent_id) self.save_state() # Use the persistent orchestrator persistent_orchestrator = PersistentOrchestrator() . ",
    "url": "/muxi/orchestrator/#persistence-and-state-management",
    
    "relUrl": "/orchestrator/#persistence-and-state-management"
  },"152": {
    "doc": "Orchestrator",
    "title": "Best Practices",
    "content": ". | Agent Management: Create agents with clear, distinct roles and responsibilities . | Resource Allocation: Be mindful of resource usage when running multiple agents simultaneously . | Error Handling: Implement robust error handling to prevent failures in one agent from affecting others . | State Management: Consider implementing persistence for important agent states . | Security: Implement appropriate access controls if multiple users are interacting with different agents . | . ",
    "url": "/muxi/orchestrator/#best-practices",
    
    "relUrl": "/orchestrator/#best-practices"
  },"153": {
    "doc": "Orchestrator",
    "title": "Troubleshooting",
    "content": "Agent Creation Issues . | Ensure all required parameters are provided | Check for duplicate agent IDs | Verify that the LLM provider is properly configured | . Communication Issues . | Ensure messages are being routed to the correct agent | Check for rate limiting or quota issues with LLM providers | Verify that shared resources (e.g., memory) are accessible | . Performance Issues . | Consider using a thread pool for parallel agent execution | Implement caching for frequently requested information | Monitor resource usage and scale accordingly | . ",
    "url": "/muxi/orchestrator/#troubleshooting",
    
    "relUrl": "/orchestrator/#troubleshooting"
  },"154": {
    "doc": "Orchestrator",
    "title": "Next Steps",
    "content": "After setting up your Orchestrator and agents, you might want to explore: . | Implementing custom tools for specialized agent capabilities | Setting up memory systems for improved agent recall | Creating a WebSocket connection for real-time agent interaction | Exploring MCP features for advanced LLM control | . ",
    "url": "/muxi/orchestrator/#next-steps",
    
    "relUrl": "/orchestrator/#next-steps"
  },"155": {
    "doc": "Orchestrator",
    "title": "Orchestrator",
    "content": " ",
    "url": "/muxi/orchestrator/",
    
    "relUrl": "/orchestrator/"
  },"156": {
    "doc": "Introduction",
    "title": "MUXI Framework Overview",
    "content": " ",
    "url": "/muxi/overview/#muxi-framework-overview",
    
    "relUrl": "/overview/#muxi-framework-overview"
  },"157": {
    "doc": "Introduction",
    "title": "Introduction",
    "content": "The MUXI Framework is a powerful, extensible system designed for building intelligent agents with memory persistence, tool integration, and real-time communication capabilities. This document provides a comprehensive overview of the framework’s architecture, components, and how they interact to create a cohesive system. ",
    "url": "/muxi/overview/",
    
    "relUrl": "/overview/"
  },"158": {
    "doc": "Introduction",
    "title": "Core Architecture",
    "content": "The framework follows a modular design where specialized components work together to enable complex agent behaviors: . ",
    "url": "/muxi/overview/#core-architecture",
    
    "relUrl": "/overview/#core-architecture"
  },"159": {
    "doc": "Introduction",
    "title": "Key Components",
    "content": "1. Server Layer . The Server layer is the entry point for all external communication with the framework. It encompasses: . | REST API: Provides endpoints for agent management, chat operations, and memory interactions | WebSocket Server: Enables real-time, bidirectional communication for streaming responses | Web App: A frontend interface for interacting with agents visually | CLI: A command-line interface for text-based interaction | . All these interfaces connect to the same underlying orchestrator, ensuring consistent behavior regardless of how users interact with the system. 2. Orchestrator . The Orchestrator is the central coordination mechanism that: . | Manages the lifecycle of all agents in the system | Routes messages to the appropriate agent | Handles agent creation, configuration, and removal | Provides a unified interface for all client applications | Coordinates multi-agent interactions when needed | . 3. Agent . Agents are the intelligent entities that process information and produce responses. Each Agent: . | Integrates with a specific LLM provider | Maintains its memory systems (buffer and long-term) | Has access to tools for extending its capabilities | Processes messages according to its system instructions | Can be specialized for particular tasks or domains | . 4. Model Context Protocol (MCP) . The MCP is a standardized communication layer between agents and LLMs that: . | Provides a consistent interface regardless of the underlying LLM provider | Handles specialized message formatting for tool usage | Manages message parsing and serialization | Supports tool calling, function execution, and response handling | . 5. Memory Systems . The framework includes a sophisticated memory architecture: . | Buffer Memory (FAISS): Short-term contextual memory for maintaining conversation flow | Long-Term Memory (PostgreSQL with pgvector): Persistent storage of important information | Memobase: A multi-user aware memory system that partitions memories by user ID | . 6. LLM Providers . The framework supports multiple LLM providers through adapter classes: . | OpenAI (GPT models) | Anthropic (Claude models) | Local models via Ollama | Expandable to additional providers | . 7. Tool System . The Tool system extends agent capabilities through: . | Built-in Tools: File operations, web search, calculators, etc. | Custom Tools: User-defined tools for specialized functionality | Tool Registry: Central registration and discovery mechanism | Standardized Execution: Consistent pattern for tool invocation and result handling | . ",
    "url": "/muxi/overview/#key-components",
    
    "relUrl": "/overview/#key-components"
  },"160": {
    "doc": "Introduction",
    "title": "Data Flow",
    "content": ". | Client Requests: Enter the system through one of the interfaces (REST, WebSocket, CLI, Web App) | Server Processing: The server layer validates and routes requests to the Orchestrator | Orchestration: The Orchestrator identifies the target agent and forwards the message | Agent Processing: . | The agent retrieves relevant context from memory | Formulates a prompt for the LLM using the MCP format | Sends the prompt to the LLM provider | . | LLM Interaction: . | The LLM processes the prompt and generates a response | If tool calls are required, they are identified and extracted | . | Tool Execution: . | Tool calls are routed to the appropriate tool implementations | Tools execute with the provided parameters | Results are returned to the agent | . | Response Formulation: . | The agent incorporates tool results if applicable | Formulates the final response | . | Memory Updates: . | The conversation is stored in buffer memory | Important information is persisted to long-term memory | . | Client Response: The response is returned to the client through the original interface | . ",
    "url": "/muxi/overview/#data-flow",
    
    "relUrl": "/overview/#data-flow"
  },"161": {
    "doc": "Introduction",
    "title": "Multi-User Support",
    "content": "The framework provides robust multi-user capabilities through: . | User ID Tracking: All interfaces support user identification | Memobase Integration: Memory partitioning based on user ID | Per-User Context: Each user maintains their own conversation context | Memory Operations: Search and clear functions support user-specific scoping | . ",
    "url": "/muxi/overview/#multi-user-support",
    
    "relUrl": "/overview/#multi-user-support"
  },"162": {
    "doc": "Introduction",
    "title": "Real-Time Communication",
    "content": "WebSocket support enables sophisticated real-time features: . | Streaming Responses: Partial responses can be sent as they’re generated | Tool Execution Visibility: Clients can see when tools are being executed | Connection Management: Automatic reconnection and state recovery | Subscription Model: Clients can subscribe to specific agents | . ",
    "url": "/muxi/overview/#real-time-communication",
    
    "relUrl": "/overview/#real-time-communication"
  },"163": {
    "doc": "Introduction",
    "title": "Extension Points",
    "content": "The framework is designed for extensibility at multiple levels: . | New LLM Providers: Implement the LLM interface for new providers | Custom Tools: Create tools that implement the BaseTool interface | Memory Implementations: Alternative memory storage systems | UI Customizations: The web interface can be customized | Custom Agents: Specialized agent implementations for specific domains | . ",
    "url": "/muxi/overview/#extension-points",
    
    "relUrl": "/overview/#extension-points"
  },"164": {
    "doc": "Introduction",
    "title": "Integration Patterns",
    "content": "The framework can be integrated into larger systems through: . | REST API Integration: For traditional request-response patterns | WebSocket Integration: For real-time communication needs | Library Usage: Direct integration at the code level | Containerized Deployment: For cloud and microservice architectures | . ",
    "url": "/muxi/overview/#integration-patterns",
    
    "relUrl": "/overview/#integration-patterns"
  },"165": {
    "doc": "Introduction",
    "title": "Conclusion",
    "content": "The MUXI Framework provides a comprehensive solution for building, deploying, and managing intelligent agents. Its modular design allows for extensive customization while maintaining a consistent core architecture. By separating concerns between orchestration, agent behavior, memory systems, and tool execution, the framework enables building sophisticated agent-based applications with minimal overhead. Whether you’re building a simple chatbot, a complex multi-agent system, or integrating AI capabilities into existing applications, the framework provides the necessary building blocks and extension points to realize your vision. ",
    "url": "/muxi/overview/#conclusion",
    
    "relUrl": "/overview/#conclusion"
  },"166": {
    "doc": "Quick Start",
    "title": "Quick Start Guide",
    "content": "This guide will help you quickly get started with the MUXI Framework using a simplified configuration-based approach. For more detailed configuration options, see the Configuration Guide. ",
    "url": "/muxi/quick-start/#quick-start-guide",
    
    "relUrl": "/quick-start/#quick-start-guide"
  },"167": {
    "doc": "Quick Start",
    "title": "Installation",
    "content": "Install the MUXI Framework from GitHub: . git clone https://github.com/your-organization/muxi.git cd muxi pip install -e . ",
    "url": "/muxi/quick-start/#installation",
    
    "relUrl": "/quick-start/#installation"
  },"168": {
    "doc": "Quick Start",
    "title": "Set Up Environment Variables",
    "content": "Create a .env file with your API keys: . OPENAI_API_KEY=your-openai-key DATABASE_URL=your-database-connection-string . ",
    "url": "/muxi/quick-start/#set-up-environment-variables",
    
    "relUrl": "/quick-start/#set-up-environment-variables"
  },"169": {
    "doc": "Quick Start",
    "title": "Create Agent Configuration Files",
    "content": "YAML Configuration Example . Create a file agents/weather_agent.yaml: . name: weather_assistant description: \"Specialized in providing weather forecasts and current conditions.\" system_message: You are a helpful assistant that can check the weather. model: provider: openai api_key: \"${OPENAI_API_KEY}\" model: gpt-4o temperature: 0.7 memory: buffer: 10 # Sets buffer window size to 10 long_term: true # Enables long-term memory tools: - enable_calculator - enable_web_search . JSON Configuration Example . Alternatively, create agents/finance_agent.json: . { \"name\": \"finance_assistant\", \"description\": \"Expert in financial analysis, investments, and market trends.\", \"system_message\": \"You are a helpful finance assistant.\", \"model\": { \"provider\": \"openai\", \"api_key\": \"${OPENAI_API_KEY}\", \"model\": \"gpt-4o\", \"temperature\": 0.2 }, \"memory\": { \"buffer\": 5, \"long_term\": false }, \"tools\": [ \"enable_calculator\" ] } . ",
    "url": "/muxi/quick-start/#create-agent-configuration-files",
    
    "relUrl": "/quick-start/#create-agent-configuration-files"
  },"170": {
    "doc": "Quick Start",
    "title": "Automatic Agent Selection",
    "content": "When you add multiple agents to your application, MUXI will automatically route messages to the most appropriate agent based on their descriptions: . from src import muxi # Create a new MUXI instance app = muxi() # Add multiple agents with different specializations app.add_agent(\"weather\", \"agents/weather_agent.yaml\") app.add_agent(\"finance\", \"agents/finance_agent.json\") # Let the orchestrator automatically select the appropriate agent response = app.chat(\"What's the current stock market trend?\") print(response) # Will likely be handled by the finance agent response = app.chat(\"What's the weather in New York?\") print(response) # Will likely be handled by the weather agent . The routing system uses a dedicated LLM to analyze the message content and agent descriptions to determine the best match. You can configure this behavior with environment variables: . # Routing LLM configuration ROUTING_LLM=openai ROUTING_LLM_MODEL=gpt-4o-mini ROUTING_LLM_TEMPERATURE=0.0 ROUTING_USE_CACHING=true . ",
    "url": "/muxi/quick-start/#automatic-agent-selection",
    
    "relUrl": "/quick-start/#automatic-agent-selection"
  },"171": {
    "doc": "Quick Start",
    "title": "Create Your Application",
    "content": "Create a new Python file with minimal code: . from dotenv import load_dotenv from src import muxi # Load environment variables load_dotenv() # Initialize MUXI - no connection string needed # It will be loaded from DATABASE_URL when required app = muxi() # Add agents from configuration files app.add_agent(\"weather\", \"agents/weather_agent.yaml\") app.add_agent(\"finance\", \"agents/finance_agent.json\") # Option 1: Interactive usage # Chat with a specific agent response = app.chat(\"What's the weather in New York?\", agent_name=\"weather\") print(response) # Let the orchestrator automatically select the appropriate agent response = app.chat(\"What's the current stock market trend?\") print(response) # Will likely be handled by the finance agent # Chat with user-specific context response = app.chat(\"What's the weather in my city?\") print(response) # Will use Alice's location data # Option 2: Start server and web UI app.run() . ",
    "url": "/muxi/quick-start/#create-your-application",
    
    "relUrl": "/quick-start/#create-your-application"
  },"172": {
    "doc": "Quick Start",
    "title": "Multi-User Support",
    "content": "For multi-user applications: . user_id = 123 # Add domain knowledge for a specific user (if not already added in the past) knowledge = { \"name\": \"Alice\", \"location\": {\"city\": \"New York\"} } app.add_user_domain_knowledge(user_id, knowledge) # Chat with user-specific context response = app.chat(\"weather\", \"What's the weather in my city?\", user_id=user_id) print(response) # Will use Alice's location data . ",
    "url": "/muxi/quick-start/#multi-user-support",
    
    "relUrl": "/quick-start/#multi-user-support"
  },"173": {
    "doc": "Quick Start",
    "title": "Using MCP Servers",
    "content": "To integrate external Model Context Protocol servers: . mcp_servers: - name: weather_api url: http://localhost:5001 credentials: - id: weather_api_key param_name: api_key required: true . For credentials, the framework will: . | First look for user-specific credentials in the database | Then look for system-wide credentials in the database | Finally, fall back to environment variables | . ",
    "url": "/muxi/quick-start/#using-mcp-servers",
    
    "relUrl": "/quick-start/#using-mcp-servers"
  },"174": {
    "doc": "Quick Start",
    "title": "Advanced Usage",
    "content": "See the Configuration Guide for advanced options and API Reference for the complete API. ",
    "url": "/muxi/quick-start/#advanced-usage",
    
    "relUrl": "/quick-start/#advanced-usage"
  },"175": {
    "doc": "Quick Start",
    "title": "Quick Start",
    "content": " ",
    "url": "/muxi/quick-start/",
    
    "relUrl": "/quick-start/"
  },"176": {
    "doc": "Quickstart Guide",
    "title": "Quick Start Guide",
    "content": "This guide will help you quickly set up and start using MUXI for building AI agents. ",
    "url": "/muxi/quickstart/#quick-start-guide",
    
    "relUrl": "/quickstart/#quick-start-guide"
  },"177": {
    "doc": "Quickstart Guide",
    "title": "Installation",
    "content": "# Clone the repository git clone https://github.com/ranaroussi/muxi.git cd muxi # Install dependencies pip install -r requirements.txt . ",
    "url": "/muxi/quickstart/#installation",
    
    "relUrl": "/quickstart/#installation"
  },"178": {
    "doc": "Quickstart Guide",
    "title": "Basic Usage",
    "content": "Creating Agents with Python . from src.core.orchestrator import Orchestrator from src.models.openai import OpenAIModel from src.memory.buffer import BufferMemory from src.memory.long_term import LongTermMemory from src.memory.memobase import Memobase # Create an orchestrator to manage agents orchestrator = Orchestrator() # Create a basic agent with buffer memory orchestrator.create_agent( agent_id=\"assistant\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), system_message=\"You are a helpful AI assistant.\" ) # Send a message to the agent response = orchestrator.chat(\"assistant\", \"Hello, can you help me with a Python question?\") print(response) . Working with Advanced Memory Systems . # Create an agent with long-term memory long_term_memory = LongTermMemory() orchestrator.create_agent( agent_id=\"researcher\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), long_term_memory=long_term_memory, system_message=\"You are a helpful research assistant.\" ) # Create an agent with multi-user support long_term_memory = LongTermMemory() memobase = Memobase(long_term_memory=long_term_memory) orchestrator.create_agent( agent_id=\"multi_user_assistant\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), long_term_memory=memobase, system_message=\"You are a helpful assistant that supports multiple users.\" ) # Chat with a multi-user agent (specify user_id) response = orchestrator.chat(\"multi_user_assistant\", \"Remember that my name is Alice\", user_id=123) print(response) . ",
    "url": "/muxi/quickstart/#basic-usage",
    
    "relUrl": "/quickstart/#basic-usage"
  },"179": {
    "doc": "Quickstart Guide",
    "title": "Using the Command Line Interface",
    "content": "MUXI comes with a rich command-line interface for interacting with agents: . # Start the CLI python -m src.cli # Start a chat session with an agent python -m src.cli chat --agent-id assistant # Send a one-off message to an agent python -m src.cli send --agent-id assistant \"What is the capital of France?\" . ",
    "url": "/muxi/quickstart/#using-the-command-line-interface",
    
    "relUrl": "/quickstart/#using-the-command-line-interface"
  },"180": {
    "doc": "Quickstart Guide",
    "title": "Starting the API Server",
    "content": "To use the REST API and WebSocket server: . # Start the API server python -m src.cli api # Start both API server and web UI python -m src.cli run # or simply python -m src . ",
    "url": "/muxi/quickstart/#starting-the-api-server",
    
    "relUrl": "/quickstart/#starting-the-api-server"
  },"181": {
    "doc": "Quickstart Guide",
    "title": "Using the WebSocket Interface",
    "content": "Connect to the WebSocket server from JavaScript: . // Browser WebSocket client example const socket = new WebSocket('ws://localhost:5050/ws'); socket.onopen = () =&gt; { console.log('Connected to WebSocket server'); // Set user ID (for multi-user agents) socket.send(JSON.stringify({ type: 'set_user', user_id: 123 })); // Subscribe to an agent socket.send(JSON.stringify({ type: 'subscribe', agent_id: 'agent' })); // Send a message socket.send(JSON.stringify({ type: 'chat', message: 'Tell me about artificial intelligence' })); }; socket.onmessage = (event) =&gt; { const response = JSON.parse(event.data); console.log('Received:', response); }; . ",
    "url": "/muxi/quickstart/#using-the-websocket-interface",
    
    "relUrl": "/quickstart/#using-the-websocket-interface"
  },"182": {
    "doc": "Quickstart Guide",
    "title": "Using the REST API",
    "content": "Create a new agent: . curl -X POST http://localhost:5050/agents \\ -H \"Content-Type: application/json\" \\ -d '{ \"agent_id\": \"agent\", \"system_message\": \"You are a helpful AI assistant.\" }' . Send a message to the agent: . curl -X POST http://localhost:5050/chat/agent \\ -H \"Content-Type: application/json\" \\ -d '{ \"message\": \"Hello, what can you help me with today?\" }' . ",
    "url": "/muxi/quickstart/#using-the-rest-api",
    
    "relUrl": "/quickstart/#using-the-rest-api"
  },"183": {
    "doc": "Quickstart Guide",
    "title": "Next Steps",
    "content": "Now that you’ve set up MUXI and created your first agents, check out these resources to learn more: . | Architecture Overview to understand how MUXI components fit together | Agent Guide for details on agent capabilities and configuration | Memory Systems to learn about different memory options | Tools Overview to extend your agents with additional capabilities | . ",
    "url": "/muxi/quickstart/#next-steps",
    
    "relUrl": "/quickstart/#next-steps"
  },"184": {
    "doc": "Quickstart Guide",
    "title": "Quickstart Guide",
    "content": " ",
    "url": "/muxi/quickstart/",
    
    "relUrl": "/quickstart/"
  },"185": {
    "doc": "Resources",
    "title": "Resources",
    "content": "Additional resources for MUXI users and contributors. ",
    "url": "/muxi/resources/",
    
    "relUrl": "/resources/"
  },"186": {
    "doc": "Resources",
    "title": "In this section",
    "content": ". | Contributing to MUXI - Learn how to contribute to the MUXI project | Roadmap - See what’s planned for future MUXI releases | Documentation Progress - View the organization of MUXI documentation | . Reference . | Agents vs Tools - Understand the difference between agents and tools | Tools vs MCP - Learn the difference between tools and MCP | . ",
    "url": "/muxi/resources/#in-this-section",
    
    "relUrl": "/resources/#in-this-section"
  },"187": {
    "doc": "Development Roadmap",
    "title": "Development Roadmap",
    "content": "This document outlines the high-level strategic vision and planned future development of the muxi framework. For detailed developer tasks and implementation checkboxes, see the NEXT_STEPS file in the project root. ",
    "url": "/muxi/roadmap/",
    
    "relUrl": "/roadmap/"
  },"188": {
    "doc": "Development Roadmap",
    "title": "Short-term Goals",
    "content": ". | Complete core agent functionality ✅ | Implement basic memory systems ✅ | Add essential built-in tools ✅ | Finalize REST API specification ✅ | Implement WebSocket communication ✅ | Add user-specific domain knowledge support ✅ | Implement service-oriented architecture | Create client-server model for remote operation | Add support for hybrid communication protocol (HTTP/SSE/WebSockets) | . ",
    "url": "/muxi/roadmap/#short-term-goals",
    
    "relUrl": "/roadmap/#short-term-goals"
  },"189": {
    "doc": "Development Roadmap",
    "title": "Medium-term Goals",
    "content": ". | Enhance LLM provider support | Improve agent orchestration capabilities ✅ | Add advanced memory persistence ✅ | Develop more sophisticated built-in tools | Create comprehensive documentation | Implement modular packaging strategy | Develop client libraries for multiple languages | . ",
    "url": "/muxi/roadmap/#medium-term-goals",
    
    "relUrl": "/roadmap/#medium-term-goals"
  },"190": {
    "doc": "Development Roadmap",
    "title": "Long-term Vision",
    "content": ". | Support multi-modal agents | Implement distributed orchestration | Create an agent marketplace | Build advanced analytics and monitoring | Develop managed SaaS offering | . ",
    "url": "/muxi/roadmap/#long-term-vision",
    
    "relUrl": "/roadmap/#long-term-vision"
  },"191": {
    "doc": "Development Roadmap",
    "title": "Planned Features",
    "content": "Service-Oriented Architecture . Transform the framework into a flexible, distributed system: . | Client-Server Model . | Local and remote operation with the same API | Flexible authentication mechanisms | Connection management utilities | Support for multiple connection profiles | . | Modular Packaging . | Core package with minimal dependencies | Server package with full capabilities | CLI package for remote connections | Web package for browser-based access | Installation options for different use cases | . | Hybrid Communication Protocol . | HTTP for standard API requests | SSE (Server-Sent Events) for streaming responses | WebSockets for multi-modal and continuous interactions | . | Authentication Implementation . | API key authentication | Auto-generated keys with one-time display | Environment variable configuration | Explicit auth configuration options | . | MCP Server Unification . | Tool system based on MCP servers | Adapters for local Python tools | Service discovery mechanisms | Deployment utilities | . | . Multi-Modal Agent Capabilities . Transform agents into omni agents capable of handling various media types: . | Image processing and understanding . | Support for image attachments in messages | Integration with vision-capable models (GPT-4V, Claude 3, Gemini) | Image preprocessing and optimization | . | Audio processing . | Audio file handling and transcription | Streaming audio input/output capabilities | Real-time speech-to-text and text-to-speech | Support for voice conversations with agents | . | Document processing . | PDF, Word, and other document format support | Text extraction and semantic understanding | OCR for scanned documents | . | Video capabilities (long-term) . | Video file processing and frame extraction | Streaming video support for real-time interactions | Video content analysis and understanding | . | Media storage and retrieval system . | Efficient storage of media attachments | Caching mechanisms for frequently accessed media | Support for cloud storage integration (S3, etc.) | . | Unified streaming protocol . | Bi-directional streaming for audio/video | WebRTC integration for real-time media communication | WebSocket extensions for streaming media | . | . Docker Distribution . Develop a comprehensive Docker image strategy: . | Create a base image with core framework functionality | Bundle popular community-provided MCP servers | Implement a plugin architecture for easy MCP server addition | Support layered customization options: . | Use base image as-is with common MCP servers | Extend the image with additional MCP servers | Mount custom MCP server code at runtime | Build custom images from the base | . | Provide clear documentation for customization | Establish a community contribution process for MCP servers | Consider maintaining a registry of verified MCP servers | . Memory System Enhancements . | Implement vector database integration ✅ | Add support for structured knowledge storage ✅ | Add support for user-specific domain knowledge ✅ | Implement robust database schema with optimized indexes ✅ | Create migration system for schema version control ✅ | Develop memory pruning/summarization | Create memory visualization tools | . Tool Ecosystem . | Implement base tool interface ✅ | Create tool registry for managing tools ✅ | Add example tools (Calculator, Web Search) ✅ | Develop a tool marketplace | Implement tool versioning | Create a tool testing framework | Standardize tool documentation | . UI/UX Improvements . | CLI interface with rich terminal-based interaction ✅ | Initial web dashboard ✅ | Enhanced web dashboard | Agent performance analytics | Real-time monitoring | Conversation visualization | . Real-Time Communication . | WebSocket server for real-time agent interaction ✅ | Message serialization for MCP messages ✅ | Shared orchestrator instance between REST and WebSocket ✅ | Resilient connection handling with automatic reconnection ✅ | Comprehensive error handling ✅ | Multi-user WebSocket connection support ✅ | . Developer Experience . | Interactive MCP server generator through CLI (muxi create mcp-server) ✅ | Scaffolding for custom MCP servers with best practices ✅ | Template-based code generation for common MCP server patterns ✅ | Built-in testing utilities for MCP servers | Documentation generator for MCP servers | MCP server validation tools | Connection utilities for remote server development | Client libraries for multiple programming languages | . Deployment and Operations . | Kubernetes operator | Cloud-specific deployments | Performance benchmarking | Scalability testing | Database schema optimization and indexing ✅ | . ",
    "url": "/muxi/roadmap/#planned-features",
    
    "relUrl": "/roadmap/#planned-features"
  },"192": {
    "doc": "Development Roadmap",
    "title": "Community Contributions",
    "content": "We welcome community input on prioritization of these roadmap items. Please open GitHub issues to suggest additions or changes to the roadmap. ",
    "url": "/muxi/roadmap/#community-contributions",
    
    "relUrl": "/roadmap/#community-contributions"
  },"193": {
    "doc": "Development Roadmap",
    "title": "Version Targets",
    "content": "v0.1.0 (Initial Release) ✅ . | Basic agent functionality | Core memory systems | REST API and WebSocket support | Limited tool support | . v0.2.0 (Current) . | Enhanced orchestration ✅ . | Intelligent agent selection with LLM-based routing ✅ | Agent descriptions for specialized capabilities ✅ | Route caching for performance optimization ✅ | . | Improved memory persistence ✅ | User-specific domain knowledge ✅ | Additional built-in tools ✅ | Initial web dashboard ✅ | Real-time communication via WebSockets ✅ | Multi-user support ✅ | Robust database schema with optimized indexes ✅ | Simplified configuration-based API ✅ | Comprehensive configuration documentation ✅ | . v0.3.0 (Next) . | Service-oriented architecture | Client-server model | Hybrid communication protocol (HTTP/SSE/WebSockets) | Flexible authentication mechanisms | Initial modular packaging | Client connector for remote servers | WebSocket support for Omni capabilities | Enhanced CLI with remote connection support | . v1.0.0 . | Production-ready release | Complete documentation | Docker distribution with MCP servers | Comprehensive tool ecosystem | Adopt a more permissive open-source license | Multiple LLM provider support | Advanced user authentication and authorization | Interactive MCP server generator ✅ | Developer tooling for MCP server creation ✅ | Initial multi-modal capabilities (image and audio support) | Complete modular packaging with standalone components | Client libraries for multiple programming languages | . v1.5.0 . | Advanced multi-modal capabilities | Streaming audio support | Document processing capabilities | Media storage system enhancements | Expanded model provider integrations | . ",
    "url": "/muxi/roadmap/#version-targets",
    
    "relUrl": "/roadmap/#version-targets"
  },"194": {
    "doc": "Testing Guide",
    "title": "Testing Guide",
    "content": "This guide covers the testing approach for the MUXI Framework, including automated tests, utilities for test environment setup, and best practices. ",
    "url": "/muxi/testing/",
    
    "relUrl": "/testing/"
  },"195": {
    "doc": "Testing Guide",
    "title": "Test Structure",
    "content": "The MUXI Framework uses Python’s built-in unittest framework along with pytest for testing. Tests are organized by component and functionality: . | tests/test_agent.py - Tests for the Agent class | tests/test_memory.py - Tests for memory systems | tests/test_mcp.py - Tests for the Model Context Protocol | tests/test_orchestrator.py - Tests for the Orchestrator | tests/test_tools.py - Tests for the Tool system | tests/test_intelligent_routing.py - Tests for agent routing | . ",
    "url": "/muxi/testing/#test-structure",
    
    "relUrl": "/testing/#test-structure"
  },"196": {
    "doc": "Testing Guide",
    "title": "Environment Setup for Tests",
    "content": "When running tests, particularly those involving LLM providers like OpenAI, you need to properly set up environment variables. The framework includes a utility to streamline this process: . Using the Environment Setup Utility . The env_setup.py utility is designed to ensure consistent environment variable loading across all tests: . from tests.utils.env_setup import load_api_keys # At the beginning of your test load_api_keys() . This utility: . | Loads environment variables from the .env file with override=True | Forces API keys to be loaded directly from the .env file | Ensures consistency across different test modules | . Implementation Details . def load_api_keys(): \"\"\" Load API keys from .env file and ensure they're correctly set in the environment. This function: 1. Loads variables from .env with override=True 2. Forces the OPENAI_API_KEY to be loaded directly from the .env file \"\"\" # Load environment variables with override load_dotenv(override=True) # Force load the API key from .env file env_path = os.path.join(os.getcwd(), '.env') if os.path.exists(env_path): with open(env_path, 'r') as f: env_content = f.readlines() for line in env_content: if line.strip().startswith(\"OPENAI_API_KEY=\"): key = line.strip().split(\"=\", 1)[1] os.environ[\"OPENAI_API_KEY\"] = key break # Also load other API keys if needed elif line.strip().startswith(\"ANTHROPIC_API_KEY=\"): key = line.strip().split(\"=\", 1)[1] os.environ[\"ANTHROPIC_API_KEY\"] = key elif line.strip().startswith(\"MISTRAL_API_KEY=\"): key = line.strip().split(\"=\", 1)[1] os.environ[\"MISTRAL_API_KEY\"] = key . ",
    "url": "/muxi/testing/#environment-setup-for-tests",
    
    "relUrl": "/testing/#environment-setup-for-tests"
  },"197": {
    "doc": "Testing Guide",
    "title": "Running Tests",
    "content": "To run all tests: . pytest . To run a specific test module: . pytest tests/test_agent.py . To run a specific test function: . pytest tests/test_agent.py::test_create_agent . ",
    "url": "/muxi/testing/#running-tests",
    
    "relUrl": "/testing/#running-tests"
  },"198": {
    "doc": "Testing Guide",
    "title": "Test Environment Configuration",
    "content": "Create a .env file in the project root with your API keys: . # LLM API Keys (use dummy keys for testing if needed) OPENAI_API_KEY=your_openai_api_key_here ANTHROPIC_API_KEY=your_anthropic_api_key_here MISTRAL_API_KEY=your_mistral_api_key_here # Database Configuration DATABASE_URL=postgresql://username:password@localhost:5432/muxi_test . For automated testing environments, you may use dummy API keys for providers you don’t need to test. ",
    "url": "/muxi/testing/#test-environment-configuration",
    
    "relUrl": "/testing/#test-environment-configuration"
  },"199": {
    "doc": "Testing Guide",
    "title": "Mocking External Services",
    "content": "When testing components that depend on external services (like LLM providers), use mocks to avoid actual API calls: . import unittest from unittest.mock import patch, MagicMock class TestLLMModel(unittest.TestCase): @patch('src.models.providers.openai.OpenAIModel.chat') def test_chat_functionality(self, mock_chat): # Configure the mock mock_chat.return_value = \"Mocked response from the LLM\" # Test your component that uses the LLM # ... ",
    "url": "/muxi/testing/#mocking-external-services",
    
    "relUrl": "/testing/#mocking-external-services"
  },"200": {
    "doc": "Testing Guide",
    "title": "Best Practices",
    "content": ". | Isolate tests: Each test should be independent and not rely on the state from other tests | Use fixtures: Create reusable test fixtures for common setup code | Clean up resources: Always clean up resources (like database connections) after tests | Test edge cases: Include tests for error conditions and edge cases | Use consistent API key handling: Always use the environment setup utility for API keys | . ",
    "url": "/muxi/testing/#best-practices",
    
    "relUrl": "/testing/#best-practices"
  },"201": {
    "doc": "Testing Guide",
    "title": "Continuous Integration",
    "content": "The framework uses GitHub Actions for continuous integration. Tests are automatically run on pull requests and pushes to the main branch. ",
    "url": "/muxi/testing/#continuous-integration",
    
    "relUrl": "/testing/#continuous-integration"
  },"202": {
    "doc": "Documentation Progress",
    "title": "Documentation Progress",
    "content": "This document tracks the progress of the MUXI documentation. It shows which documentation sections have been completed and which are still in development. This serves as both a roadmap for contributors looking to help with documentation and a guide for users to understand what documentation is currently available. ",
    "url": "/muxi/toc/",
    
    "relUrl": "/toc/"
  },"203": {
    "doc": "Documentation Progress",
    "title": "1. Getting Started",
    "content": ". | Introduction | Quick Start Guide | Architecture | Installation &amp; Setup | Architecture Evolution | . ",
    "url": "/muxi/toc/#1-getting-started",
    
    "relUrl": "/toc/#1-getting-started"
  },"204": {
    "doc": "Documentation Progress",
    "title": "2. Core Concepts",
    "content": ". | Agents &amp; Orchestration . | Agents | Orchestrator | . | Memory . | Memory | Memory Types (Buffer, Long-term, Memobase) | Multi-User Memory Management | . | Tools . | Tools | Creating Custom Tools | . | LLM Integration . | Models | MCP | LLM Provider Integration | . | Reference . | Agents vs Tools | Tools vs MCP | . | . ",
    "url": "/muxi/toc/#2-core-concepts",
    
    "relUrl": "/toc/#2-core-concepts"
  },"205": {
    "doc": "Documentation Progress",
    "title": "3. User Interfaces",
    "content": ". | Command Line Interface . | Overview | Commands Reference | Chat Sessions | Server Management | . | Web Dashboard . | Overview | Features &amp; Navigation | Agent Management | Chat Interface | . | REST API . | API | Endpoints Reference | Authentication | Error Handling | . | WebSocket Interface . | WebSocket | Real-time Communication | Message Types | Connection Management | . | . ",
    "url": "/muxi/toc/#3-user-interfaces",
    
    "relUrl": "/toc/#3-user-interfaces"
  },"206": {
    "doc": "Documentation Progress",
    "title": "4. Advanced Topics",
    "content": ". | Multi-Agent Orchestration . | Overview | Collaboration Patterns | Message Routing | . | Multi-User Support . | Overview | User Management | Memory Partitioning | . | Service-Oriented Architecture . | Client-Server Model | Hybrid Communication Protocol | Authentication | Streaming Responses | WebSocket for Omni Capabilities | . | . ",
    "url": "/muxi/toc/#4-advanced-topics",
    
    "relUrl": "/toc/#4-advanced-topics"
  },"207": {
    "doc": "Documentation Progress",
    "title": "5. Reference",
    "content": ". | API Reference | Configuration Options | Message Formats | Troubleshooting Guide | . ",
    "url": "/muxi/toc/#5-reference",
    
    "relUrl": "/toc/#5-reference"
  },"208": {
    "doc": "Documentation Progress",
    "title": "6. Resources",
    "content": ". | Contributing to MUXI | Roadmap | License Information | Example Projects | . ",
    "url": "/muxi/toc/#6-resources",
    
    "relUrl": "/toc/#6-resources"
  },"209": {
    "doc": "Documentation Progress",
    "title": "Setup and Configuration",
    "content": ". | Quick Start Guide | Configuration Guide | Testing Guide | . ",
    "url": "/muxi/toc/#setup-and-configuration",
    
    "relUrl": "/toc/#setup-and-configuration"
  },"210": {
    "doc": "Documentation Progress",
    "title": "Components",
    "content": ". | Orchestrator | Agents | Memory Systems | Domain Knowledge | Tool System | Model Context Protocol (MCP) | Multi-Modal Capabilities | Architecture Evolution | . ",
    "url": "/muxi/toc/#components",
    
    "relUrl": "/toc/#components"
  },"211": {
    "doc": "Tools vs MCP",
    "title": "Tools versus MCP Servers",
    "content": "This document outlines the key differences, trade-offs, and use cases for using internal Tools versus external MCP Servers in the MUXI Framework. ",
    "url": "/muxi/tools-vs-mcp/#tools-versus-mcp-servers",
    
    "relUrl": "/tools-vs-mcp/#tools-versus-mcp-servers"
  },"212": {
    "doc": "Tools vs MCP",
    "title": "Key Concepts",
    "content": "Internal Tools . Internal tools are native implementations within the framework that provide specific capabilities to agents. They are: . | Directly integrated into the framework codebase | Called as internal functions | Managed by the framework’s tool registry | . MCP Servers . MCP (Model Context Protocol) Servers are external services that expose capabilities via a standardized API. They: . | Run as separate services (potentially on different machines) | Communicate via HTTP/WebSocket | Follow the MCP specification for request/response formats | Can be used by multiple different AI systems that support the MCP protocol | . ",
    "url": "/muxi/tools-vs-mcp/#key-concepts",
    
    "relUrl": "/tools-vs-mcp/#key-concepts"
  },"213": {
    "doc": "Tools vs MCP",
    "title": "Architectural Differences",
    "content": "| Aspect | Internal Tools | MCP Servers | . | Integration | Direct code integration | Network API calls | . | Hosting | Within framework process | Separate service | . | Communication | Function calls | HTTP/WebSocket | . | Deployment | Part of main application | Independent deployment | . | Reusability | Framework-specific | Cross-platform | . | Extension | Requires code changes | Plug-and-play | . ",
    "url": "/muxi/tools-vs-mcp/#architectural-differences",
    
    "relUrl": "/tools-vs-mcp/#architectural-differences"
  },"214": {
    "doc": "Tools vs MCP",
    "title": "Performance Considerations",
    "content": "Internal tools generally offer better performance due to: . | No network latency | No HTTP request/response overhead | No serialization/deserialization costs | Direct function calls without protocol overhead | No authentication handshakes | Shared memory access | . MCP servers may introduce performance overhead, particularly noticeable in: . | High-frequency tool calls | Tools that process large data volumes | Low-latency requirements | Tools with quick execution times (where network overhead becomes the dominant cost) | . However, the performance difference may be negligible when: . | The tool’s actual execution time is long (e.g., complex analysis) | Call frequency is low | The MCP server is hosted very close to your application | . ",
    "url": "/muxi/tools-vs-mcp/#performance-considerations",
    
    "relUrl": "/tools-vs-mcp/#performance-considerations"
  },"215": {
    "doc": "Tools vs MCP",
    "title": "Use Case Recommendations",
    "content": "When to Use Internal Tools . | Performance-critical operations | Core functionality that rarely changes | Proprietary capabilities specific to your use case | Tightly coupled with framework internals | Simple utilities with minimal dependencies | . When to Use MCP Servers . | Widely used capabilities (GitHub, web search, etc.) | Functionality that may be shared across multiple AI systems | When offering a SaaS where customers need to extend functionality | Complex tools with many dependencies | Tools that require regular updates | When security isolation is important | . ",
    "url": "/muxi/tools-vs-mcp/#use-case-recommendations",
    
    "relUrl": "/tools-vs-mcp/#use-case-recommendations"
  },"216": {
    "doc": "Tools vs MCP",
    "title": "Implementation Strategy",
    "content": "A balanced approach would be: . | Implement core, performance-sensitive functionality as internal tools | Use MCP servers for: . | Common, standardized capabilities | Customer extension points | Complex tools with many dependencies | Tools that might be reused across different AI systems | . | . ",
    "url": "/muxi/tools-vs-mcp/#implementation-strategy",
    
    "relUrl": "/tools-vs-mcp/#implementation-strategy"
  },"217": {
    "doc": "Tools vs MCP",
    "title": "SaaS Considerations",
    "content": "When offering this framework as a SaaS: . | Internal tools provide the core capabilities | MCP servers provide the extension mechanism for customers | Customers can develop their own MCP servers without needing access to your codebase | You can maintain a marketplace or registry of third-party MCP servers | . ",
    "url": "/muxi/tools-vs-mcp/#saas-considerations",
    
    "relUrl": "/tools-vs-mcp/#saas-considerations"
  },"218": {
    "doc": "Tools vs MCP",
    "title": "Real-World Examples",
    "content": "| Capability | Internal Tool Approach | MCP Server Approach | . | File operations | Direct filesystem access | File management MCP server | . | Web search | API client in codebase | External search MCP server | . | GitHub PR review | GitHub API client | GitHub integration MCP server | . | Code generation | Direct LLM calls | Specialized coding MCP server | . | Database queries | Direct DB connections | Data access MCP server | . ",
    "url": "/muxi/tools-vs-mcp/#real-world-examples",
    
    "relUrl": "/tools-vs-mcp/#real-world-examples"
  },"219": {
    "doc": "Tools vs MCP",
    "title": "Conclusion",
    "content": "Both approaches have their place in a well-designed AI agent system. The key is choosing the right approach for each specific capability based on performance needs, reusability requirements, and extension patterns. For a production system, a hybrid approach leveraging the strengths of both patterns will likely yield the best results. ",
    "url": "/muxi/tools-vs-mcp/#conclusion",
    
    "relUrl": "/tools-vs-mcp/#conclusion"
  },"220": {
    "doc": "Tools vs MCP",
    "title": "Creating Custom MCP Servers",
    "content": "The MUXI framework provides a built-in utility to help you create custom MCP servers quickly: . # Generate a new MCP server interactively muxi create mcp-server . This command launches an interactive wizard that guides you through: . | Naming your MCP server | Adding a description | Defining custom tools | Generating all necessary code | . Benefits of the Generator . The MCP server generator: . | Creates the proper file structure for an MCP server | Sets up FastAPI endpoints following the MCP specification | Generates tool template classes ready to be implemented | Creates a README with usage instructions | Configures installation files (setup.py) | Provides example client code | . Customizing Generated Servers . Once generated, you can implement your tool logic by editing the tool files in the tools/ directory. Each tool includes a template with: . | Proper class inheritance | Method signatures | Documentation placeholders | Example return values | . For example, a generated tool might look like: . class WeatherTool(BaseTool): \"\"\" Get weather information for a location. \"\"\" name = \"weather\" description = \"Get weather information for a location\" def execute(self, location: str, units: str = \"metric\") -&gt; Dict[str, Any]: \"\"\" Execute the weather tool. Args: location: The location to get weather for units: The units to use (metric or imperial) Returns: Dict[str, Any]: The weather information \"\"\" # TODO: Implement your weather API call here return {\"result\": f\"Weather information for {location}\"} . The generator significantly reduces the boilerplate needed to create MCP servers, making it easier to build and share custom capabilities. ",
    "url": "/muxi/tools-vs-mcp/#creating-custom-mcp-servers",
    
    "relUrl": "/tools-vs-mcp/#creating-custom-mcp-servers"
  },"221": {
    "doc": "Tools vs MCP",
    "title": "Tools vs MCP",
    "content": " ",
    "url": "/muxi/tools-vs-mcp/",
    
    "relUrl": "/tools-vs-mcp/"
  },"222": {
    "doc": "Tools",
    "title": "Tools",
    "content": "Tools extend the capabilities of AI agents by enabling them to perform specific actions or retrieve information beyond their training data. ",
    "url": "/muxi/tools/",
    
    "relUrl": "/tools/"
  },"223": {
    "doc": "Tools",
    "title": "What are Tools?",
    "content": "Tools are specialized functions that agents can use to: . | Access external data sources and APIs | Perform calculations | Execute code | Interact with databases | Manipulate files and documents | And much more… | . ",
    "url": "/muxi/tools/#what-are-tools",
    
    "relUrl": "/tools/#what-are-tools"
  },"224": {
    "doc": "Tools",
    "title": "Tools vs MCP: Understanding the Distinction",
    "content": "It’s important to understand how tools relate to the Model Context Protocol (MCP) in the muxi framework: . Tools are Functional Extensions, Not the Protocol . | Tools: Specific capabilities that execute in your application environment, allowing the agent to perform actions like web searches, calculations, or API calls. | MCP (Model Context Protocol): The standardized communication protocol that enables the LLM to request tool execution and receive results. | . How Tools and MCP Work Together . When a tool is used in the muxi framework, the process follows this flow: . | The LLM (not your application code) decides a tool is needed to answer a query | The LLM sends a tool call request formatted according to the MCP standard | The MCP Handler in your application parses this request | Your application’s Tool Registry locates the appropriate tool | The tool executes in your application environment (not in the LLM or MCP) | The tool’s results are formatted as an MCP message | These results are sent back to the LLM via the MCP Handler | The LLM uses the results to generate its final response | . This clarifies that tools are the actual functionality that gets executed, while MCP is just the communication protocol that enables the LLM to request tool execution and receive results. For a more detailed explanation of this relationship, see the MCP documentation. ",
    "url": "/muxi/tools/#tools-vs-mcp-understanding-the-distinction",
    
    "relUrl": "/tools/#tools-vs-mcp-understanding-the-distinction"
  },"225": {
    "doc": "Tools",
    "title": "Built-in Tools",
    "content": "The framework includes several built-in tools that provide common functionality. Calculator Tool . The Calculator tool allows agents to evaluate mathematical expressions. from src.tools.calculator import CalculatorTool # Create a calculator tool calculator = CalculatorTool() # Execute the tool result = await calculator.execute(expression=\"2 * (3 + 4)\") print(result) # Outputs: 14 . Web Search Tool . The Web Search tool enables agents to search the internet for information. from src.tools.web_search import WebSearchTool # Create a web search tool web_search = WebSearchTool() # Execute the tool results = await web_search.execute(query=\"latest AI developments 2023\") for result in results: print(f\"Title: {result['title']}\") print(f\"URL: {result['url']}\") print(f\"Snippet: {result['snippet']}\") print(\"---\") . File Operations Tool . The File Operations tool provides capabilities for reading and writing files. from src.tools.file_operations import FileOperationsTool # Create a file operations tool file_ops = FileOperationsTool() # Read a file content = await file_ops.execute(operation=\"read\", path=\"data/example.txt\") print(content) # Write to a file await file_ops.execute( operation=\"write\", path=\"data/output.txt\", content=\"This is some example text.\" ) # List files in a directory files = await file_ops.execute(operation=\"list\", path=\"data/\") print(files) . ",
    "url": "/muxi/tools/#built-in-tools",
    
    "relUrl": "/tools/#built-in-tools"
  },"226": {
    "doc": "Tools",
    "title": "Creating Custom Tools",
    "content": "You can create custom tools by inheriting from the BaseTool class and implementing the required methods. Tool Base Class . The BaseTool class defines the interface that all tools must implement: . from src.tools.base import BaseTool class MyCustomTool(BaseTool): @property def name(self) -&gt; str: \"\"\"Return the name of the tool.\"\"\" return \"my_custom_tool\" @property def description(self) -&gt; str: \"\"\"Return a description of what the tool does.\"\"\" return \"A description of what my custom tool does\" @property def parameters(self) -&gt; dict: \"\"\"Return the parameters that the tool accepts.\"\"\" return { \"param1\": { \"type\": \"string\", \"description\": \"Description of parameter 1\" }, \"param2\": { \"type\": \"integer\", \"description\": \"Description of parameter 2\" } } @property def required_parameters(self) -&gt; list: \"\"\"Return a list of parameters that are required.\"\"\" return [\"param1\"] async def execute(self, **kwargs) -&gt; any: \"\"\"Execute the tool with the provided parameters.\"\"\" param1 = kwargs.get(\"param1\") param2 = kwargs.get(\"param2\", 0) # Default value for optional parameter # Implement your tool logic here result = f\"Processed {param1} with value {param2}\" return result . Example: Weather Tool . Here’s an example of a custom tool that retrieves weather information: . import aiohttp from src.tools.base import BaseTool class WeatherTool(BaseTool): @property def name(self) -&gt; str: return \"weather\" @property def description(self) -&gt; str: return \"Gets current weather information for a specified location\" @property def parameters(self) -&gt; dict: return { \"location\": { \"type\": \"string\", \"description\": \"The city and state/country (e.g., 'New York, NY')\" }, \"units\": { \"type\": \"string\", \"description\": \"Temperature units: 'metric' (Celsius) or 'imperial' (Fahrenheit)\" } } @property def required_parameters(self) -&gt; list: return [\"location\"] async def execute(self, **kwargs) -&gt; any: location = kwargs.get(\"location\") units = kwargs.get(\"units\", \"metric\") api_key = \"YOUR_WEATHER_API_KEY\" # In production, use environment variables url = f\"https://api.openweathermap.org/data/2.5/weather\" async with aiohttp.ClientSession() as session: async with session.get(url, params={ \"q\": location, \"units\": units, \"appid\": api_key }) as response: if response.status == 200: data = await response.json() temp = data[\"main\"][\"temp\"] description = data[\"weather\"][0][\"description\"] humidity = data[\"main\"][\"humidity\"] wind_speed = data[\"wind\"][\"speed\"] unit_symbol = \"°C\" if units == \"metric\" else \"°F\" return { \"temperature\": f\"{temp}{unit_symbol}\", \"description\": description, \"humidity\": f\"{humidity}%\", \"wind_speed\": wind_speed } else: error_data = await response.json() return f\"Error: {error_data.get('message', 'Unknown error')}\" . Example: Database Query Tool . Here’s a tool that allows agents to run SQL queries against a database: . import asyncpg from src.tools.base import BaseTool class DatabaseQueryTool(BaseTool): def __init__(self, connection_string): self.connection_string = connection_string self._conn_pool = None async def _get_connection_pool(self): if self._conn_pool is None: self._conn_pool = await asyncpg.create_pool(self.connection_string) return self._conn_pool @property def name(self) -&gt; str: return \"database_query\" @property def description(self) -&gt; str: return \"Runs SQL queries against a database and returns the results\" @property def parameters(self) -&gt; dict: return { \"query\": { \"type\": \"string\", \"description\": \"The SQL query to execute\" }, \"max_rows\": { \"type\": \"integer\", \"description\": \"Maximum number of rows to return\" } } @property def required_parameters(self) -&gt; list: return [\"query\"] async def execute(self, **kwargs) -&gt; any: query = kwargs.get(\"query\") max_rows = kwargs.get(\"max_rows\", 100) # Basic SQL injection protection if any(keyword in query.lower() for keyword in [\"insert\", \"update\", \"delete\", \"drop\"]): return \"Error: Only SELECT queries are allowed for safety reasons\" pool = await self._get_connection_pool() try: async with pool.acquire() as conn: results = await conn.fetch(query) # Convert to a list of dictionaries formatted_results = [] for row in results[:max_rows]: formatted_results.append(dict(row)) return { \"rows\": formatted_results, \"row_count\": len(results), \"truncated\": len(results) &gt; max_rows } except Exception as e: return f\"Error executing query: {str(e)}\" . ",
    "url": "/muxi/tools/#creating-custom-tools",
    
    "relUrl": "/tools/#creating-custom-tools"
  },"227": {
    "doc": "Tools",
    "title": "Registering Tools with Agents",
    "content": "To make tools available to an agent, they need to be registered when creating the agent: . from src.core.orchestrator import Orchestrator from src.models import OpenAIModel from src.memory.buffer import BufferMemory from src.tools.calculator import CalculatorTool from src.tools.web_search import WebSearchTool # Create custom tools weather_tool = WeatherTool() db_tool = DatabaseQueryTool(\"postgresql://user:password@localhost:5432/mydb\") # Create an agent with tools orchestrator = Orchestrator() orchestrator.create_agent( agent_id=\"assistant\", model=OpenAIModel(model=\"gpt-4o\"), buffer_memory=BufferMemory(), tools=[CalculatorTool(), WebSearchTool(), weather_tool, db_tool], system_message=\"You are a helpful assistant with access to tools.\" ) . ",
    "url": "/muxi/tools/#registering-tools-with-agents",
    
    "relUrl": "/tools/#registering-tools-with-agents"
  },"228": {
    "doc": "Tools",
    "title": "Tool Management",
    "content": "Tool Registry . The framework includes a tool registry for centralized tool management: . from src.tools.registry import ToolRegistry from src.tools.calculator import CalculatorTool from src.tools.web_search import WebSearchTool # Create a tool registry registry = ToolRegistry() # Register built-in tools registry.register(CalculatorTool()) registry.register(WebSearchTool()) # Register custom tools registry.register(WeatherTool()) registry.register(DatabaseQueryTool(\"postgresql://user:password@localhost:5432/mydb\")) # Get a tool by name calculator = registry.get_tool(\"calculator\") # List all available tools all_tools = registry.list_tools() for tool_name in all_tools: print(tool_name) # Check if a tool exists if registry.has_tool(\"weather\"): print(\"Weather tool is available\") . Configuring Tools via API . Tools can be configured and registered through the API: . # Register a tool via API curl -X POST http://localhost:5050/tools/register \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"weather\", \"class_path\": \"custom_tools.weather.WeatherTool\", \"config\": { \"api_key\": \"YOUR_API_KEY\" } }' # List available tools curl -X GET http://localhost:5050/tools # Get tool information curl -X GET http://localhost:5050/tools/weather # Update tool configuration curl -X PATCH http://localhost:5050/tools/weather \\ -H \"Content-Type: application/json\" \\ -d '{ \"config\": { \"api_key\": \"NEW_API_KEY\" } }' . ",
    "url": "/muxi/tools/#tool-management",
    
    "relUrl": "/tools/#tool-management"
  },"229": {
    "doc": "Tools",
    "title": "Advanced Tool Features",
    "content": "Tool Pipelines . You can create tool pipelines that chain multiple tools together: . from src.tools.pipeline import ToolPipeline # Create a pipeline of tools pipeline = ToolPipeline([ (\"web_search\", WebSearchTool()), (\"summarize\", TextSummarizationTool()) ]) # Execute the pipeline result = await pipeline.execute( web_search={\"query\": \"latest developments in AI 2023\"}, summarize={\"text\": lambda outputs: \"\\n\".join([r[\"snippet\"] for r in outputs[\"web_search\"]]), \"max_length\": 200} ) print(result[\"summarize\"]) # Final output from the pipeline . Tool Authorization . Implement authorization for sensitive tools: . from src.tools.base import BaseTool from src.auth import check_user_permission class SecureFileTool(BaseTool): @property def name(self) -&gt; str: return \"secure_file\" @property def description(self) -&gt; str: return \"Securely reads or writes files with permission checks\" @property def parameters(self) -&gt; dict: return { \"operation\": { \"type\": \"string\", \"description\": \"The operation to perform (read/write)\" }, \"path\": { \"type\": \"string\", \"description\": \"Path to the file\" }, \"content\": { \"type\": \"string\", \"description\": \"Content to write (for write operations)\" }, \"user_id\": { \"type\": \"string\", \"description\": \"ID of the user making the request\" } } @property def required_parameters(self) -&gt; list: return [\"operation\", \"path\", \"user_id\"] async def execute(self, **kwargs) -&gt; any: operation = kwargs.get(\"operation\") path = kwargs.get(\"path\") user_id = kwargs.get(\"user_id\") # Check permissions has_permission = await check_user_permission(user_id, operation, path) if not has_permission: return \"Error: Permission denied\" # Proceed with operation if operation == \"read\": # Read file logic pass elif operation == \"write\": content = kwargs.get(\"content\", \"\") # Write file logic pass else: return \"Error: Invalid operation\" . Tool Error Handling . Implement robust error handling in your tools: . from src.tools.base import BaseTool class RobustTool(BaseTool): # ... other properties ... async def execute(self, **kwargs) -&gt; any: try: # Main tool logic param = kwargs.get(\"param\") if param is None: return {\"error\": \"Missing required parameter\", \"error_code\": \"MISSING_PARAM\"} # Business logic result = await self._process(param) return {\"success\": True, \"result\": result} except ConnectionError as e: # Handle connection issues return { \"error\": \"Connection error\", \"error_code\": \"CONNECTION_ERROR\", \"details\": str(e) } except TimeoutError as e: # Handle timeouts return { \"error\": \"Operation timed out\", \"error_code\": \"TIMEOUT\", \"details\": str(e) } except Exception as e: # Catch-all for unexpected errors return { \"error\": \"An unexpected error occurred\", \"error_code\": \"UNKNOWN_ERROR\", \"details\": str(e) } . ",
    "url": "/muxi/tools/#advanced-tool-features",
    
    "relUrl": "/tools/#advanced-tool-features"
  },"230": {
    "doc": "Tools",
    "title": "Best Practices",
    "content": ". | Clear Documentation: Provide clear descriptions and parameter documentation for each tool . | Input Validation: Validate all inputs before processing to prevent errors and security issues . | Error Handling: Implement robust error handling and provide meaningful error messages . | Statelessness: Design tools to be stateless whenever possible to simplify scaling . | Security First: Implement appropriate security checks for tools that access sensitive data . | Performance: Keep tool execution efficient, especially for synchronous operations . | Testing: Thoroughly test tools with various inputs to ensure reliability . | . ",
    "url": "/muxi/tools/#best-practices",
    
    "relUrl": "/tools/#best-practices"
  },"231": {
    "doc": "Tools",
    "title": "Troubleshooting",
    "content": "Tool Not Being Found . | Check that the tool is correctly registered with the agent | Verify that the tool name is consistent across registration and references | . Parameter Errors . | Ensure that all required parameters are being provided | Check parameter types and formats match the tool’s expectations | . Execution Failures . | Implement detailed logging in tool execution | Check for connectivity issues with external services | Verify that any required API keys or credentials are valid | . ",
    "url": "/muxi/tools/#troubleshooting",
    
    "relUrl": "/tools/#troubleshooting"
  },"232": {
    "doc": "Tools",
    "title": "Next Steps",
    "content": "After implementing tools, you might want to explore: . | Creating agents that can effectively use your tools | Setting up memory systems to retain information across tool executions | Implementing WebSocket support for real-time tool execution updates | Exploring orchestrator features for coordinating multiple tools and agents | . ",
    "url": "/muxi/tools/#next-steps",
    
    "relUrl": "/tools/#next-steps"
  },"233": {
    "doc": "User Interfaces",
    "title": "User Interfaces",
    "content": "MUXI provides multiple interfaces for interacting with agents, from command-line tools to web applications and APIs. ",
    "url": "/muxi/user-interfaces/",
    
    "relUrl": "/user-interfaces/"
  },"234": {
    "doc": "User Interfaces",
    "title": "In this section",
    "content": "Command Line Interface . | CLI - Learn how to use the MUXI command-line interface | . Web Dashboard . | Dashboard - Explore the web-based user interface | . REST API . | API - Integrate MUXI with your applications using the REST API | . WebSocket Interface . | WebSocket - Set up real-time communication with MUXI agents | . ",
    "url": "/muxi/user-interfaces/#in-this-section",
    
    "relUrl": "/user-interfaces/#in-this-section"
  },"235": {
    "doc": "Webapp",
    "title": "MUXI Framework Dashboard",
    "content": "A modern web dashboard for interacting with the MUXI Framework. ",
    "url": "/muxi/webapp/#muxi-framework-dashboard",
    
    "relUrl": "/webapp/#muxi-framework-dashboard"
  },"236": {
    "doc": "Webapp",
    "title": "Features",
    "content": ". | Agent Management: Create, delete, and view details of your AI agents | Real-time Chat: Chat with agents in real-time using WebSockets | Memory Search: Search through an agent’s memories | Tool Exploration: See which tools are available to each agent | Responsive Design: Works on desktop and mobile devices | . ",
    "url": "/muxi/webapp/#features",
    
    "relUrl": "/webapp/#features"
  },"237": {
    "doc": "Webapp",
    "title": "Technologies Used",
    "content": ". | React: UI library | Chakra UI: Component library for consistent design | React Router: For navigation | Axios: For API requests | WebSocket API: For real-time communication | . ",
    "url": "/muxi/webapp/#technologies-used",
    
    "relUrl": "/webapp/#technologies-used"
  },"238": {
    "doc": "Webapp",
    "title": "Getting Started",
    "content": "Prerequisites . | Node.js 14+ and npm | MUXI Framework API server running | . Installation . | Navigate to the web dashboard directory: cd src/web . | Install dependencies: npm install . | Create a .env file (optional): . # Replace with your API server's address (IP or FQDN) including the port BACKEND_API_URL=http://localhost:5050 # Examples: # BACKEND_API_URL=http://192.168.1.100:5050 # BACKEND_API_URL=http://api.yourdomain.com:8000 . | Start the development server: npm start . | . The dashboard will be available at http://localhost:3000. ",
    "url": "/muxi/webapp/#getting-started",
    
    "relUrl": "/webapp/#getting-started"
  },"239": {
    "doc": "Webapp",
    "title": "Building for Production",
    "content": "To build the dashboard for production: . npm run build . This will create a build directory with optimized production files. ",
    "url": "/muxi/webapp/#building-for-production",
    
    "relUrl": "/webapp/#building-for-production"
  },"240": {
    "doc": "Webapp",
    "title": "Configuration",
    "content": "You can configure the dashboard using the following environment variables: . | BACKEND_API_URL: URL of the MUXI Framework API (defaults to the same host as the dashboard) | . ",
    "url": "/muxi/webapp/#configuration",
    
    "relUrl": "/webapp/#configuration"
  },"241": {
    "doc": "Webapp",
    "title": "Deployment",
    "content": "With the MUXI Framework . The MUXI Framework API server can serve the dashboard. To do this: . | Build the dashboard: 2. npm run build . | Copy the contents of the build directory to the appropriate location in the API server. | . Standalone Deployment . The dashboard can also be deployed separately using services like: . | Netlify | Vercel | GitHub Pages | Any static file server | . ",
    "url": "/muxi/webapp/#deployment",
    
    "relUrl": "/webapp/#deployment"
  },"242": {
    "doc": "Webapp",
    "title": "Screenshots",
    "content": ". | Dashboard View: A quick overview of your agents and stats | Chat Interface: Real-time communication with your agents | Agent Details: View and manage specific agent details and memory | . ",
    "url": "/muxi/webapp/#screenshots",
    
    "relUrl": "/webapp/#screenshots"
  },"243": {
    "doc": "Webapp",
    "title": "Development",
    "content": "Project Structure . src/ ├── components/ # Reusable UI components ├── hooks/ # Custom React hooks ├── pages/ # Main application pages ├── services/ # API and WebSocket services ├── styles/ # CSS and theme files └── utils/ # Utility functions . Adding New Features . | New Pages: Add new pages in the pages directory and update the routing in App.js | API Endpoints: Add new API endpoints in services/api.js | WebSocket Events: Handle new WebSocket events in hooks/useWebSocket.js | . ",
    "url": "/muxi/webapp/#development",
    
    "relUrl": "/webapp/#development"
  },"244": {
    "doc": "Webapp",
    "title": "Webapp",
    "content": " ",
    "url": "/muxi/webapp/",
    
    "relUrl": "/webapp/"
  },"245": {
    "doc": "WebSocket",
    "title": "WebSocket Communication",
    "content": "WebSockets provide real-time, bidirectional communication between clients and the MUXI Framework. This guide explains how to implement, use, and troubleshoot WebSocket connections for interactive agent communication. ",
    "url": "/muxi/websocket/#websocket-communication",
    
    "relUrl": "/websocket/#websocket-communication"
  },"246": {
    "doc": "WebSocket",
    "title": "What is WebSocket Support?",
    "content": "WebSocket support in the MUXI Framework: . | Enables real-time communication with agents | Provides immediate responses as they’re generated | Supports subscription to specific agents | Allows for streaming responses from LLMs | Facilitates tool execution updates | Supports multi-user interactions with user-specific memory | . ",
    "url": "/muxi/websocket/#what-is-websocket-support",
    
    "relUrl": "/websocket/#what-is-websocket-support"
  },"247": {
    "doc": "WebSocket",
    "title": "Server-side Implementation",
    "content": "Basic WebSocket Server . The framework includes a WebSocket server implementation: . # src/api/websocket.py import asyncio import json import logging from fastapi import WebSocket, WebSocketDisconnect from src.core.orchestrator import Orchestrator logger = logging.getLogger(__name__) # Shared orchestrator instance _orchestrator = None def set_orchestrator(orchestrator): \"\"\"Set the shared orchestrator instance.\"\"\" global _orchestrator _orchestrator = orchestrator def get_orchestrator(): \"\"\"Get the shared orchestrator instance.\"\"\" global _orchestrator return _orchestrator class WebSocketManager: def __init__(self): self.active_connections = {} self.agent_subscribers = {} async def connect(self, websocket: WebSocket, client_id: str): \"\"\"Connect a new client.\"\"\" await websocket.accept() self.active_connections[client_id] = websocket logger.info(f\"Client {client_id} connected\") # Send connection confirmation await self.send_json(websocket, { \"type\": \"connected\", \"client_id\": client_id }) def disconnect(self, client_id: str): \"\"\"Disconnect a client.\"\"\" if client_id in self.active_connections: del self.active_connections[client_id] # Remove client from agent subscriptions for agent_id in list(self.agent_subscribers.keys()): if client_id in self.agent_subscribers[agent_id]: self.agent_subscribers[agent_id].remove(client_id) # Clean up empty agent subscriptions if not self.agent_subscribers[agent_id]: del self.agent_subscribers[agent_id] logger.info(f\"Client {client_id} disconnected\") async def send_json(self, websocket: WebSocket, data: dict): \"\"\"Send JSON data to a websocket.\"\"\" try: await websocket.send_json(data) except Exception as e: logger.error(f\"Error sending message: {str(e)}\") async def broadcast_to_agent_subscribers(self, agent_id: str, message: dict): \"\"\"Send a message to all subscribers of an agent.\"\"\" if agent_id not in self.agent_subscribers: return for client_id in self.agent_subscribers[agent_id]: if client_id in self.active_connections: await self.send_json(self.active_connections[client_id], message) async def subscribe_to_agent(self, client_id: str, agent_id: str): \"\"\"Subscribe a client to an agent's messages.\"\"\" # Initialize agent subscribers list if needed if agent_id not in self.agent_subscribers: self.agent_subscribers[agent_id] = set() # Add client to subscribers self.agent_subscribers[agent_id].add(client_id) # Get the client's websocket websocket = self.active_connections.get(client_id) if websocket: # Confirm subscription await self.send_json(websocket, { \"type\": \"subscribed\", \"agent_id\": agent_id }) logger.info(f\"Client {client_id} subscribed to agent {agent_id}\") # Create a singleton manager manager = WebSocketManager() async def websocket_endpoint(websocket: WebSocket, client_id: str = None): \"\"\"Handle WebSocket connections.\"\"\" if client_id is None: client_id = f\"client_{id(websocket)}\" await manager.connect(websocket, client_id) try: while True: # Receive and parse message data = await websocket.receive_json() message_type = data.get(\"type\") if message_type == \"ping\": # Respond to ping with pong await manager.send_json(websocket, {\"type\": \"ping\"}) elif message_type == \"subscribe\": # Subscribe to an agent agent_id = data.get(\"agent_id\") if not agent_id: await manager.send_json(websocket, { \"type\": \"error\", \"message\": \"No agent_id provided for subscription\" }) continue # Check if agent exists orchestrator = get_orchestrator() if orchestrator and not orchestrator.has_agent(agent_id): await manager.send_json(websocket, { \"type\": \"error\", \"message\": f\"No agent with ID '{agent_id}' exists\" }) continue # Subscribe to the agent await manager.subscribe_to_agent(client_id, agent_id) elif message_type == \"chat\": # Process a chat message agent_id = data.get(\"agent_id\") message = data.get(\"message\") if not message: await manager.send_json(websocket, { \"type\": \"error\", \"message\": \"No message provided\" }) continue # Get the orchestrator orchestrator = get_orchestrator() if not orchestrator: await manager.send_json(websocket, { \"type\": \"error\", \"message\": \"Orchestrator not available\" }) continue # Notify that processing has started await manager.send_json(websocket, { \"type\": \"agent_thinking\", \"agent_id\": agent_id }) try: # Process the message response = await orchestrator.run(agent_id, message) # Send the response await manager.send_json(websocket, { \"type\": \"response\", \"agent_id\": agent_id, \"message\": response }) # Notify that processing is complete await manager.send_json(websocket, { \"type\": \"agent_done\", \"agent_id\": agent_id }) except Exception as e: logger.error(f\"Error processing message: {str(e)}\") await manager.send_json(websocket, { \"type\": \"error\", \"message\": f\"Error processing message: {str(e)}\" }) else: # Unknown message type await manager.send_json(websocket, { \"type\": \"error\", \"message\": f\"Unknown message type: {message_type}\" }) except WebSocketDisconnect: manager.disconnect(client_id) except Exception as e: logger.error(f\"WebSocket error: {str(e)}\") manager.disconnect(client_id) . Integrating with FastAPI . To integrate the WebSocket server with FastAPI: . # src/api/app.py from fastapi import FastAPI, WebSocket, Depends import uuid from src.core.orchestrator import Orchestrator from src.api.websocket import websocket_endpoint, set_orchestrator app = FastAPI(title=\"MUXI Framework API\") # Create orchestrator orchestrator = Orchestrator() # Share orchestrator with websocket module set_orchestrator(orchestrator) # Define WebSocket endpoint @app.websocket(\"/ws\") async def websocket_route(websocket: WebSocket): client_id = str(uuid.uuid4()) await websocket_endpoint(websocket, client_id) . ",
    "url": "/muxi/websocket/#server-side-implementation",
    
    "relUrl": "/websocket/#server-side-implementation"
  },"248": {
    "doc": "WebSocket",
    "title": "Client-side Implementation",
    "content": "JavaScript Client . Here’s a basic JavaScript WebSocket client: . class AgentWebSocketClient { constructor(url = 'ws://localhost:5050/ws') { this.url = url; this.socket = null; this.connected = false; this.reconnectAttempts = 0; this.maxReconnectAttempts = 5; this.reconnectDelay = 1000; this.messageCallbacks = { 'connected': [], 'subscribed': [], 'response': [], 'agent_thinking': [], 'agent_done': [], 'error': [], 'ping': [] }; this.connectSocket(); } connectSocket() { this.socket = new WebSocket(this.url); this.socket.onopen = () =&gt; { console.log('WebSocket connected'); this.connected = true; this.reconnectAttempts = 0; // Start ping interval to keep connection alive this.pingInterval = setInterval(() =&gt; { if (this.connected) { this.sendPing(); } }, 30000); }; this.socket.onmessage = (event) =&gt; { const data = JSON.parse(event.data); console.log('Received:', data); // Call registered callbacks for this message type if (data.type &amp;&amp; this.messageCallbacks[data.type]) { this.messageCallbacks[data.type].forEach(callback =&gt; { try { callback(data); } catch (e) { console.error('Error in callback:', e); } }); } }; this.socket.onclose = (event) =&gt; { this.connected = false; clearInterval(this.pingInterval); console.log('WebSocket disconnected:', event.code, event.reason); // Attempt to reconnect if (this.reconnectAttempts &lt; this.maxReconnectAttempts) { this.reconnectAttempts++; const delay = this.reconnectDelay * this.reconnectAttempts; console.log(`Reconnecting in ${delay}ms (attempt ${this.reconnectAttempts}/${this.maxReconnectAttempts})...`); setTimeout(() =&gt; { this.connectSocket(); }, delay); } else { console.error('Max reconnect attempts reached'); } }; this.socket.onerror = (error) =&gt; { console.error('WebSocket error:', error); }; } sendMessage(data) { if (!this.connected) { console.error('Cannot send message: WebSocket not connected'); return false; } try { this.socket.send(JSON.stringify(data)); return true; } catch (e) { console.error('Error sending message:', e); return false; } } sendPing() { return this.sendMessage({ type: 'ping' }); } subscribeToAgent(agentId) { return this.sendMessage({ type: 'subscribe', agent_id: agentId }); } sendChatMessage(agentId, message) { return this.sendMessage({ type: 'chat', agent_id: agentId, message: message }); } on(messageType, callback) { if (!this.messageCallbacks[messageType]) { this.messageCallbacks[messageType] = []; } this.messageCallbacks[messageType].push(callback); return this; } close() { if (this.socket) { clearInterval(this.pingInterval); this.socket.close(); } } searchMemory(agentId, query, limit = 5, useLongTerm = true) { if (!this.connected) { throw new Error('WebSocket is not connected'); } this.sendMessage({ type: 'search_memory', agent_id: agentId, query: query, limit: limit, use_long_term: useLongTerm }); } clearMemory(agentId, clearLongTerm = false) { if (!this.connected) { throw new Error('WebSocket is not connected'); } this.sendMessage({ type: 'clear_memory', agent_id: agentId, clear_long_term: clearLongTerm }); } } // Usage example const client = new AgentWebSocketClient('ws://localhost:5050/ws'); client .on('connected', (data) =&gt; { console.log('Connected with client ID:', data.client_id); client.subscribeToAgent('my_agent'); }) .on('subscribed', (data) =&gt; { console.log('Subscribed to agent:', data.agent_id); client.sendChatMessage('my_agent', 'Hello, agent!'); }) .on('agent_thinking', (data) =&gt; { console.log('Agent is thinking...'); }) .on('response', (data) =&gt; { console.log('Agent response:', data.message); }) .on('agent_done', (data) =&gt; { console.log('Agent completed processing'); }) .on('error', (data) =&gt; { console.error('Error:', data.message); }); . Python Client . Here’s a Python WebSocket client using websockets: . import asyncio import json import uuid import websockets import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class AgentWebSocketClient: def __init__(self, url=\"ws://localhost:5050/ws\"): self.url = url self.connected = False self.websocket = None self.client_id = str(uuid.uuid4()) self.current_agent_id = None self.reconnect_attempts = 0 self.max_reconnect_attempts = 5 self.callbacks = { \"connected\": [], \"subscribed\": [], \"response\": [], \"agent_thinking\": [], \"agent_done\": [], \"error\": [], \"ping\": [] } async def connect(self): \"\"\"Connect to the WebSocket server.\"\"\" try: self.websocket = await websockets.connect(self.url) self.connected = True logger.info(\"Connected to WebSocket server\") # Start listener asyncio.create_task(self._listen()) return True except Exception as e: logger.error(f\"Connection error: {str(e)}\") return False async def _listen(self): \"\"\"Listen for messages from the server.\"\"\" try: while self.connected: try: message = await self.websocket.recv() data = json.loads(message) logger.debug(f\"Received: {data}\") # Execute callbacks for this message type message_type = data.get(\"type\") if message_type in self.callbacks: for callback in self.callbacks[message_type]: try: await callback(data) except Exception as e: logger.error(f\"Error in callback: {str(e)}\") except websockets.ConnectionClosed: logger.warning(\"Connection closed\") self.connected = False await self._try_reconnect() break except Exception as e: logger.error(f\"Listener error: {str(e)}\") self.connected = False async def _try_reconnect(self): \"\"\"Try to reconnect to the server.\"\"\" if self.reconnect_attempts &gt;= self.max_reconnect_attempts: logger.error(\"Max reconnect attempts reached\") return False self.reconnect_attempts += 1 delay = 1 * self.reconnect_attempts logger.info(f\"Reconnecting in {delay}s (attempt {self.reconnect_attempts}/{self.max_reconnect_attempts})...\") await asyncio.sleep(delay) success = await self.connect() if success and self.current_agent_id: # Resubscribe to the previous agent await self.subscribe_to_agent(self.current_agent_id) return success async def send_json(self, data): \"\"\"Send JSON data to the server.\"\"\" if not self.connected: logger.error(\"Cannot send message: not connected\") return False try: await self.websocket.send(json.dumps(data)) return True except Exception as e: logger.error(f\"Send error: {str(e)}\") self.connected = False return False async def send_ping(self): \"\"\"Send a ping message to keep the connection alive.\"\"\" return await self.send_json({\"type\": \"ping\"}) async def subscribe_to_agent(self, agent_id): \"\"\"Subscribe to an agent's messages.\"\"\" self.current_agent_id = agent_id return await self.send_json({ \"type\": \"subscribe\", \"agent_id\": agent_id }) async def send_chat_message(self, message, agent_id=None): \"\"\"Send a chat message to an agent.\"\"\" if not agent_id and not self.current_agent_id: logger.error(\"No agent ID specified\") return False return await self.send_json({ \"type\": \"chat\", \"agent_id\": agent_id or self.current_agent_id, \"message\": message }) def on(self, message_type, callback): \"\"\"Register a callback for a message type.\"\"\" if message_type in self.callbacks: self.callbacks[message_type].append(callback) return self async def close(self): \"\"\"Close the WebSocket connection.\"\"\" if self.websocket: await self.websocket.close() self.connected = False logger.info(\"Connection closed\") # Usage example async def main(): client = AgentWebSocketClient() # Register callbacks client.on(\"connected\", lambda data: logger.info(f\"Connected with client ID: {data.get('client_id')}\")) client.on(\"subscribed\", lambda data: logger.info(f\"Subscribed to agent: {data.get('agent_id')}\")) client.on(\"response\", lambda data: logger.info(f\"Agent response: {data.get('message')}\")) client.on(\"error\", lambda data: logger.error(f\"Error: {data.get('message')}\")) # Define chat callback async def on_response(data): print(f\"\\nAgent: {data.get('message')}\\n\") client.on(\"response\", on_response) # Connect and subscribe await client.connect() await client.subscribe_to_agent(\"my_agent\") # Chat loop try: while True: message = input(\"\\nYou: \") if message.lower() in [\"exit\", \"quit\", \"bye\"]: break await client.send_chat_message(message) finally: await client.close() if __name__ == \"__main__\": asyncio.run(main()) . ",
    "url": "/muxi/websocket/#client-side-implementation",
    
    "relUrl": "/websocket/#client-side-implementation"
  },"249": {
    "doc": "WebSocket",
    "title": "Message Types",
    "content": "The WebSocket protocol supports several message types: . Client to Server . | subscribe . | Subscribe to an agent’s messages | Parameters: agent_id | . { \"type\": \"subscribe\", \"agent_id\": \"my_agent\" } . | chat . | Send a message to an agent | Parameters: agent_id, message | . { \"type\": \"chat\", \"agent_id\": \"my_agent\", \"message\": \"What's the weather like today?\" } . | ping . | Keep the connection alive | No additional parameters | . { \"type\": \"ping\" } . | set_user . | Set the user ID for this connection | Parameters: user_id | . { \"type\": \"set_user\", \"user_id\": 123 } . | search_memory . | Search agent memory for a query | Parameters: agent_id, query, limit, use_long_term | . { \"type\": \"search_memory\", \"agent_id\": \"my_agent\", \"query\": \"What did we discuss yesterday?\", \"limit\": 5, \"use_long_term\": true } . | clear_memory . | Clear agent memory for a specific user | Parameters: agent_id, clear_long_term | . { \"type\": \"clear_memory\", \"agent_id\": \"my_agent\", \"clear_long_term\": false } . | . Server to Client . | connected . | Confirms successful connection establishment | Parameters: client_id | . { \"type\": \"connected\", \"client_id\": \"client_123\" } . | subscribed . | Confirms successful agent subscription | Parameters: agent_id | . { \"type\": \"subscribed\", \"agent_id\": \"my_agent\" } . | response . | A response from an agent | Parameters: agent_id, message | . { \"type\": \"response\", \"agent_id\": \"my_agent\", \"message\": \"The weather in New York is sunny today.\" } . | agent_thinking . | Indicates the agent is processing | Parameters: agent_id | . { \"type\": \"agent_thinking\", \"agent_id\": \"my_agent\" } . | agent_done . | Indicates the agent has finished processing | Parameters: agent_id | . { \"type\": \"agent_done\", \"agent_id\": \"my_agent\" } . | error . | An error message | Parameters: message | . { \"type\": \"error\", \"message\": \"No agent with ID 'unknown_agent' exists\" } . | ping . | Heartbeat to keep the connection alive | No additional parameters | . { \"type\": \"ping\" } . | . ",
    "url": "/muxi/websocket/#message-types",
    
    "relUrl": "/websocket/#message-types"
  },"250": {
    "doc": "WebSocket",
    "title": "WebSocket Message Types",
    "content": "Client to Server Messages . The client can send these message types to the server: . 1. Connect . When a WebSocket connects, the server automatically assigns a connection ID and sends a welcome message. No special message is needed from the client. 2. Set User ID . Set the user ID for this connection. This is used for multi-user agents to maintain separate memory contexts per user. { \"type\": \"set_user\", \"user_id\": 123 } . Response: . { \"type\": \"user_set\", \"user_id\": 123 } . 3. Subscribe to Agent . Subscribe to a specific agent to receive messages and updates. { \"type\": \"subscribe\", \"agent_id\": \"assistant\" } . Response: . { \"type\": \"subscribed\", \"agent_id\": \"assistant\", \"message\": \"Successfully subscribed to agent assistant\" } . 4. Unsubscribe from Agent . Unsubscribe from a specific agent. { \"type\": \"unsubscribe\", \"agent_id\": \"assistant\" } . Response: . { \"type\": \"unsubscribed\", \"agent_id\": \"assistant\", \"message\": \"Successfully unsubscribed from agent assistant\" } . 5. Send Chat Message . Send a message to an agent. The user_id is automatically set to the one specified with the “set_user” message. { \"type\": \"chat\", \"agent_id\": \"assistant\", \"message\": \"Hello, can you help me with a question?\" } . 6. Search Memory . Search an agent’s memory for relevant information. The user_id is automatically set to the one specified with the “set_user” message. { \"type\": \"search_memory\", \"agent_id\": \"assistant\", \"query\": \"What did we discuss yesterday?\", \"limit\": 5, \"use_long_term\": true } . 7. Keep-alive Ping . Send a ping to keep the connection alive. { \"type\": \"ping\" } . Server to Client Messages . The server can send these message types to the client: . ",
    "url": "/muxi/websocket/#websocket-message-types",
    
    "relUrl": "/websocket/#websocket-message-types"
  },"251": {
    "doc": "WebSocket",
    "title": "Client Implementation Examples",
    "content": "Browser Implementation . class AgentWebSocket { constructor(url = 'ws://localhost:5050/ws') { this.url = url; this.socket = null; this.isConnected = false; this.messageHandlers = { 'message': [], 'thinking': [], 'error': [] }; // Reconnection settings this.reconnectAttempts = 0; this.maxReconnectAttempts = 5; this.reconnectDelay = 1000; // User settings this.userId = null; } connect() { return new Promise((resolve, reject) =&gt; { this.socket = new WebSocket(this.url); this.socket.onopen = () =&gt; { console.log('WebSocket connected'); this.isConnected = true; this.reconnectAttempts = 0; resolve(); // Set user ID if available if (this.userId !== null) { this.setUserId(this.userId); } }; this.socket.onclose = (event) =&gt; { console.log(`WebSocket closed: ${event.code} ${event.reason}`); this.isConnected = false; // Attempt to reconnect if (this.reconnectAttempts &lt; this.maxReconnectAttempts) { this.reconnectAttempts++; const delay = this.reconnectDelay * Math.pow(1.5, this.reconnectAttempts - 1); console.log(`Reconnecting in ${delay}ms (attempt ${this.reconnectAttempts})`); setTimeout(() =&gt; { this.connect().catch(err =&gt; { console.error('Reconnection failed:', err); }); }, delay); } }; this.socket.onerror = (error) =&gt; { console.error('WebSocket error:', error); reject(error); }; this.socket.onmessage = (event) =&gt; { try { const data = JSON.parse(event.data); console.log('Received:', data); // Call appropriate handlers if (this.messageHandlers[data.type]) { this.messageHandlers[data.type].forEach(handler =&gt; handler(data)); } // Call general message handlers this.messageHandlers['*']?.forEach(handler =&gt; handler(data)); } catch (error) { console.error('Error parsing message:', error); } }; }); } setUserId(userId) { this.userId = userId; if (this.isConnected) { this.send({ type: 'set_user', user_id: userId }); } } subscribe(agentId) { if (!this.isConnected) { throw new Error('WebSocket is not connected'); } this.send({ type: 'subscribe', agent_id: agentId }); } unsubscribe(agentId) { if (!this.isConnected) { throw new Error('WebSocket is not connected'); } this.send({ type: 'unsubscribe', agent_id: agentId }); } sendMessage(agentId, message) { if (!this.isConnected) { throw new Error('WebSocket is not connected'); } this.send({ type: 'chat', agent_id: agentId, message: message }); } searchMemory(agentId, query, limit = 5, useLongTerm = true) { if (!this.isConnected) { throw new Error('WebSocket is not connected'); } this.send({ type: 'search_memory', agent_id: agentId, query: query, limit: limit, use_long_term: useLongTerm }); } clearMemory(agentId, clearLongTerm = false) { if (!this.isConnected) { throw new Error('WebSocket is not connected'); } this.send({ type: 'clear_memory', agent_id: agentId, clear_long_term: clearLongTerm }); } send(data) { if (!this.isConnected) { throw new Error('WebSocket is not connected'); } this.socket.send(JSON.stringify(data)); } on(eventType, callback) { if (!this.messageHandlers[eventType]) { this.messageHandlers[eventType] = []; } this.messageHandlers[eventType].push(callback); return this; } off(eventType, callback) { if (this.messageHandlers[eventType]) { this.messageHandlers[eventType] = this.messageHandlers[eventType] .filter(handler =&gt; handler !== callback); } return this; } close() { if (this.socket) { this.socket.close(); } } } // Usage example const ws = new AgentWebSocket('ws://localhost:5050/ws'); // Set handlers ws.on('message', data =&gt; { console.log('Agent response:', data.content); }); ws.on('thinking', data =&gt; { console.log('Agent is thinking...'); }); ws.on('error', data =&gt; { console.error('Error:', data.message); }); // Connect and interact async function startChat() { try { await ws.connect(); // Set user ID for multi-user support ws.setUserId(123); // Subscribe to an agent ws.subscribe('multi_user_agent'); // Send a message ws.sendMessage('multi_user_agent', 'Hello, my name is John.'); // Later, search memory ws.searchMemory('multi_user_agent', 'What is my name?'); } catch (error) { console.error('Failed to connect:', error); } } startChat(); . ",
    "url": "/muxi/websocket/#client-implementation-examples",
    
    "relUrl": "/websocket/#client-implementation-examples"
  },"252": {
    "doc": "WebSocket",
    "title": "Multi-User Interaction Example",
    "content": "This example demonstrates how to create a simple chat interface that supports multiple users with separate memory contexts: . // Set up the form const userIdInput = document.getElementById('user-id'); const agentIdInput = document.getElementById('agent-id'); const messageInput = document.getElementById('message'); const sendButton = document.getElementById('send-button'); const chatMessages = document.getElementById('chat-messages'); let currentUserId = 0; const ws = new AgentWebSocket('ws://localhost:5050/ws'); // Update user ID when changed userIdInput.addEventListener('change', () =&gt; { currentUserId = parseInt(userIdInput.value, 10); // Update WebSocket user ID ws.setUserId(currentUserId); // Add system message addMessage('system', `Switched to user ID: ${currentUserId}`); }); // Send message when button is clicked sendButton.addEventListener('click', () =&gt; { const agentId = agentIdInput.value; const message = messageInput.value; if (!message.trim()) return; // Add user message to chat addMessage('user', message); // Clear input messageInput.value = ''; // Send via WebSocket ws.sendMessage(agentId, message); }); // Add message to chat UI function addMessage(role, text) { const messageEl = document.createElement('div'); messageEl.className = `message ${role}`; // Add user ID for user messages const header = role === 'user' ? `User (ID: ${currentUserId})` : role === 'assistant' ? 'Assistant' : 'System'; messageEl.innerHTML = ` &lt;div class=\"message-header\"&gt;${header}&lt;/div&gt; &lt;div class=\"message-content\"&gt;${text}&lt;/div&gt; `; chatMessages.appendChild(messageEl); chatMessages.scrollTop = chatMessages.scrollHeight; } // Set up WebSocket ws.on('message', data =&gt; { addMessage('assistant', data.content); }); ws.on('thinking', data =&gt; { addMessage('system', 'Assistant is thinking...'); }); ws.on('error', data =&gt; { addMessage('system', `Error: ${data.message}`); }); // Connect and subscribe async function initChat() { try { await ws.connect(); addMessage('system', 'Connected to server'); // Set initial user ID ws.setUserId(currentUserId); // Subscribe to default agent const defaultAgent = agentIdInput.value; ws.subscribe(defaultAgent); addMessage('system', `Subscribed to agent: ${defaultAgent}`); } catch (error) { addMessage('system', `Connection error: ${error.message}`); } } initChat(); . &lt;!-- HTML for the multi-user chat interface --&gt; &lt;div class=\"chat-container\"&gt; &lt;div class=\"chat-header\"&gt; &lt;div class=\"user-selector\"&gt; &lt;label for=\"user-id\"&gt;User ID:&lt;/label&gt; &lt;input type=\"number\" id=\"user-id\" value=\"0\" min=\"0\"&gt; &lt;/div&gt; &lt;div class=\"agent-selector\"&gt; &lt;label for=\"agent-id\"&gt;Agent:&lt;/label&gt; &lt;input type=\"text\" id=\"agent-id\" value=\"multi_user_agent\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=\"chat-messages\" class=\"chat-messages\"&gt;&lt;/div&gt; &lt;div class=\"chat-input\"&gt; &lt;input type=\"text\" id=\"message\" placeholder=\"Type your message...\"&gt; &lt;button id=\"send-button\"&gt;Send&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; . ",
    "url": "/muxi/websocket/#multi-user-interaction-example",
    
    "relUrl": "/websocket/#multi-user-interaction-example"
  },"253": {
    "doc": "WebSocket",
    "title": "Best Practices",
    "content": ". | Error Handling: Implement comprehensive error handling for all WebSocket operations . | Reconnection Logic: Include automatic reconnection in clients to handle network interruptions . | Message Validation: Validate all incoming messages before processing . | Connection Management: Properly manage connection lifecycles to prevent resource leaks . | Load Balancing: For production deployments, consider load balancing WebSocket connections . | Security: Implement proper authentication and authorization for WebSocket connections . | Logging: Add detailed logging for debugging connection issues . | . ",
    "url": "/muxi/websocket/#best-practices",
    
    "relUrl": "/websocket/#best-practices"
  },"254": {
    "doc": "WebSocket",
    "title": "Troubleshooting",
    "content": "Connection Issues . | Check if the WebSocket server is running and accessible | Verify network connectivity between client and server | Ensure the correct WebSocket URL is being used | Check for firewall or proxy settings that might block WebSocket connections | . Message Processing Errors . | Validate message format on both client and server | Check for proper JSON formatting in messages | Verify that required fields are present in messages | Ensure agent IDs exist before attempting to subscribe or send messages | . Performance Issues . | Monitor WebSocket connection count and resource usage | Implement connection limits to prevent server overload | Consider using message queues for high-load scenarios | Optimize message size and frequency | . ",
    "url": "/muxi/websocket/#troubleshooting",
    
    "relUrl": "/websocket/#troubleshooting"
  },"255": {
    "doc": "WebSocket",
    "title": "Next Steps",
    "content": "After implementing WebSocket support, you might want to explore: . | Creating agents that can utilize WebSocket connections | Setting up tool systems that send real-time progress updates | Implementing MCP features for structured communication | Enhancing memory systems with real-time updates | . ",
    "url": "/muxi/websocket/#next-steps",
    
    "relUrl": "/websocket/#next-steps"
  },"256": {
    "doc": "WebSocket",
    "title": "WebSocket",
    "content": " ",
    "url": "/muxi/websocket/",
    
    "relUrl": "/websocket/"
  }
}
