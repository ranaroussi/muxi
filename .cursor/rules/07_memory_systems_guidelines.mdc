---
description:
globs:
alwaysApply: false
---
- Memory systems are initialized and managed at the orchestrator level, not at the agent level
- Configure memory through the orchestrator constructor: `Orchestrator(buffer_memory=buffer, long_term_memory=long_term)`
- For the muxi facade, configure memory in the constructor: `muxi(buffer_memory=50, long_term_memory="connection_string", config_file="config.yaml")`
- Access memory through orchestrator methods: `orchestrator.search_memory()`, `orchestrator.add_to_buffer_memory()`
- Agent constructor no longer accepts direct memory parameters
- Memory is shared between agents through the orchestrator
- Multi-user support is configured at the orchestrator level with Memobase
- Configuration files should have agents in an array at the top level, not as separate files
- Agent instances access memory indirectly through their orchestrator reference
- All memory operations should include agent_id when filtering is needed

## Memory Configuration

### Declarative Configuration
- Memory is configured at initialization time, not through setup methods:
  ```python
  app = muxi(
      buffer_memory=10,
      long_term_memory="postgresql://user:pass@localhost/db",
      config_file="configs/muxi_config.yaml"
  )
  ```

- Configuration files should define multiple agents in a single file:
  ```yaml
  # configs/muxi_config.yaml
  agents:
    - agent_id: assistant1
      description: "..."
      model: { ... }
    - agent_id: assistant2
      description: "..."
      model: { ... }
  ```

### Programmatic Configuration
- Initialize memory components first:
  ```python
  from muxi.core.models.providers.openai import OpenAIModel
  from muxi.core.memory.buffer import SmartBufferMemory
  from muxi.core.memory.long_term import LongTermMemory

  # Create embedding model for FAISS vector search
  embedding_model = OpenAIModel(model="text-embedding-ada-002")

  # Create smart buffer memory with FAISS
  buffer = SmartBufferMemory(
      max_size=100,                    # Maximum number of messages to store
      model=embedding_model,           # Model for generating embeddings
      vector_dimension=1536,           # Dimension for OpenAI embeddings
      recency_bias=0.3                 # Balance between semantic (0.7) and recency (0.3)
  )

  # Create long-term memory
  long_term = LongTermMemory(connection_string="...")
  ```

- Create orchestrator with memory:
  ```python
  orchestrator = Orchestrator(
      buffer_memory=buffer,
      long_term_memory=long_term
  )
  ```

- Create agents through orchestrator:
  ```python
  orchestrator.create_agent(
      agent_id="assistant",
      description="...",
      model=model  # No memory parameters
  )
  ```

## Smart Buffer Memory (Short-Term)

### Hybrid Search Architecture
- SmartBufferMemory implements a hybrid approach combining:
  1. Semantic search using FAISS vector similarity
  2. Recency-based retrieval using buffer positions
  3. Combined scoring with configurable weighting between the two

- Example implementation:
  ```python
  from muxi.core.memory.buffer import SmartBufferMemory
  from muxi.core.models.providers.openai import OpenAIModel

  # Create buffer with FAISS-backed vector search
  buffer = SmartBufferMemory(
      max_size=100,
      model=OpenAIModel(model="text-embedding-ada-002"),
      recency_bias=0.3   # 30% weight for recency, 70% for semantic relevance
  )

  # Add to buffer (through orchestrator)
  await orchestrator.add_to_buffer_memory(
      "Important information about quantum computing",
      metadata={"topic": "science", "importance": "high"}
  )

  # Search with default parameters (balances semantic and recency)
  results = await orchestrator.search_memory(
      "Tell me about quantum computing",
      k=5
  )

  # Search with higher recency bias (favor recent messages)
  results = await orchestrator.search_memory(
      "What did we just talk about?",
      recency_bias=0.7,  # 70% weight for recency, 30% for semantic
      k=3
  )

  # Search with lower recency bias (favor semantic relevance)
  results = await orchestrator.search_memory(
      "Tell me about quantum theory",
      recency_bias=0.1,  # 10% weight for recency, 90% for semantic
      k=5
  )

  # Filter by metadata
  results = await orchestrator.search_memory(
      "Tell me important science facts",
      filter_metadata={"topic": "science", "importance": "high"},
      k=5
  )
  ```

### Key Features and Best Practices
- Use FAISS for vector similarity search with appropriate index types
- Configure recency_bias parameter based on use case:
  - For human conversations (favor recency): 0.5-0.7
  - For factual queries (favor semantics): 0.1-0.3
  - Default (balanced approach): 0.3
- Benefit from graceful degradation to recency-only search when:
  - No model is provided for generating embeddings
  - Embedding generation fails
  - No valid search results are found via vector search
- Utilize thread-safe operations for concurrent access
- Use metadata filtering for targeted retrieval
- Implement proper index rebuilding when buffer is full
- Optimize memory usage for large context buffers
- Balance buffer size against vector search performance
- Configure vector dimensions to match your embedding model (1536 for OpenAI)
- Properly handle errors during vector operations with fallbacks
- Store important metadata alongside messages for filtering

### Smart Buffer Memory vs. Simple Memory
- Smart buffer memory combines semantic+recency for better recall
- Simple deque memory only retrieves based on recency
- Smart buffer memory can find relevant information regardless of when it was added
- Smart buffer supports metadata filtering for targeted retrieval
- Use SmartBufferMemory whenever possible for improved information retrieval

## Long-Term Memory - Database Options

### Configuration Options
- For simplified configuration, use direct connection string format:
  - PostgreSQL: `long_term_memory: "postgresql://user:password@localhost:5432/dbname"`
  - SQLite: `long_term_memory: "sqlite:///path/to/database.db"`
  - Default SQLite: `long_term_memory: true` (creates muxi.db in app root)
- The system automatically detects database type based on the connection string prefix
- Maintain backward compatibility with legacy configuration format
- Use environment variables for connection strings in production: `long_term_memory: "${DATABASE_URL}"`

### PostgreSQL with pgvector
- Recommended for production and multi-user deployments
- Implement proper schema design for memory storage
- Use appropriate indexes for vector columns
- Implement proper query optimization for vector searches
- Handle large volumes of data efficiently
- Implement connection pooling
- Support sharding and horizontal scaling for large deployments
- Use transactions for related operations

### SQLite with sqlite-vec
- Recommended for local development and single-user deployments
- Prefer using the sqlite-vec Python package for vector operations
- Avoid relying on binary extensions when possible for better portability and maintenance
- Implement proper error handling with fallback mechanisms only when necessary
- Use appropriate indexes for vector columns
- Optimize queries for vector searches
- Consider performance limitations for large datasets
- Be aware of concurrency limitations due to file locking
- Suitable for edge deployments and resource-constrained environments

### Common Database Practices
- Implement memory cleanup and archiving
- Support metadata filtering in searches
- Implement proper error handling for database operations
- Handle concurrent access properly
- Support memory exports and imports
- Implement connection timeout and retry logic
- Use appropriate database pooling based on deployment size

## Vector Database Selection Guidelines
- Use SQLite with sqlite-vec for:
  - Local development and testing
  - Single-user applications
  - Edge computing and resource-constrained environments
  - Situations where a simple file-based database is preferred
  - Rapid prototyping and proof-of-concept projects

- Use PostgreSQL with pgvector for:
  - Production environments
  - Multi-user applications
  - High-throughput systems
  - Enterprise deployments with high availability requirements
  - Scenarios requiring horizontal scaling

## Multi-User Memory Best Practices
- Always use PostgreSQL for multi-user applications
- Enable multi-user support through Memobase:
  ```python
  # Create Memobase for multi-user support
  long_term_memory = LongTermMemory(connection_string="postgresql://...")
  memobase = Memobase(long_term_memory=long_term_memory)

  # Initialize orchestrator with multi-user memory
  orchestrator = Orchestrator(
      buffer_memory=buffer,
      long_term_memory=memobase
  )
  ```
- Always include user_id when working with user-specific data
- Handle user privacy and data isolation appropriately
- Implement proper authentication and authorization for user data access
- Consider data retention policies for user-specific memories

## Memobase (User-Aware Memory)
- Configure multi-user support at orchestrator level: `orchestrator = Orchestrator(long_term_memory=memobase, is_multi_user=True)`
- Properly partition memories by user_id
- Implement access control for memories
- Support cross-user memory sharing when appropriate
- Implement proper cleanup of user memories
- Handle user deletion appropriately
- Support memory migration between users
- Implement analytics on memory usage
- Support different memory retention policies per user
- Implement memory prioritization
- Handle memory conflicts
- Support memory search across users for administrators
- Implement proper backup and restore mechanisms

## Vector Operations
- Choose appropriate vector dimensions based on model
- Implement proper vector normalization when using cosine similarity
- Use dimensionality reduction for very high-dimensional vectors
- Benchmark different vector similarity approaches
- Optimize vector storage format
- Implement caching for frequent vector operations
- Use batch operations for multiple vectors
- Implement appropriate vector preprocessing
- Handle out-of-vocabulary tokens in embeddings
- Support different embedding models
- Implement fallbacks for embedding generation failures
- Document vector format and dimensionality
- Handle vector serialization appropriately for different databases
- Use float32 format for vectors to balance precision and size
- Prefer using standardized Python packages over custom binary extensions

## Memory Retrieval
- Access memory through orchestrator: `orchestrator.search_memory(agent_id="assistant", query="...", k=3, threshold=0.7)`
- For smart buffer memory, tune recency_bias to prioritize semantic or recency:
  ```python
  # For recent conversations (favor recency)
  results = await orchestrator.search_memory(
      "What did we just talk about?",
      recency_bias=0.7,
      k=5
  )

  # For factual information (favor semantics)
  results = await orchestrator.search_memory(
      "Tell me about quantum physics",
      recency_bias=0.1,
      k=5
  )
  ```
- Implement relevance scoring for retrieved memories
- Support hybrid search (vector + keyword)
- Implement proper ranking algorithms
- Support different retrieval strategies based on query type
- Implement context-aware retrieval
- Support filters in retrieval operations
- Optimize top-k retrieval performance
- Implement memory deduplication
- Support time-weighted retrieval
- Implement recency bias when appropriate
- Handle retrieval errors gracefully
- Support pagination for large result sets

## Memory Integration
- Properly integrate memories into prompt construction
- Handle context length limits by prioritizing memories
- Implement memory summarization for efficient context use
- Support different memory integration strategies
- Properly attribute information to memory sources
- Handle conflicting information from different memories
- Implement memory weighting based on relevance
- Support streaming memory integration
- Handle memory format conversion for different LLMs
- Support memory collection during conversations
- Implement feedback loops for memory relevance
- Document memory integration patterns

## Configuration File Patterns
- Memory configuration belongs at the top level of YAML/JSON files, not within agent objects:
```yaml
# Good
buffer_memory: 20
long_term_memory: "postgresql://user:password@localhost:5432/muxi"
agents:
  - agent_id: "assistant"
    model: ...

# Bad
agents:
  - agent_id: "assistant"
    model: ...
    memory:  # Don't put memory here
      buffer: 20
      long_term: "postgresql://..."
```

## Performance and Scalability
- Optimize vector operations for performance
- Implement appropriate caching strategies
- Use batch operations for efficiency
- Monitor memory system performance
- Implement sharding for large memory stores (PostgreSQL)
- Optimize database queries for vector operations
- Use connection pooling for database access (PostgreSQL)
- Implement proper indexing for frequent queries
- Benchmark memory operations with realistic workloads
- Implement circuit breakers for external dependencies
- Support horizontal scaling of memory systems (PostgreSQL)
- Document performance characteristics for each database option
- Consider memory constraints when choosing between SQLite and PostgreSQL

## Database Structure
- Structure database around users as the central entity
- Use separate tables for different memory types and collections
- Store user credentials in a dedicated table with proper encryption
- Use nanoid (CHAR(21)) for public-facing identifiers across all tables
- Implement proper foreign key relationships with cascade deletes
- Include created_at and updated_at timestamps on all tables
- Index all columns used for filtering, sorting and joining
- Use JSONB for flexible metadata storage with GIN indexes (PostgreSQL)
- Use JSON for metadata storage in SQLite
- Implement composite indexes for common query patterns (e.g., user_id + created_at)
- Use the pgvector extension for embedding storage with IVFFLAT indexes (PostgreSQL)
- Use the sqlite-vec extension for embedding storage in SQLite
- Set appropriate vector dimensions based on embedding models (e.g., 1536 for OpenAI)
- Create separate indexes for timestamp columns to optimize time-based queries
- Structure the core schema around these key tables:
  - users (id, user_id, timestamps)
  - collections (id, user_id, collection_id, name, description, timestamps)
  - memories (id, user_id, memory_id, collection_id, content, metadata, embedding, timestamps)
  - credentials (id, user_id, credential_id, name, service, credentials, timestamps)
- Ensure all tables have appropriate indexes for their primary access patterns
- Implement proper constraints to maintain data integrity
- Use transactions for operations that affect multiple tables
- Use PostgreSQL's JSONB features for flexible schema evolution
- Implement proper migrations for all schema changes
- Keep schema compatible between PostgreSQL and SQLite where possible
